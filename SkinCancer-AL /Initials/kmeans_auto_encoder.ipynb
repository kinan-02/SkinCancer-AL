{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinan-02/SkinCancer-AL/blob/main/SkinCancer-AL%20/Initials/kmeans_auto_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78bb2bf0-0d30-49f1-96d9-39f768f952da",
      "metadata": {
        "id": "78bb2bf0-0d30-49f1-96d9-39f768f952da"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import entropy\n",
        "from collections import defaultdict\n",
        "import argparse\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "train_df = pd.read_csv('train_dataset/metadata.csv')\n",
        "test_df = pd.read_csv('test_dataset/metadata.csv')\n",
        "val_df = pd.read_csv('validation_dataset/metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "181b48ba-7d82-4b2d-99a9-95a79e561337",
      "metadata": {
        "id": "181b48ba-7d82-4b2d-99a9-95a79e561337",
        "outputId": "abff8f8f-4eed-4f50-c944-442271dd606d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diagnosis\n",
            "nevus                         1205\n",
            "melanoma                      1113\n",
            "pigmented benign keratosis    1099\n",
            "basal cell carcinoma           514\n",
            "squamous cell carcinoma        197\n",
            "vascular lesion                142\n",
            "actinic keratosis              130\n",
            "dermatofibroma                 115\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_df['diagnosis'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dce2916-3cff-4c72-a13d-79a9d7446c57",
      "metadata": {
        "id": "9dce2916-3cff-4c72-a13d-79a9d7446c57",
        "outputId": "ae4da38b-a6eb-45c1-ace1-1b51bb89fcaa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'actinic keratosis': 0,\n",
              " 'basal cell carcinoma': 1,\n",
              " 'dermatofibroma': 2,\n",
              " 'melanoma': 3,\n",
              " 'nevus': 4,\n",
              " 'pigmented benign keratosis': 5,\n",
              " 'squamous cell carcinoma': 6,\n",
              " 'vascular lesion': 7}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_mapping = {\n",
        "    \"actinic keratosis\": 0,\n",
        "    \"basal cell carcinoma\": 1,\n",
        "    \"dermatofibroma\": 2,\n",
        "    \"melanoma\": 3,\n",
        "    \"nevus\": 4,\n",
        "    \"pigmented benign keratosis\": 5,\n",
        "    \"squamous cell carcinoma\": 6,\n",
        "    \"vascular lesion\":7\n",
        "}\n",
        "class_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d140454d-c528-4ef5-91a5-7e90923232b3",
      "metadata": {
        "id": "d140454d-c528-4ef5-91a5-7e90923232b3"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define image transformations (resize, convert to tensor)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),          # Resize images to 224x224 (matching ResNet input size)\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# we made this class to read the data from the file and to use it later in the dataloader\n",
        "class Dataset():\n",
        "    def __init__(self, dataframe, transform, train='train'):\n",
        "        self.dataframe=dataframe\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.path_to_image=self._create_path_to_image_dict()\n",
        "        self.paths=list(self.path_to_image.keys())\n",
        "        self.labels=list(self.path_to_image.values())\n",
        "\n",
        "    #\n",
        "    def _create_path_to_image_dict(self):\n",
        "    \"\"\"\n",
        "    Return the dictionary where the keys are the image paths and the values are the labels.\n",
        "    \"\"\"\n",
        "      path_to_image={}\n",
        "      for index,row in self.dataframe.iterrows():\n",
        "        if self.train == 'train':\n",
        "          img_path = os.path.join('train_dataset/',row['isic_id']+'.jpg')\n",
        "        elif self.train == 'test':\n",
        "          img_path = os.path.join('test_dataset/',row['isic_id']+'.jpg')\n",
        "        else:\n",
        "            img_path = os.path.join('val_dataset/',row['isic_id']+'.jpg')\n",
        "        label=row['diagnosis']\n",
        "        path_to_image[img_path]=label\n",
        "      return path_to_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        \"\"\"\n",
        "         return the image , image label (after the mapping) and the index\n",
        "        \"\"\"\n",
        "        img_path=self.paths[index]\n",
        "        img_label=self.labels[index]\n",
        "        image=Image.open(img_path)\n",
        "        image=self.transform(image)\n",
        "        if self.train == 'val':\n",
        "            return image, class_mapping[img_label], index\n",
        "        return image, img_label, index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be65fdbd-872a-4b7a-9456-531296a2b6f7",
      "metadata": {
        "id": "be65fdbd-872a-4b7a-9456-531296a2b6f7"
      },
      "outputs": [],
      "source": [
        "train_df = Dataset(train_df, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "586c6195-75d0-4d89-8b08-945bbd56df80",
      "metadata": {
        "id": "586c6195-75d0-4d89-8b08-945bbd56df80",
        "outputId": "e61c9986-658c-49b7-98e6-6ada2332f482"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Autoencoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=100352, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=100352, bias=True)\n",
              "  (decoder): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Unflatten(dim=1, unflattened_size=(32, 56, 56))\n",
              "    (2): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (5): ReLU()\n",
              "    (6): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#This is the class of the AutoEncoder that we  build to extract the latent vector for each image.\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, encoded_dim=256):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # input channels=3 for RGB, output channels=32\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),                   # (112x112)\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),                   # (56x56)\n",
        "        )\n",
        "\n",
        "        # Flatten and fully connected layer to get 1D vector\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, encoded_dim)  # From 32x56x56 to encoded_dim\n",
        "\n",
        "        # Decoder\n",
        "        self.fc2 = nn.Linear(encoded_dim, 32 * 56 * 56)  # Fully connected to expand back\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (32, 56, 56)),               # Reshape 1D vector back to (32x56x56)\n",
        "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1), # (112x112)\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1), # (224x224)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 3, kernel_size=3, padding=1),  # Output channels=3 for RGB\n",
        "            nn.Sigmoid()  # Output should be between 0 and 1 for normalized RGB\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoding\n",
        "        x = self.encoder(x)\n",
        "        x = self.flatten(x)         # Flatten to 1D vector\n",
        "        x = self.fc1(x)             # Project to encoded_dim\n",
        "\n",
        "        # Decoding\n",
        "        x = self.fc2(x)             # Expand back to match the flattened shape of feature maps\n",
        "        x = self.decoder(x)         # Pass through the decoder\n",
        "        return x\n",
        "\n",
        "\n",
        "model = Autoencoder()\n",
        "model.load_state_dict(torch.load(f\"ae_model.pth\"))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b8371bf-6ed6-4e7e-84bd-36badc2b8aec",
      "metadata": {
        "id": "9b8371bf-6ed6-4e7e-84bd-36badc2b8aec",
        "outputId": "98f01b20-93d7-49a8-d823-444a2c6567bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4515, 256)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_df, batch_size=32, shuffle=True)\n",
        "\n",
        "def extract_vae_features(dataloader, model):\n",
        "  \"\"\"\n",
        "  Return the latent vector for each image and the corresponding indices.\n",
        "\n",
        "  \"\"\"\n",
        "    features_list = []\n",
        "    indices_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _, indices in dataloader:\n",
        "            x = images.to(device)\n",
        "            with torch.no_grad():\n",
        "                x = model.encoder(x)\n",
        "                x = model.flatten(x)\n",
        "                x = model.fc1(x)\n",
        "\n",
        "            features_list.append(x.cpu().numpy())\n",
        "            indices_list.extend(indices)\n",
        "\n",
        "    features = np.vstack(features_list)\n",
        "    return features, indices_list\n",
        "\n",
        "train_features, train_indices = extract_vae_features(train_loader, model)\n",
        "print(train_features.shape)\n",
        "\n",
        "# Apply K-Means clustering on the train_features\n",
        "n_clusters = 30\n",
        "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=0)\n",
        "kmeans.fit(train_features)\n",
        "\n",
        "# Get cluster labels for each image\n",
        "cluster_labels = kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e0ef939-3409-4851-90a6-3860ea5f8ced",
      "metadata": {
        "id": "5e0ef939-3409-4851-90a6-3860ea5f8ced",
        "outputId": "6ef93c43-8e2e-491c-e596-f401ac49b75f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: [tensor(1380)], 1: [tensor(1482)], 2: [tensor(1787)], 3: [tensor(2612)], 4: [tensor(235)], 5: [tensor(3743)], 6: [tensor(2044)], 7: [tensor(2939)], 8: [tensor(3667)], 9: [tensor(1570)], 10: [tensor(779)], 11: [tensor(2491)], 12: [tensor(4318)], 13: [tensor(346)], 14: [tensor(2031)], 15: [tensor(455)], 16: [tensor(4340)], 17: [tensor(3895)], 18: [tensor(2774)], 19: [tensor(4097)], 20: [tensor(2119)], 21: [tensor(1383)], 22: [tensor(822)], 23: [tensor(4215)], 24: [tensor(2803)], 25: [tensor(166)], 26: [tensor(137)], 27: [tensor(4128)], 28: [tensor(1054)], 29: [tensor(1118)]}\n"
          ]
        }
      ],
      "source": [
        "train_cluster_labels = kmeans.labels_\n",
        "\n",
        "def get_representative_images(cluster_labels, indices):\n",
        "  \"\"\"\n",
        "  Return the k samples that are the nearest to the centroid of each cluster, where k is equal to the budget.\n",
        "  \"\"\"\n",
        "    cluster_to_images = {}\n",
        "    for i in range(kmeans.n_clusters):\n",
        "        cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
        "        cluster_features = train_features[cluster_indices]\n",
        "        distances = np.linalg.norm(cluster_features - kmeans.cluster_centers_[i], axis=1)\n",
        "        nearest_indices = cluster_indices[np.argsort(distances)[:1]]\n",
        "        cluster_to_images[i] = [indices[idx] for idx in nearest_indices]\n",
        "    return cluster_to_images\n",
        "\n",
        "representative_images = get_representative_images(train_cluster_labels, train_indices)\n",
        "print(representative_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbb25dca-b54b-48e0-9e9e-2c55fbcf9b85",
      "metadata": {
        "id": "dbb25dca-b54b-48e0-9e9e-2c55fbcf9b85",
        "outputId": "23e72c6e-0bef-4482-e181-f4ab94aa40dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1380,\n",
              " 1482,\n",
              " 1787,\n",
              " 2612,\n",
              " 235,\n",
              " 3743,\n",
              " 2044,\n",
              " 2939,\n",
              " 3667,\n",
              " 1570,\n",
              " 779,\n",
              " 2491,\n",
              " 4318,\n",
              " 346,\n",
              " 2031,\n",
              " 455,\n",
              " 4340,\n",
              " 3895,\n",
              " 2774,\n",
              " 4097,\n",
              " 2119,\n",
              " 1383,\n",
              " 822,\n",
              " 4215,\n",
              " 2803,\n",
              " 166,\n",
              " 137,\n",
              " 4128,\n",
              " 1054,\n",
              " 1118]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ids.item() for l in representative_images.values() for ids in l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a405381-90a2-4e1e-8cf1-ed1970a5df4c",
      "metadata": {
        "id": "7a405381-90a2-4e1e-8cf1-ed1970a5df4c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinan-02/SkinCancer-AL/blob/main/Stratiges/custom_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjgyTgaRfy4k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import entropy\n",
        "from collections import defaultdict\n",
        "import argparse\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "train_df = pd.read_csv('train_dataset/metadata.csv')\n",
        "test_df = pd.read_csv('test_dataset/metadata.csv')\n",
        "val_df = pd.read_csv('validation_dataset/metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "bcHCCahYj6B7",
        "outputId": "1facd164-3e8b-40f4-81d7-52fc3110e538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diagnosis\n",
            "nevus                         1205\n",
            "melanoma                      1113\n",
            "pigmented benign keratosis    1099\n",
            "basal cell carcinoma           514\n",
            "squamous cell carcinoma        197\n",
            "vascular lesion                142\n",
            "actinic keratosis              130\n",
            "dermatofibroma                 115\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_df['diagnosis'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcS9AAMTmQe2",
        "outputId": "dcc69bc3-94ed-45ab-b0da-04d1b318d89f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'actinic keratosis': 0,\n",
              " 'basal cell carcinoma': 1,\n",
              " 'dermatofibroma': 2,\n",
              " 'melanoma': 3,\n",
              " 'nevus': 4,\n",
              " 'pigmented benign keratosis': 5,\n",
              " 'squamous cell carcinoma': 6,\n",
              " 'vascular lesion': 7}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_mapping = {\n",
        "    \"actinic keratosis\": 0,\n",
        "    \"basal cell carcinoma\": 1,\n",
        "    \"dermatofibroma\": 2,\n",
        "    \"melanoma\": 3,\n",
        "    \"nevus\": 4,\n",
        "    \"pigmented benign keratosis\": 5,\n",
        "    \"squamous cell carcinoma\": 6,\n",
        "    \"vascular lesion\":7\n",
        "}\n",
        "class_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLruwetXmPem"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define image transformations (resize, convert to tensor)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),          # Resize images to 224x224 (matching ResNet input size)\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# we made this class to read the data from the file and to use it later in the dataloader\n",
        "class Dataset():\n",
        "    def __init__(self, dataframe, transform, train='train'):\n",
        "        self.dataframe=dataframe\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.path_to_image=self._create_path_to_image_dict()\n",
        "        self.paths=list(self.path_to_image.keys())\n",
        "        self.labels=list(self.path_to_image.values())\n",
        "\n",
        "    def _create_path_to_image_dict(self):\n",
        "      path_to_image={}\n",
        "      for index,row in self.dataframe.iterrows():\n",
        "        if self.train == 'train':\n",
        "          img_path = os.path.join('train_dataset/',row['isic_id']+'.jpg')\n",
        "        elif self.train == 'test':\n",
        "          img_path = os.path.join('test_dataset/',row['isic_id']+'.jpg')\n",
        "        else:\n",
        "            img_path = os.path.join('validation_dataset/',row['isic_id']+'.jpg')\n",
        "        label=row['diagnosis']\n",
        "        path_to_image[img_path]=label\n",
        "      return path_to_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        img_path=self.paths[index]\n",
        "        img_label=self.labels[index]\n",
        "        image=Image.open(img_path)\n",
        "        image=self.transform(image)\n",
        "        if self.train == 'val':\n",
        "            return image, class_mapping[img_label], index\n",
        "        return image, img_label, index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yCvnprDoa_7"
      },
      "outputs": [],
      "source": [
        "train_df = Dataset(train_df, transform)\n",
        "val_df = Dataset(val_df, transform,train='val')\n",
        "test_df = Dataset(test_df, transform,train='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5L0NcpesnZI",
        "outputId": "22f97091-b571-4138-964f-95a509890583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Load pre-trained ResNet50 model from torchvision\n",
        "base_model = models.resnet50(pretrained=True)\n",
        "\n",
        "num_classes = 8\n",
        "base_model.fc = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(base_model.fc.in_features, 128),  # Add a fully connected layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, num_classes),  # Final layer with number of classes\n",
        "    nn.Softmax(dim=1)  # Softmax activation for multi-class classification\n",
        ")\n",
        "\n",
        "for param in base_model.parameters():\n",
        "    param.requires_grad = False  # Freeze all layers except the fully connected ones\n",
        "\n",
        "# Unfreeze the final fully connected layer\n",
        "for param in base_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(base_model.fc.parameters(), lr=0.0008)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model.to(device)\n",
        "x = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__MKYZe5mQe3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 4\n",
        "val_loader = DataLoader(val_df, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vk8kyZsgACa",
        "outputId": "4df6db77-1d5a-4a6a-c139-3b9cf986fdc2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-16 10:21:37.265475: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-16 10:21:43.329739: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
            "2024-10-16 10:21:43.329772: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-10-16 10:21:54.854592: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
            "2024-10-16 10:21:54.854809: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
            "2024-10-16 10:21:54.854825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "\n",
        "class ActiveLearningPipeline:\n",
        "    def __init__(self, model,\n",
        "                 available_pool_indices,\n",
        "                 train_indices,\n",
        "                 test_indices,\n",
        "                 selection_criterion,\n",
        "                 iterations,\n",
        "                 budget_per_iter,\n",
        "                 num_epochs):\n",
        "        self.model = model\n",
        "        self.iterations = iterations\n",
        "        self.budget_per_iter = budget_per_iter\n",
        "        self.available_pool_indices = available_pool_indices\n",
        "        self.train_indices = train_indices\n",
        "        self.test_indices = test_indices\n",
        "        self.selection_criterion = selection_criterion\n",
        "        if self.selection_criterion == 'random':\n",
        "          self.train_indices = []\n",
        "        self.num_epochs = num_epochs\n",
        "        self.pool_features = []\n",
        "        self.pool_indices = []\n",
        "        self.train_features = []\n",
        "        # self.best_acc = 0\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"\n",
        "        Run the active learning pipeline\n",
        "        :return\n",
        "        accuracy_scores: list, accuracy scores at each iteration\n",
        "        \"\"\"\n",
        "        accuracy_scores = []\n",
        "        self._get_features()\n",
        "        for iteration in range(self.iterations):\n",
        "            print(f\"--------- Number of Iteration {iteration} ---------\")\n",
        "            if self.selection_criterion == 'random':\n",
        "                self._random_sampling()\n",
        "            elif self.selection_criterion == 'custom':\n",
        "                self._custom_sampling()\n",
        "\n",
        "            train_images = [train_df.__getitem__(index)[0] for index in self.train_indices]\n",
        "            label_df = [class_mapping[train_df.__getitem__(index)[1]] for index in self.train_indices]\n",
        "            self._train_model(train_images, label_df)\n",
        "            self.model.load_state_dict(torch.load(f\"best_{self.selection_criterion}_model.pth\"))\n",
        "            accuracy = self._evaluate_model()\n",
        "            accuracy_scores.append(accuracy)\n",
        "        return accuracy_scores\n",
        "\n",
        "    def calculate_class_weights(self, label_counts, num_classes=8):\n",
        "      \"\"\"\n",
        "      calculate class weights to handle data imbalance for the loss function.\n",
        "      \"\"\"\n",
        "        total_samples = sum(label_counts.values())\n",
        "        class_weights = torch.zeros(num_classes)\n",
        "\n",
        "        for cls in range(num_classes):\n",
        "            if cls in label_counts:\n",
        "                class_weights[cls] = total_samples / (num_classes * label_counts[cls])\n",
        "            else:\n",
        "                class_weights[cls] = 1.0  # Handle the case where a class has zero samples in the current epoch\n",
        "\n",
        "        return class_weights\n",
        "\n",
        "    def _train_model(self, train_images, label_df):\n",
        "      \"\"\"\n",
        "      This function trains the base model on the provided training set and saves the best-performing model.\n",
        "      \"\"\"\n",
        "      label_counts = defaultdict(int)\n",
        "      for label in label_df:\n",
        "                label_counts[label] += 1\n",
        "      class_weights = self.calculate_class_weights(label_counts, 8).to(device)\n",
        "      loss_f = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "      train_images_tensor = torch.stack(train_images)\n",
        "      label_df_tensor = torch.tensor(label_df)\n",
        "      train_dataset = TensorDataset(train_images_tensor, label_df_tensor)\n",
        "\n",
        "      batch_size = 32\n",
        "      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "      best_acc = 0\n",
        "      for epoch in range(self.num_epochs):\n",
        "                self.model.train()\n",
        "                running_loss = 0.0  # Track the running loss\n",
        "                correct_predictions = 0\n",
        "                total_predictions = 0\n",
        "                # Training loop\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs = inputs\n",
        "                    inputs= inputs.to(device)\n",
        "                    labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                    # Zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.model(inputs)\n",
        "                    # outputs = outputs.logits\n",
        "                    loss = loss_f(outputs, labels)\n",
        "\n",
        "                    # Backward pass and optimization\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    correct_predictions += torch.sum(preds == labels)\n",
        "                    total_predictions += inputs.shape[0]\n",
        "\n",
        "                # Print loss and accuracy at the end of each epoch\n",
        "                epoch_loss = running_loss / len(train_loader)\n",
        "                epoch_acc = correct_predictions.double() / total_predictions\n",
        "                print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "\n",
        "                val_acc = self._check_model()\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "                    torch.save(self.model.state_dict(), f\"best_{self.selection_criterion}_model.pth\")\n",
        "      print(\"--\"*30)\n",
        "\n",
        "    def _check_model(self):\n",
        "      \"\"\"\n",
        "      Returns the accuracy of the base model on the validation set.\n",
        "      \"\"\"\n",
        "        self.model.eval()\n",
        "        running_corrects = 0\n",
        "        total_predictions = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, _ in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                running_corrects += torch.sum(preds == labels)\n",
        "                total_predictions += inputs.shape[0]\n",
        "        val_acc = running_corrects.double() / total_predictions\n",
        "        return val_acc.item()\n",
        "\n",
        "    def _evaluate_model(self):\n",
        "        \"\"\"\n",
        "        Returns the accuracy of the base model on the test set.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        running_corrects = 0\n",
        "        test_images_tensor = torch.stack(test_images)\n",
        "        label_df_tensor = torch.tensor(test_label_df)\n",
        "        test_dataset = TensorDataset(test_images_tensor, label_df_tensor)\n",
        "        batch_size = 32\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "        total_predictions = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                running_corrects += torch.sum(preds == labels)\n",
        "                total_predictions += inputs.shape[0]\n",
        "        test_acc = running_corrects.double() / total_predictions\n",
        "        return test_acc.item()\n",
        "\n",
        "    def _random_sampling(self):\n",
        "      \"\"\"\n",
        "      Adds samples to the training set using a random sampling strategy.\n",
        "      \"\"\"\n",
        "      selected_indices = np.random.choice(self.available_pool_indices, self.budget_per_iter, replace=False)\n",
        "      selected_indices = selected_indices.tolist()\n",
        "      self.train_indices = self.train_indices + selected_indices\n",
        "\n",
        "      available_pool_set = set(self.available_pool_indices)\n",
        "      train_set = set(self.train_indices)\n",
        "      self.available_pool_indices = list(available_pool_set - train_set)\n",
        "\n",
        "    def extract_vae_features(self, dataloader, model, feature_extractor):\n",
        "        \"\"\"\n",
        "        Return the latent vector for each image and the corresponding indices.\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "        indices_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, indices in dataloader:\n",
        "                images = images.to(device)\n",
        "                images_list = [transforms.ToPILImage()(img) for img in images]\n",
        "                inputs = feature_extractor(images=images_list, return_tensors=\"pt\")\n",
        "                with torch.no_grad():\n",
        "                    inputs = inputs.to(device)\n",
        "                    outputs = model(**inputs)\n",
        "\n",
        "                x = outputs.last_hidden_state[:, 0, :]\n",
        "                features_list.append(x.cpu().numpy())\n",
        "\n",
        "                # Collect indices\n",
        "                indices_list.extend(indices)\n",
        "\n",
        "        # Stack all features into a 2D array (n_samples, hidden_dim)\n",
        "        features = np.vstack(features_list)\n",
        "\n",
        "        return features, indices_list\n",
        "\n",
        "    def _get_features(self):\n",
        "      \"\"\"\n",
        "      Creates latent feature vectors for each image in the available pool using a pre-trained Vision Transformer (ViT) model from Google.\n",
        "      \"\"\"\n",
        "        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        feature_extractor = feature_extractor.to(device)\n",
        "        model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        model = model.to(device)\n",
        "\n",
        "        train_images = [train_df.__getitem__(index)[0] for index in self.train_indices]\n",
        "        train_images_tensor = torch.stack(train_images)\n",
        "        label_df_tensor = torch.tensor(self.train_indices)\n",
        "        train_dataset = TensorDataset(train_images_tensor, label_df_tensor)\n",
        "        batch_size = 32\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        self.train_features, self.train_indices = self.extract_vae_features(train_loader, model, feature_extractor)\n",
        "\n",
        "        X_unlabeled = [train_df.__getitem__(index)[0] for index in self.available_pool_indices]\n",
        "        pool_images_tensor = torch.stack(X_unlabeled)\n",
        "        pool_indices_tensor = torch.tensor(self.available_pool_indices)\n",
        "        pool_dataset = TensorDataset(pool_images_tensor, pool_indices_tensor)\n",
        "\n",
        "        batch_size = 32\n",
        "        pool_loader = DataLoader(pool_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        self.pool_features, self.pool_indices = self.extract_vae_features(pool_loader, model, feature_extractor)\n",
        "\n",
        "    def _custom_sampling(self):\n",
        "      \"\"\"\n",
        "      Adds samples to the training set using a custom sampling strategy.\n",
        "      \"\"\"\n",
        "          misclassified_indices = []\n",
        "          model = self.model\n",
        "          model.eval()\n",
        "          train_images = [train_df.__getitem__(index)[0] for index in self.train_indices]\n",
        "          label_df = [class_mapping[train_df.__getitem__(index)[1]] for index in self.train_indices]\n",
        "          train_images_tensor = torch.stack(train_images)\n",
        "          label_df_tensor = torch.tensor(label_df)\n",
        "          indices_tensor = torch.tensor(self.train_indices)\n",
        "          train_dataset = TensorDataset(train_images_tensor, label_df_tensor, indices_tensor)\n",
        "\n",
        "          batch_size = 32\n",
        "          train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "          with torch.no_grad():\n",
        "            for idx, (inputs, labels, indices) in enumerate(train_loader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                incorrect = (preds != labels).cpu().numpy()\n",
        "                misclassified_indices += [i for i, incorrect_flag in zip(indices, incorrect) if incorrect_flag]\n",
        "          # An array containing the feature vectors of the samples that the model misclassified.\n",
        "          misclassified_features = np.array([self.train_features[self.train_indices.index(i)] for i in misclassified_indices])\n",
        "\n",
        "          max_similarities = []\n",
        "          for pool_feature in self.pool_features:\n",
        "            # Compute cosine similarity between pool features and misclassified features\n",
        "            similarities = cosine_similarity([pool_feature], misclassified_features)\n",
        "            max_similarity = np.max(similarities)\n",
        "            max_similarities.append(max_similarity)\n",
        "\n",
        "          #selected_indices is the indices of the samples from the pool that have the highest similarity to the misclassified samples\n",
        "          selected_indices = np.argsort(max_similarities)[:self.budget_per_iter]\n",
        "          temp = np.array(self.available_pool_indices)\n",
        "          selected_indices = temp[selected_indices]\n",
        "\n",
        "          for i in selected_indices:\n",
        "              index = self.pool_indices.index(i)\n",
        "\n",
        "              self.train_features = np.append(self.train_features, [self.pool_features[index]], axis=0)\n",
        "              self.train_indices.append(i)\n",
        "\n",
        "              self.pool_features = np.delete(self.pool_features, index, axis=0)\n",
        "              self.pool_indices.pop(index)\n",
        "\n",
        "          available_pool_set = set(self.available_pool_indices)\n",
        "          train_set = set(self.train_indices)\n",
        "          self.available_pool_indices = list(available_pool_set - train_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5829ZYDh1Rp"
      },
      "outputs": [],
      "source": [
        "def generate_plot(accuracy_scores_dict):\n",
        "    \"\"\"\n",
        "    Generate a plot\n",
        "    \"\"\"\n",
        "    for criterion, accuracy_scores in accuracy_scores_dict.items():\n",
        "        plt.plot(range(1, len(accuracy_scores) + 1), accuracy_scores, label=criterion)\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ2IZmfRuYgX"
      },
      "outputs": [],
      "source": [
        "available_pool_indices = []\n",
        "for i in range(len(train_df)):\n",
        "  \"\"\"Initialize the available pool indices \"\"\"\n",
        "    image, label, index = train_df[i]\n",
        "    available_pool_indices.append(index)\n",
        "\n",
        "test_indices = []\n",
        "for i in range(len(test_df)):\n",
        "  \"\"\"Initialize the  test set \"\"\"\n",
        "    image, label, index = test_df[i]\n",
        "    test_indices.append(index)\n",
        "\n",
        "# Extract the images and labels for the collected test indices\n",
        "test_images = [test_df.__getitem__(index)[0] for index in test_indices]\n",
        "test_label_df = [class_mapping[test_df.__getitem__(index)[1]] for index in test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep0GNsAkmQe5"
      },
      "outputs": [],
      "source": [
        "# This is the initial training set extracted using KMeans++ clustering with ViT (Vision Transformer) feature extraction.\n",
        "train_indices = [1372,\n",
        " 1277,\n",
        " 1255,\n",
        " 1423,\n",
        " 2925,\n",
        " 1963,\n",
        " 2335,\n",
        " 1923,\n",
        " 3791,\n",
        " 1239,\n",
        " 909,\n",
        " 134,\n",
        " 1547,\n",
        " 3931,\n",
        " 2467,\n",
        " 2832,\n",
        " 1789,\n",
        " 3022,\n",
        " 2424,\n",
        " 780,\n",
        " 2412,\n",
        " 3038,\n",
        " 2158,\n",
        " 3335,\n",
        " 1868,\n",
        " 1771,\n",
        " 2015,\n",
        " 1535,\n",
        " 710,\n",
        " 3007]\n",
        "available_pool_set = set(available_pool_indices)\n",
        "train_set = set(train_indices)\n",
        "available_pool_indices = list(available_pool_set - train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmcf2jc6h2S9"
      },
      "outputs": [],
      "source": [
        "# train_indices = []\n",
        "iterations = 20\n",
        "budget_per_iter = 60\n",
        "num_epoch = 15\n",
        "selection_criteria = ['custom']\n",
        "accuracy_scores_dict = defaultdict(list)\n",
        "model = base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "4UGO2jW1h5Ql",
        "outputId": "0993cc7b-2fc2-4a84-a31a-ef45ed5febff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 0 ---------\n",
            "[tensor(1868), tensor(2015), tensor(1255), tensor(1963), tensor(2335), tensor(2925), tensor(1923), tensor(1547), tensor(3022), tensor(909), tensor(3931), tensor(1277), tensor(1771), tensor(710), tensor(1372), tensor(3791), tensor(3007), tensor(1423), tensor(2832), tensor(1535), tensor(780), tensor(2467), tensor(2158), tensor(3038), tensor(134), tensor(2424), tensor(1239), tensor(3335), tensor(2412)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 2.0695, Accuracy: 0.1111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 2.0317, Accuracy: 0.1333\n",
            "Epoch [3/15], Loss: 1.9597, Accuracy: 0.4333\n",
            "Epoch [4/15], Loss: 1.9084, Accuracy: 0.4111\n",
            "Epoch [5/15], Loss: 1.8495, Accuracy: 0.5222\n",
            "Epoch [6/15], Loss: 1.8091, Accuracy: 0.5889\n",
            "Epoch [7/15], Loss: 1.7261, Accuracy: 0.6667\n",
            "Epoch [8/15], Loss: 1.6977, Accuracy: 0.7333\n",
            "Epoch [9/15], Loss: 1.6499, Accuracy: 0.7111\n",
            "Epoch [10/15], Loss: 1.5900, Accuracy: 0.8111\n",
            "Epoch [11/15], Loss: 1.5547, Accuracy: 0.8111\n",
            "Epoch [12/15], Loss: 1.5307, Accuracy: 0.8444\n",
            "Epoch [13/15], Loss: 1.5050, Accuracy: 0.8889\n",
            "Epoch [14/15], Loss: 1.4689, Accuracy: 0.9111\n",
            "Epoch [15/15], Loss: 1.4649, Accuracy: 0.8667\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 1 ---------\n",
            "[tensor(1923), tensor(2467), tensor(134), tensor(2981), tensor(3953), tensor(714)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6092, Accuracy: 0.7267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5861, Accuracy: 0.7333\n",
            "Epoch [3/15], Loss: 1.5607, Accuracy: 0.7667\n",
            "Epoch [4/15], Loss: 1.5329, Accuracy: 0.7800\n",
            "Epoch [5/15], Loss: 1.5077, Accuracy: 0.7933\n",
            "Epoch [6/15], Loss: 1.4843, Accuracy: 0.8067\n",
            "Epoch [7/15], Loss: 1.4808, Accuracy: 0.8267\n",
            "Epoch [8/15], Loss: 1.4863, Accuracy: 0.8133\n",
            "Epoch [9/15], Loss: 1.4434, Accuracy: 0.8600\n",
            "Epoch [10/15], Loss: 1.4430, Accuracy: 0.8400\n",
            "Epoch [11/15], Loss: 1.4413, Accuracy: 0.8200\n",
            "Epoch [12/15], Loss: 1.4313, Accuracy: 0.8600\n",
            "Epoch [13/15], Loss: 1.4188, Accuracy: 0.8667\n",
            "Epoch [14/15], Loss: 1.4457, Accuracy: 0.8600\n",
            "Epoch [15/15], Loss: 1.4080, Accuracy: 0.8733\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 2 ---------\n",
            "[tensor(1923), tensor(134), tensor(2981), tensor(714), tensor(900), tensor(3157), tensor(3910), tensor(2798), tensor(1700), tensor(3287), tensor(1207), tensor(3743), tensor(2536), tensor(3778), tensor(2601), tensor(703)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5909, Accuracy: 0.7333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5842, Accuracy: 0.7429\n",
            "Epoch [3/15], Loss: 1.5528, Accuracy: 0.7667\n",
            "Epoch [4/15], Loss: 1.5517, Accuracy: 0.7714\n",
            "Epoch [5/15], Loss: 1.5404, Accuracy: 0.7905\n",
            "Epoch [6/15], Loss: 1.5437, Accuracy: 0.8095\n",
            "Epoch [7/15], Loss: 1.5255, Accuracy: 0.8048\n",
            "Epoch [8/15], Loss: 1.5358, Accuracy: 0.8238\n",
            "Epoch [9/15], Loss: 1.5210, Accuracy: 0.8476\n",
            "Epoch [10/15], Loss: 1.5403, Accuracy: 0.8190\n",
            "Epoch [11/15], Loss: 1.5161, Accuracy: 0.8381\n",
            "Epoch [12/15], Loss: 1.5222, Accuracy: 0.8571\n",
            "Epoch [13/15], Loss: 1.5058, Accuracy: 0.8476\n",
            "Epoch [14/15], Loss: 1.5180, Accuracy: 0.8143\n",
            "Epoch [15/15], Loss: 1.4969, Accuracy: 0.8619\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 3 ---------\n",
            "[tensor(1923), tensor(134), tensor(2981), tensor(714), tensor(900), tensor(3157), tensor(3910), tensor(2798), tensor(1700), tensor(3287), tensor(1207), tensor(3743), tensor(2536), tensor(2601), tensor(703), tensor(993), tensor(2174), tensor(3142), tensor(2717), tensor(3169), tensor(2437), tensor(582), tensor(931), tensor(1491), tensor(57), tensor(193), tensor(2128), tensor(501), tensor(1770), tensor(2450), tensor(4501), tensor(2438), tensor(3271), tensor(346), tensor(72), tensor(1339), tensor(3005), tensor(1486), tensor(913), tensor(3182), tensor(475), tensor(1405), tensor(1303), tensor(3151), tensor(3358), tensor(234), tensor(2265)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6683, Accuracy: 0.7296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6929, Accuracy: 0.6370\n",
            "Epoch [3/15], Loss: 1.6205, Accuracy: 0.7481\n",
            "Epoch [4/15], Loss: 1.6175, Accuracy: 0.7778\n",
            "Epoch [5/15], Loss: 1.6149, Accuracy: 0.7667\n",
            "Epoch [6/15], Loss: 1.5846, Accuracy: 0.8037\n",
            "Epoch [7/15], Loss: 1.5896, Accuracy: 0.7556\n",
            "Epoch [8/15], Loss: 1.5871, Accuracy: 0.7926\n",
            "Epoch [9/15], Loss: 1.5643, Accuracy: 0.7852\n",
            "Epoch [10/15], Loss: 1.5421, Accuracy: 0.8222\n",
            "Epoch [11/15], Loss: 1.5506, Accuracy: 0.8185\n",
            "Epoch [12/15], Loss: 1.5407, Accuracy: 0.8148\n",
            "Epoch [13/15], Loss: 1.5441, Accuracy: 0.8185\n",
            "Epoch [14/15], Loss: 1.5249, Accuracy: 0.8370\n",
            "Epoch [15/15], Loss: 1.5415, Accuracy: 0.8407\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 4 ---------\n",
            "[tensor(134), tensor(714), tensor(900), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(3743), tensor(2536), tensor(2601), tensor(703), tensor(3169), tensor(931), tensor(1491), tensor(193), tensor(2450), tensor(4501), tensor(3271), tensor(1339), tensor(1405), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(2770), tensor(495), tensor(488), tensor(4355), tensor(1282), tensor(3188), tensor(288), tensor(2892), tensor(2013), tensor(3036), tensor(330), tensor(3690), tensor(2913), tensor(539)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5556, Accuracy: 0.7970\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5535, Accuracy: 0.7939\n",
            "Epoch [3/15], Loss: 1.5577, Accuracy: 0.7970\n",
            "Epoch [4/15], Loss: 1.5607, Accuracy: 0.8061\n",
            "Epoch [5/15], Loss: 1.5523, Accuracy: 0.8121\n",
            "Epoch [6/15], Loss: 1.5560, Accuracy: 0.7788\n",
            "Epoch [7/15], Loss: 1.5593, Accuracy: 0.8091\n",
            "Epoch [8/15], Loss: 1.5397, Accuracy: 0.8091\n",
            "Epoch [9/15], Loss: 1.5558, Accuracy: 0.8152\n",
            "Epoch [10/15], Loss: 1.5286, Accuracy: 0.8182\n",
            "Epoch [11/15], Loss: 1.5354, Accuracy: 0.8364\n",
            "Epoch [12/15], Loss: 1.5172, Accuracy: 0.8394\n",
            "Epoch [13/15], Loss: 1.4974, Accuracy: 0.8545\n",
            "Epoch [14/15], Loss: 1.5219, Accuracy: 0.8576\n",
            "Epoch [15/15], Loss: 1.5589, Accuracy: 0.7788\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 5 ---------\n",
            "[tensor(1923), tensor(134), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(2536), tensor(703), tensor(931), tensor(1491), tensor(2450), tensor(4501), tensor(3271), tensor(1339), tensor(1405), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(2770), tensor(488), tensor(4355), tensor(1282), tensor(2013), tensor(3036), tensor(330), tensor(3690), tensor(2913), tensor(539), tensor(3426), tensor(3775), tensor(528), tensor(2813), tensor(52), tensor(1390), tensor(4178), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(4231), tensor(51), tensor(291), tensor(889), tensor(685), tensor(2632), tensor(823), tensor(289), tensor(1601)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6057, Accuracy: 0.7436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5993, Accuracy: 0.7615\n",
            "Epoch [3/15], Loss: 1.5944, Accuracy: 0.7513\n",
            "Epoch [4/15], Loss: 1.5830, Accuracy: 0.7897\n",
            "Epoch [5/15], Loss: 1.5455, Accuracy: 0.7821\n",
            "Epoch [6/15], Loss: 1.5649, Accuracy: 0.7846\n",
            "Epoch [7/15], Loss: 1.5783, Accuracy: 0.7795\n",
            "Epoch [8/15], Loss: 1.5385, Accuracy: 0.8256\n",
            "Epoch [9/15], Loss: 1.5732, Accuracy: 0.8205\n",
            "Epoch [10/15], Loss: 1.5592, Accuracy: 0.8179\n",
            "Epoch [11/15], Loss: 1.5625, Accuracy: 0.8256\n",
            "Epoch [12/15], Loss: 1.5182, Accuracy: 0.8154\n",
            "Epoch [13/15], Loss: 1.5276, Accuracy: 0.8410\n",
            "Epoch [14/15], Loss: 1.5518, Accuracy: 0.8231\n",
            "Epoch [15/15], Loss: 1.5407, Accuracy: 0.8231\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 6 ---------\n",
            "[tensor(1923), tensor(134), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(2536), tensor(703), tensor(931), tensor(1491), tensor(2450), tensor(4501), tensor(3271), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(2770), tensor(495), tensor(488), tensor(3188), tensor(288), tensor(2013), tensor(3036), tensor(330), tensor(3690), tensor(2913), tensor(539), tensor(3426), tensor(3775), tensor(2813), tensor(52), tensor(4178), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(358), tensor(291), tensor(889), tensor(685), tensor(2632), tensor(823), tensor(289), tensor(1601), tensor(236), tensor(2269), tensor(3407), tensor(338), tensor(3383), tensor(681), tensor(1842), tensor(1186), tensor(905), tensor(3558), tensor(2970), tensor(3425), tensor(2753), tensor(1871), tensor(2830), tensor(1658), tensor(1748), tensor(424), tensor(1522), tensor(3208), tensor(224)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6156, Accuracy: 0.7467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.7015, Accuracy: 0.6556\n",
            "Epoch [3/15], Loss: 1.6497, Accuracy: 0.6711\n",
            "Epoch [4/15], Loss: 1.6641, Accuracy: 0.7067\n",
            "Epoch [5/15], Loss: 1.6397, Accuracy: 0.7178\n",
            "Epoch [6/15], Loss: 1.6638, Accuracy: 0.6711\n",
            "Epoch [7/15], Loss: 1.6326, Accuracy: 0.7178\n",
            "Epoch [8/15], Loss: 1.6548, Accuracy: 0.6311\n",
            "Epoch [9/15], Loss: 1.7055, Accuracy: 0.5867\n",
            "Epoch [10/15], Loss: 1.5987, Accuracy: 0.7644\n",
            "Epoch [11/15], Loss: 1.5962, Accuracy: 0.7044\n",
            "Epoch [12/15], Loss: 1.6989, Accuracy: 0.6378\n",
            "Epoch [13/15], Loss: 1.6999, Accuracy: 0.5911\n",
            "Epoch [14/15], Loss: 1.5614, Accuracy: 0.7622\n",
            "Epoch [15/15], Loss: 1.6094, Accuracy: 0.6756\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 7 ---------\n",
            "[tensor(1923), tensor(134), tensor(2981), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(3743), tensor(2536), tensor(2601), tensor(703), tensor(993), tensor(931), tensor(1491), tensor(193), tensor(4501), tensor(3271), tensor(1339), tensor(3182), tensor(3151), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(2770), tensor(488), tensor(1282), tensor(3188), tensor(288), tensor(2013), tensor(3036), tensor(2911), tensor(330), tensor(3690), tensor(2913), tensor(539), tensor(3426), tensor(3775), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(2484), tensor(631), tensor(291), tensor(889), tensor(685), tensor(2632), tensor(823), tensor(289), tensor(1601), tensor(236), tensor(2269), tensor(3407), tensor(338), tensor(3383), tensor(681), tensor(2051), tensor(1842), tensor(1186), tensor(511), tensor(905), tensor(1883), tensor(3558), tensor(2970), tensor(3425), tensor(2753), tensor(1871), tensor(2019), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(2106), tensor(3292), tensor(4448), tensor(4032), tensor(3579), tensor(181), tensor(2099), tensor(3506), tensor(4121), tensor(603), tensor(1767), tensor(3509), tensor(683), tensor(3956), tensor(2080), tensor(3207), tensor(370), tensor(2867), tensor(2136), tensor(3291)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5903, Accuracy: 0.7412\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5762, Accuracy: 0.7588\n",
            "Epoch [3/15], Loss: 1.5791, Accuracy: 0.7510\n",
            "Epoch [4/15], Loss: 1.5584, Accuracy: 0.7549\n",
            "Epoch [5/15], Loss: 1.5506, Accuracy: 0.7863\n",
            "Epoch [6/15], Loss: 1.5409, Accuracy: 0.7725\n",
            "Epoch [7/15], Loss: 1.5447, Accuracy: 0.7843\n",
            "Epoch [8/15], Loss: 1.5347, Accuracy: 0.8000\n",
            "Epoch [9/15], Loss: 1.5358, Accuracy: 0.8059\n",
            "Epoch [10/15], Loss: 1.5145, Accuracy: 0.8137\n",
            "Epoch [11/15], Loss: 1.5237, Accuracy: 0.8118\n",
            "Epoch [12/15], Loss: 1.5112, Accuracy: 0.8078\n",
            "Epoch [13/15], Loss: 1.5079, Accuracy: 0.8098\n",
            "Epoch [14/15], Loss: 1.5094, Accuracy: 0.8314\n",
            "Epoch [15/15], Loss: 1.5135, Accuracy: 0.8196\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 8 ---------\n",
            "[tensor(1923), tensor(134), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(2536), tensor(703), tensor(993), tensor(931), tensor(1491), tensor(193), tensor(4501), tensor(3271), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(488), tensor(3188), tensor(288), tensor(2013), tensor(2911), tensor(3690), tensor(539), tensor(3775), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(685), tensor(2632), tensor(823), tensor(289), tensor(1601), tensor(236), tensor(2269), tensor(3407), tensor(3383), tensor(1186), tensor(2970), tensor(3425), tensor(2753), tensor(1871), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(3579), tensor(181), tensor(2099), tensor(3506), tensor(603), tensor(1767), tensor(3509), tensor(683), tensor(3956), tensor(2080), tensor(370), tensor(2136), tensor(4364), tensor(1704), tensor(313), tensor(2279), tensor(533), tensor(3650), tensor(2176), tensor(119), tensor(2085), tensor(3586), tensor(3453), tensor(54), tensor(2515), tensor(1725), tensor(3084), tensor(1081), tensor(700), tensor(2047), tensor(587), tensor(3540), tensor(253)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5626, Accuracy: 0.7649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5282, Accuracy: 0.8018\n",
            "Epoch [3/15], Loss: 1.5268, Accuracy: 0.7860\n",
            "Epoch [4/15], Loss: 1.5412, Accuracy: 0.7807\n",
            "Epoch [5/15], Loss: 1.5321, Accuracy: 0.7895\n",
            "Epoch [6/15], Loss: 1.5317, Accuracy: 0.8053\n",
            "Epoch [7/15], Loss: 1.5278, Accuracy: 0.8140\n",
            "Epoch [8/15], Loss: 1.4979, Accuracy: 0.8263\n",
            "Epoch [9/15], Loss: 1.5108, Accuracy: 0.8333\n",
            "Epoch [10/15], Loss: 1.5122, Accuracy: 0.8158\n",
            "Epoch [11/15], Loss: 1.5027, Accuracy: 0.8333\n",
            "Epoch [12/15], Loss: 1.5087, Accuracy: 0.8351\n",
            "Epoch [13/15], Loss: 1.5060, Accuracy: 0.8333\n",
            "Epoch [14/15], Loss: 1.4974, Accuracy: 0.8474\n",
            "Epoch [15/15], Loss: 1.5015, Accuracy: 0.8316\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 9 ---------\n",
            "[tensor(1923), tensor(134), tensor(2981), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(2536), tensor(703), tensor(931), tensor(1491), tensor(4501), tensor(3271), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(488), tensor(2013), tensor(2911), tensor(539), tensor(3775), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(2632), tensor(1601), tensor(1186), tensor(2970), tensor(3425), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(2080), tensor(370), tensor(2136), tensor(4364), tensor(313), tensor(2279), tensor(3650), tensor(2176), tensor(3453), tensor(54), tensor(1725), tensor(1081), tensor(700), tensor(587), tensor(3540), tensor(2763), tensor(253), tensor(395), tensor(2039), tensor(147), tensor(1938), tensor(1252), tensor(655), tensor(4385), tensor(3598), tensor(4014), tensor(2873), tensor(1065), tensor(4086), tensor(1204), tensor(706)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5350, Accuracy: 0.7952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5157, Accuracy: 0.8079\n",
            "Epoch [3/15], Loss: 1.5230, Accuracy: 0.7921\n",
            "Epoch [4/15], Loss: 1.5107, Accuracy: 0.8127\n",
            "Epoch [5/15], Loss: 1.5040, Accuracy: 0.8048\n",
            "Epoch [6/15], Loss: 1.5034, Accuracy: 0.8270\n",
            "Epoch [7/15], Loss: 1.5133, Accuracy: 0.8111\n",
            "Epoch [8/15], Loss: 1.5228, Accuracy: 0.8000\n",
            "Epoch [9/15], Loss: 1.4950, Accuracy: 0.8333\n",
            "Epoch [10/15], Loss: 1.4989, Accuracy: 0.8333\n",
            "Epoch [11/15], Loss: 1.4955, Accuracy: 0.8302\n",
            "Epoch [12/15], Loss: 1.4941, Accuracy: 0.8365\n",
            "Epoch [13/15], Loss: 1.4940, Accuracy: 0.8444\n",
            "Epoch [14/15], Loss: 1.5136, Accuracy: 0.8238\n",
            "Epoch [15/15], Loss: 1.4980, Accuracy: 0.8429\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 10 ---------\n",
            "[tensor(1923), tensor(134), tensor(2492), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(2536), tensor(703), tensor(993), tensor(931), tensor(1491), tensor(4501), tensor(3271), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(488), tensor(2013), tensor(2911), tensor(539), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(2632), tensor(823), tensor(1601), tensor(1186), tensor(2970), tensor(3425), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(2080), tensor(3207), tensor(370), tensor(2136), tensor(4364), tensor(313), tensor(2279), tensor(3650), tensor(2176), tensor(3453), tensor(1725), tensor(1081), tensor(587), tensor(253), tensor(1938), tensor(1252), tensor(655), tensor(4014), tensor(1065), tensor(4086), tensor(2519), tensor(706), tensor(3115), tensor(2914), tensor(2922), tensor(2794), tensor(3436), tensor(516), tensor(1215), tensor(2946), tensor(3533), tensor(128), tensor(4083), tensor(2777), tensor(2121), tensor(251), tensor(366), tensor(2616), tensor(3199)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5298, Accuracy: 0.8000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5261, Accuracy: 0.7986\n",
            "Epoch [3/15], Loss: 1.5308, Accuracy: 0.8058\n",
            "Epoch [4/15], Loss: 1.5266, Accuracy: 0.8159\n",
            "Epoch [5/15], Loss: 1.5204, Accuracy: 0.7971\n",
            "Epoch [6/15], Loss: 1.5115, Accuracy: 0.8159\n",
            "Epoch [7/15], Loss: 1.5171, Accuracy: 0.8130\n",
            "Epoch [8/15], Loss: 1.5466, Accuracy: 0.7754\n",
            "Epoch [9/15], Loss: 1.5130, Accuracy: 0.8159\n",
            "Epoch [10/15], Loss: 1.5158, Accuracy: 0.8101\n",
            "Epoch [11/15], Loss: 1.5186, Accuracy: 0.8072\n",
            "Epoch [12/15], Loss: 1.4970, Accuracy: 0.8478\n",
            "Epoch [13/15], Loss: 1.5034, Accuracy: 0.8275\n",
            "Epoch [14/15], Loss: 1.4996, Accuracy: 0.8333\n",
            "Epoch [15/15], Loss: 1.5118, Accuracy: 0.8101\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 11 ---------\n",
            "[tensor(1923), tensor(134), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(2536), tensor(703), tensor(993), tensor(931), tensor(1491), tensor(4501), tensor(3271), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(488), tensor(2013), tensor(539), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(2632), tensor(823), tensor(1601), tensor(1186), tensor(2970), tensor(3425), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(2080), tensor(370), tensor(2136), tensor(4364), tensor(313), tensor(2279), tensor(3650), tensor(2176), tensor(3453), tensor(1725), tensor(1081), tensor(700), tensor(587), tensor(253), tensor(1938), tensor(1252), tensor(655), tensor(4385), tensor(4014), tensor(1065), tensor(4086), tensor(1204), tensor(2519), tensor(706), tensor(3115), tensor(2914), tensor(2922), tensor(2794), tensor(1795), tensor(3436), tensor(516), tensor(1215), tensor(2946), tensor(3533), tensor(128), tensor(4083), tensor(2777), tensor(2121), tensor(366), tensor(2616), tensor(3199), tensor(2478), tensor(3027), tensor(2466), tensor(2248), tensor(1640), tensor(1285), tensor(3721), tensor(4274), tensor(2003), tensor(2277), tensor(367), tensor(1136), tensor(3758), tensor(986), tensor(3068), tensor(3244), tensor(4276), tensor(3588), tensor(35), tensor(927), tensor(1320), tensor(2425), tensor(552), tensor(3501), tensor(556), tensor(1558), tensor(2969), tensor(997), tensor(2535), tensor(265), tensor(910)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5671, Accuracy: 0.7653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5455, Accuracy: 0.7813\n",
            "Epoch [3/15], Loss: 1.5873, Accuracy: 0.7080\n",
            "Epoch [4/15], Loss: 1.5576, Accuracy: 0.7733\n",
            "Epoch [5/15], Loss: 1.5659, Accuracy: 0.7520\n",
            "Epoch [6/15], Loss: 1.5346, Accuracy: 0.7787\n",
            "Epoch [7/15], Loss: 1.5240, Accuracy: 0.7933\n",
            "Epoch [8/15], Loss: 1.5208, Accuracy: 0.8053\n",
            "Epoch [9/15], Loss: 1.5433, Accuracy: 0.7840\n",
            "Epoch [10/15], Loss: 1.5615, Accuracy: 0.7573\n",
            "Epoch [11/15], Loss: 1.5320, Accuracy: 0.7733\n",
            "Epoch [12/15], Loss: 1.5114, Accuracy: 0.8213\n",
            "Epoch [13/15], Loss: 1.5084, Accuracy: 0.8120\n",
            "Epoch [14/15], Loss: 1.5011, Accuracy: 0.8253\n",
            "Epoch [15/15], Loss: 1.5153, Accuracy: 0.8173\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 12 ---------\n",
            "[tensor(1923), tensor(134), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(2536), tensor(703), tensor(931), tensor(1491), tensor(4501), tensor(3271), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(488), tensor(2013), tensor(539), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(2632), tensor(1601), tensor(3425), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(2080), tensor(2136), tensor(4364), tensor(313), tensor(2279), tensor(3650), tensor(2176), tensor(3453), tensor(1725), tensor(1081), tensor(587), tensor(1938), tensor(655), tensor(4014), tensor(1065), tensor(4086), tensor(1204), tensor(2519), tensor(706), tensor(3115), tensor(2914), tensor(2922), tensor(2794), tensor(3436), tensor(516), tensor(2946), tensor(128), tensor(4083), tensor(2777), tensor(2121), tensor(366), tensor(2616), tensor(3199), tensor(3027), tensor(2248), tensor(1640), tensor(1285), tensor(3721), tensor(4274), tensor(2003), tensor(367), tensor(1136), tensor(3758), tensor(986), tensor(3068), tensor(3588), tensor(927), tensor(1320), tensor(2425), tensor(552), tensor(3501), tensor(1558), tensor(997), tensor(2535), tensor(265), tensor(910), tensor(2381), tensor(431), tensor(2469), tensor(27), tensor(2180), tensor(4464), tensor(586), tensor(1399), tensor(74), tensor(2595), tensor(2908), tensor(697), tensor(1024), tensor(2604), tensor(3044), tensor(3184), tensor(1212), tensor(469), tensor(3592), tensor(117), tensor(2543), tensor(2900), tensor(1)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5209, Accuracy: 0.7988\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5442, Accuracy: 0.7716\n",
            "Epoch [3/15], Loss: 1.5724, Accuracy: 0.7395\n",
            "Epoch [4/15], Loss: 1.5405, Accuracy: 0.7815\n",
            "Epoch [5/15], Loss: 1.5325, Accuracy: 0.7852\n",
            "Epoch [6/15], Loss: 1.5282, Accuracy: 0.8025\n",
            "Epoch [7/15], Loss: 1.5370, Accuracy: 0.7938\n",
            "Epoch [8/15], Loss: 1.5412, Accuracy: 0.7889\n",
            "Epoch [9/15], Loss: 1.5355, Accuracy: 0.7914\n",
            "Epoch [10/15], Loss: 1.5285, Accuracy: 0.8086\n",
            "Epoch [11/15], Loss: 1.5173, Accuracy: 0.7951\n",
            "Epoch [12/15], Loss: 1.5129, Accuracy: 0.8049\n",
            "Epoch [13/15], Loss: 1.5123, Accuracy: 0.7975\n",
            "Epoch [14/15], Loss: 1.5088, Accuracy: 0.8123\n",
            "Epoch [15/15], Loss: 1.5085, Accuracy: 0.8012\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 13 ---------\n",
            "[tensor(1923), tensor(134), tensor(2620), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(2536), tensor(703), tensor(931), tensor(1491), tensor(4501), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(488), tensor(3188), tensor(2013), tensor(539), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(2632), tensor(823), tensor(289), tensor(1601), tensor(3425), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(687), tensor(2080), tensor(2136), tensor(4364), tensor(313), tensor(2279), tensor(2176), tensor(3453), tensor(1081), tensor(587), tensor(1938), tensor(655), tensor(4014), tensor(1065), tensor(4086), tensor(1204), tensor(706), tensor(3115), tensor(2922), tensor(2794), tensor(3436), tensor(516), tensor(2946), tensor(128), tensor(4083), tensor(2121), tensor(366), tensor(2616), tensor(3199), tensor(3027), tensor(2248), tensor(1640), tensor(1285), tensor(3721), tensor(2277), tensor(367), tensor(1136), tensor(328), tensor(3758), tensor(3068), tensor(1320), tensor(2425), tensor(3501), tensor(2535), tensor(265), tensor(910), tensor(431), tensor(2469), tensor(275), tensor(586), tensor(1399), tensor(74), tensor(2595), tensor(4398), tensor(2908), tensor(697), tensor(1024), tensor(2604), tensor(3184), tensor(1212), tensor(469), tensor(3592), tensor(117), tensor(1), tensor(4255), tensor(4481), tensor(2802), tensor(4337), tensor(1311), tensor(2493), tensor(3949), tensor(1672), tensor(2567), tensor(3194), tensor(555), tensor(3447), tensor(404), tensor(4250), tensor(2387), tensor(2685), tensor(2408), tensor(2760), tensor(728), tensor(3421), tensor(1470)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5359, Accuracy: 0.7839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5474, Accuracy: 0.7724\n",
            "Epoch [3/15], Loss: 1.5514, Accuracy: 0.7782\n",
            "Epoch [4/15], Loss: 1.5710, Accuracy: 0.7517\n",
            "Epoch [5/15], Loss: 1.5609, Accuracy: 0.7782\n",
            "Epoch [6/15], Loss: 1.5513, Accuracy: 0.7851\n",
            "Epoch [7/15], Loss: 1.5393, Accuracy: 0.7747\n",
            "Epoch [8/15], Loss: 1.5376, Accuracy: 0.7885\n",
            "Epoch [9/15], Loss: 1.5380, Accuracy: 0.7793\n",
            "Epoch [10/15], Loss: 1.5526, Accuracy: 0.7609\n",
            "Epoch [11/15], Loss: 1.5876, Accuracy: 0.7149\n",
            "Epoch [12/15], Loss: 1.5105, Accuracy: 0.8184\n",
            "Epoch [13/15], Loss: 1.5207, Accuracy: 0.8103\n",
            "Epoch [14/15], Loss: 1.5172, Accuracy: 0.8195\n",
            "Epoch [15/15], Loss: 1.5356, Accuracy: 0.7828\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 14 ---------\n",
            "[tensor(134), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(2536), tensor(703), tensor(993), tensor(931), tensor(1491), tensor(4501), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(488), tensor(2013), tensor(539), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(685), tensor(2632), tensor(1601), tensor(3425), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(2080), tensor(2136), tensor(4364), tensor(313), tensor(2279), tensor(3650), tensor(2176), tensor(3453), tensor(1081), tensor(587), tensor(1938), tensor(655), tensor(4014), tensor(2873), tensor(1065), tensor(4086), tensor(1204), tensor(706), tensor(3115), tensor(2922), tensor(2794), tensor(3436), tensor(516), tensor(2946), tensor(1110), tensor(128), tensor(4083), tensor(2777), tensor(2121), tensor(366), tensor(2616), tensor(3199), tensor(3027), tensor(2248), tensor(1640), tensor(1285), tensor(3721), tensor(4274), tensor(367), tensor(1136), tensor(3758), tensor(986), tensor(3068), tensor(1320), tensor(2425), tensor(3501), tensor(1558), tensor(997), tensor(2535), tensor(265), tensor(910), tensor(431), tensor(27), tensor(2180), tensor(586), tensor(1399), tensor(74), tensor(2595), tensor(2908), tensor(697), tensor(1024), tensor(2604), tensor(3044), tensor(3184), tensor(1212), tensor(469), tensor(3592), tensor(117), tensor(2543), tensor(2900), tensor(1), tensor(4255), tensor(2802), tensor(4337), tensor(1311), tensor(2493), tensor(3949), tensor(1672), tensor(2567), tensor(3194), tensor(555), tensor(3675), tensor(3447), tensor(404), tensor(4250), tensor(2387), tensor(2685), tensor(2408), tensor(2760), tensor(728), tensor(3421), tensor(4235), tensor(4209), tensor(1470), tensor(2658), tensor(653), tensor(4407), tensor(2732), tensor(436), tensor(906), tensor(3836), tensor(2201), tensor(3877), tensor(2793), tensor(2043), tensor(2810), tensor(3646), tensor(2064), tensor(1570), tensor(1008), tensor(3760), tensor(4288), tensor(2624), tensor(1573), tensor(3554), tensor(1314), tensor(2219), tensor(3055), tensor(3866), tensor(1887)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5656, Accuracy: 0.7656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6498, Accuracy: 0.6215\n",
            "Epoch [3/15], Loss: 1.5616, Accuracy: 0.7473\n",
            "Epoch [4/15], Loss: 1.6692, Accuracy: 0.6011\n",
            "Epoch [5/15], Loss: 1.6375, Accuracy: 0.6161\n",
            "Epoch [6/15], Loss: 1.7472, Accuracy: 0.4333\n",
            "Epoch [7/15], Loss: 1.6419, Accuracy: 0.5978\n",
            "Epoch [8/15], Loss: 1.6284, Accuracy: 0.6441\n",
            "Epoch [9/15], Loss: 1.6216, Accuracy: 0.6355\n",
            "Epoch [10/15], Loss: 1.7143, Accuracy: 0.5290\n",
            "Epoch [11/15], Loss: 1.6229, Accuracy: 0.6806\n",
            "Epoch [12/15], Loss: 1.6442, Accuracy: 0.6763\n",
            "Epoch [13/15], Loss: 1.5747, Accuracy: 0.7419\n",
            "Epoch [14/15], Loss: 1.6271, Accuracy: 0.6645\n",
            "Epoch [15/15], Loss: 1.5673, Accuracy: 0.7720\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 15 ---------\n",
            "[tensor(3038), tensor(134), tensor(3175), tensor(2981), tensor(714), tensor(3157), tensor(1700), tensor(1207), tensor(1727), tensor(2536), tensor(703), tensor(931), tensor(1491), tensor(4501), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(488), tensor(2013), tensor(539), tensor(2813), tensor(4056), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(2632), tensor(1601), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(2080), tensor(2136), tensor(4364), tensor(313), tensor(2279), tensor(3650), tensor(2176), tensor(3453), tensor(3071), tensor(1081), tensor(587), tensor(3540), tensor(253), tensor(1938), tensor(4014), tensor(1065), tensor(4086), tensor(1204), tensor(2519), tensor(706), tensor(3115), tensor(2922), tensor(2794), tensor(3436), tensor(516), tensor(2946), tensor(128), tensor(4083), tensor(2121), tensor(366), tensor(2616), tensor(3199), tensor(2248), tensor(1640), tensor(1285), tensor(367), tensor(1136), tensor(3068), tensor(1320), tensor(2425), tensor(3501), tensor(997), tensor(2535), tensor(265), tensor(910), tensor(431), tensor(27), tensor(2180), tensor(586), tensor(1399), tensor(74), tensor(2595), tensor(2908), tensor(697), tensor(2604), tensor(3184), tensor(1212), tensor(469), tensor(3592), tensor(117), tensor(1), tensor(4255), tensor(4481), tensor(2802), tensor(4337), tensor(1311), tensor(2493), tensor(3949), tensor(3131), tensor(1672), tensor(2567), tensor(3194), tensor(555), tensor(404), tensor(2387), tensor(2408), tensor(2760), tensor(728), tensor(3421), tensor(1470), tensor(2658), tensor(653), tensor(4407), tensor(906), tensor(2201), tensor(3877), tensor(2793), tensor(2043), tensor(2064), tensor(1008), tensor(2624), tensor(1314), tensor(3055), tensor(3866), tensor(1887), tensor(4173), tensor(4166), tensor(966), tensor(3132), tensor(442), tensor(3486), tensor(2023), tensor(2164), tensor(1618), tensor(3089), tensor(4171), tensor(1375), tensor(4324), tensor(2701), tensor(3350), tensor(1524), tensor(1959), tensor(3794), tensor(1082), tensor(2759), tensor(4353), tensor(1506), tensor(2422), tensor(3173)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5416, Accuracy: 0.7717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5562, Accuracy: 0.7505\n",
            "Epoch [3/15], Loss: 1.5490, Accuracy: 0.7697\n",
            "Epoch [4/15], Loss: 1.5406, Accuracy: 0.7768\n",
            "Epoch [5/15], Loss: 1.5326, Accuracy: 0.7848\n",
            "Epoch [6/15], Loss: 1.5254, Accuracy: 0.7980\n",
            "Epoch [7/15], Loss: 1.5392, Accuracy: 0.7949\n",
            "Epoch [8/15], Loss: 1.5368, Accuracy: 0.7768\n",
            "Epoch [9/15], Loss: 1.5164, Accuracy: 0.8152\n",
            "Epoch [10/15], Loss: 1.5118, Accuracy: 0.8182\n",
            "Epoch [11/15], Loss: 1.5238, Accuracy: 0.8010\n",
            "Epoch [12/15], Loss: 1.5142, Accuracy: 0.8131\n",
            "Epoch [13/15], Loss: 1.5430, Accuracy: 0.7616\n",
            "Epoch [14/15], Loss: 1.5260, Accuracy: 0.7960\n",
            "Epoch [15/15], Loss: 1.5173, Accuracy: 0.8081\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 16 ---------\n",
            "[tensor(134), tensor(3175), tensor(2981), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(1727), tensor(2536), tensor(703), tensor(931), tensor(1491), tensor(4501), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(814), tensor(488), tensor(2013), tensor(539), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(2632), tensor(1601), tensor(3425), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1267), tensor(1583), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(2080), tensor(2136), tensor(4364), tensor(313), tensor(2279), tensor(3650), tensor(2176), tensor(3453), tensor(1081), tensor(587), tensor(1938), tensor(655), tensor(4014), tensor(2873), tensor(1065), tensor(4086), tensor(1204), tensor(2519), tensor(706), tensor(2922), tensor(2794), tensor(516), tensor(2946), tensor(1110), tensor(128), tensor(4083), tensor(2777), tensor(2121), tensor(251), tensor(366), tensor(2616), tensor(3199), tensor(3027), tensor(2248), tensor(1640), tensor(1285), tensor(367), tensor(1136), tensor(3758), tensor(3068), tensor(2425), tensor(3501), tensor(2535), tensor(265), tensor(910), tensor(431), tensor(2469), tensor(27), tensor(2180), tensor(586), tensor(1399), tensor(74), tensor(2595), tensor(2908), tensor(697), tensor(2604), tensor(3184), tensor(1212), tensor(469), tensor(3592), tensor(117), tensor(1), tensor(4255), tensor(4481), tensor(2802), tensor(4337), tensor(1311), tensor(2493), tensor(3949), tensor(1672), tensor(2567), tensor(3194), tensor(555), tensor(3447), tensor(404), tensor(2387), tensor(2408), tensor(2760), tensor(728), tensor(1470), tensor(2658), tensor(653), tensor(4407), tensor(906), tensor(2201), tensor(3877), tensor(2793), tensor(2043), tensor(2064), tensor(1008), tensor(2624), tensor(1573), tensor(1314), tensor(3055), tensor(3866), tensor(4173), tensor(4166), tensor(966), tensor(3132), tensor(442), tensor(2023), tensor(2164), tensor(1618), tensor(4171), tensor(1375), tensor(4324), tensor(2701), tensor(3350), tensor(1536), tensor(1082), tensor(2759), tensor(4353), tensor(1506), tensor(2422), tensor(3173), tensor(2808), tensor(3468), tensor(583), tensor(2647), tensor(1194), tensor(1775), tensor(1956), tensor(2772), tensor(11), tensor(869), tensor(4015), tensor(1607), tensor(1978), tensor(2323), tensor(3627), tensor(45), tensor(1988), tensor(3351), tensor(4306), tensor(598), tensor(2293), tensor(2340), tensor(3147), tensor(1104)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5518, Accuracy: 0.7705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5424, Accuracy: 0.7810\n",
            "Epoch [3/15], Loss: 1.5601, Accuracy: 0.7619\n",
            "Epoch [4/15], Loss: 1.5381, Accuracy: 0.7867\n",
            "Epoch [5/15], Loss: 1.5377, Accuracy: 0.7714\n",
            "Epoch [6/15], Loss: 1.5321, Accuracy: 0.7933\n",
            "Epoch [7/15], Loss: 1.5451, Accuracy: 0.7771\n",
            "Epoch [8/15], Loss: 1.5345, Accuracy: 0.7914\n",
            "Epoch [9/15], Loss: 1.5330, Accuracy: 0.7867\n",
            "Epoch [10/15], Loss: 1.5226, Accuracy: 0.8076\n",
            "Epoch [11/15], Loss: 1.5343, Accuracy: 0.7905\n",
            "Epoch [12/15], Loss: 1.5267, Accuracy: 0.7962\n",
            "Epoch [13/15], Loss: 1.5392, Accuracy: 0.7800\n",
            "Epoch [14/15], Loss: 1.5388, Accuracy: 0.7952\n",
            "Epoch [15/15], Loss: 1.5480, Accuracy: 0.7771\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 17 ---------\n",
            "[tensor(134), tensor(4249), tensor(2620), tensor(2981), tensor(2492), tensor(714), tensor(3157), tensor(1700), tensor(1207), tensor(1727), tensor(2536), tensor(703), tensor(931), tensor(1491), tensor(4501), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(488), tensor(2013), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(2632), tensor(1601), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(2080), tensor(2136), tensor(4364), tensor(313), tensor(2279), tensor(3650), tensor(2176), tensor(3453), tensor(1081), tensor(587), tensor(1938), tensor(4014), tensor(1065), tensor(4086), tensor(2519), tensor(706), tensor(3115), tensor(2922), tensor(2794), tensor(516), tensor(2946), tensor(128), tensor(4083), tensor(251), tensor(366), tensor(2616), tensor(3199), tensor(2248), tensor(1640), tensor(1285), tensor(1136), tensor(3758), tensor(3068), tensor(1320), tensor(2425), tensor(3501), tensor(2535), tensor(265), tensor(910), tensor(431), tensor(2469), tensor(586), tensor(1399), tensor(74), tensor(2595), tensor(2908), tensor(697), tensor(1024), tensor(2604), tensor(3184), tensor(1212), tensor(469), tensor(3592), tensor(117), tensor(1), tensor(4255), tensor(2802), tensor(4337), tensor(1311), tensor(2493), tensor(3949), tensor(1672), tensor(2567), tensor(3194), tensor(555), tensor(404), tensor(2387), tensor(2408), tensor(2760), tensor(728), tensor(1470), tensor(2658), tensor(653), tensor(4407), tensor(906), tensor(2201), tensor(3877), tensor(2793), tensor(2043), tensor(2064), tensor(3760), tensor(4288), tensor(2624), tensor(3055), tensor(3866), tensor(4173), tensor(4166), tensor(966), tensor(3132), tensor(442), tensor(3486), tensor(2023), tensor(2164), tensor(1618), tensor(786), tensor(4171), tensor(4387), tensor(1375), tensor(4324), tensor(2701), tensor(3350), tensor(1536), tensor(4008), tensor(1082), tensor(2759), tensor(4353), tensor(1506), tensor(2422), tensor(3173), tensor(2808), tensor(1768), tensor(3468), tensor(583), tensor(2647), tensor(1194), tensor(1775), tensor(1956), tensor(2772), tensor(11), tensor(869), tensor(4015), tensor(1607), tensor(1978), tensor(2323), tensor(3627), tensor(3712), tensor(45), tensor(1988), tensor(4306), tensor(598), tensor(3147), tensor(1104), tensor(1880), tensor(868), tensor(322), tensor(3051), tensor(979), tensor(3236), tensor(2972), tensor(3703), tensor(2169), tensor(866), tensor(2199), tensor(637), tensor(76), tensor(3098), tensor(856), tensor(2365), tensor(4002), tensor(1519)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5611, Accuracy: 0.7595\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5528, Accuracy: 0.7649\n",
            "Epoch [3/15], Loss: 1.5716, Accuracy: 0.7459\n",
            "Epoch [4/15], Loss: 1.5625, Accuracy: 0.7595\n",
            "Epoch [5/15], Loss: 1.5541, Accuracy: 0.7685\n",
            "Epoch [6/15], Loss: 1.5389, Accuracy: 0.7811\n",
            "Epoch [7/15], Loss: 1.5591, Accuracy: 0.7622\n",
            "Epoch [8/15], Loss: 1.5488, Accuracy: 0.7694\n",
            "Epoch [9/15], Loss: 1.5577, Accuracy: 0.7622\n",
            "Epoch [10/15], Loss: 1.5234, Accuracy: 0.7991\n",
            "Epoch [11/15], Loss: 1.5324, Accuracy: 0.8000\n",
            "Epoch [12/15], Loss: 1.5512, Accuracy: 0.7649\n",
            "Epoch [13/15], Loss: 1.5348, Accuracy: 0.7946\n",
            "Epoch [14/15], Loss: 1.5413, Accuracy: 0.7838\n",
            "Epoch [15/15], Loss: 1.5338, Accuracy: 0.7883\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 18 ---------\n",
            "[tensor(134), tensor(2981), tensor(714), tensor(3157), tensor(2798), tensor(1700), tensor(1207), tensor(1727), tensor(2536), tensor(1797), tensor(703), tensor(993), tensor(931), tensor(1491), tensor(4501), tensor(1339), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(814), tensor(488), tensor(2013), tensor(539), tensor(2813), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(2632), tensor(1601), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(1583), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(2080), tensor(4364), tensor(313), tensor(2279), tensor(3650), tensor(2176), tensor(3453), tensor(1081), tensor(587), tensor(1938), tensor(4014), tensor(1065), tensor(4086), tensor(2519), tensor(706), tensor(2922), tensor(2794), tensor(516), tensor(2946), tensor(128), tensor(4083), tensor(2121), tensor(366), tensor(2616), tensor(3199), tensor(2248), tensor(1640), tensor(1285), tensor(367), tensor(1136), tensor(3758), tensor(3068), tensor(1320), tensor(2425), tensor(3501), tensor(2535), tensor(265), tensor(910), tensor(431), tensor(27), tensor(2180), tensor(586), tensor(1399), tensor(74), tensor(2595), tensor(2908), tensor(697), tensor(1024), tensor(2604), tensor(3184), tensor(1212), tensor(469), tensor(3592), tensor(117), tensor(1), tensor(4255), tensor(2802), tensor(4337), tensor(1311), tensor(2493), tensor(3949), tensor(1672), tensor(2567), tensor(3194), tensor(555), tensor(404), tensor(2387), tensor(2408), tensor(2760), tensor(728), tensor(1470), tensor(2658), tensor(653), tensor(4407), tensor(906), tensor(2201), tensor(3877), tensor(2793), tensor(2043), tensor(3646), tensor(2064), tensor(1570), tensor(4288), tensor(2624), tensor(1573), tensor(3055), tensor(3866), tensor(4173), tensor(4166), tensor(966), tensor(1116), tensor(3132), tensor(2023), tensor(2164), tensor(1618), tensor(786), tensor(4171), tensor(1375), tensor(4324), tensor(2701), tensor(3350), tensor(1536), tensor(1082), tensor(1178), tensor(2759), tensor(4353), tensor(3155), tensor(1506), tensor(2422), tensor(3173), tensor(2808), tensor(3468), tensor(583), tensor(2647), tensor(1194), tensor(1775), tensor(1956), tensor(2772), tensor(11), tensor(869), tensor(4015), tensor(1607), tensor(1978), tensor(2323), tensor(3627), tensor(3712), tensor(45), tensor(1988), tensor(4306), tensor(598), tensor(2293), tensor(1104), tensor(1880), tensor(868), tensor(322), tensor(3051), tensor(3236), tensor(2972), tensor(3703), tensor(2169), tensor(866), tensor(2199), tensor(637), tensor(76), tensor(3098), tensor(856), tensor(2365), tensor(4002), tensor(2741), tensor(775), tensor(1014), tensor(1485), tensor(3273), tensor(2787), tensor(3133), tensor(3989), tensor(3827), tensor(2664), tensor(2292), tensor(3604), tensor(2801), tensor(2740), tensor(3697), tensor(2603), tensor(759), tensor(4000), tensor(740), tensor(1623), tensor(292), tensor(2123), tensor(2667), tensor(1185), tensor(716), tensor(1426), tensor(1545)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5663, Accuracy: 0.7547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5490, Accuracy: 0.7718\n",
            "Epoch [3/15], Loss: 1.5458, Accuracy: 0.7761\n",
            "Epoch [4/15], Loss: 1.5681, Accuracy: 0.7573\n",
            "Epoch [5/15], Loss: 1.5789, Accuracy: 0.7333\n",
            "Epoch [6/15], Loss: 1.5481, Accuracy: 0.7658\n",
            "Epoch [7/15], Loss: 1.5377, Accuracy: 0.7752\n",
            "Epoch [8/15], Loss: 1.5263, Accuracy: 0.7949\n",
            "Epoch [9/15], Loss: 1.5392, Accuracy: 0.7795\n",
            "Epoch [10/15], Loss: 1.5256, Accuracy: 0.7923\n",
            "Epoch [11/15], Loss: 1.5279, Accuracy: 0.8000\n",
            "Epoch [12/15], Loss: 1.5282, Accuracy: 0.7915\n",
            "Epoch [13/15], Loss: 1.5464, Accuracy: 0.7624\n",
            "Epoch [14/15], Loss: 1.5534, Accuracy: 0.7598\n",
            "Epoch [15/15], Loss: 1.5210, Accuracy: 0.8009\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 19 ---------\n",
            "[tensor(134), tensor(3175), tensor(2981), tensor(2991), tensor(714), tensor(3157), tensor(486), tensor(857), tensor(2798), tensor(1207), tensor(2536), tensor(703), tensor(993), tensor(931), tensor(1491), tensor(4501), tensor(1339), tensor(1380), tensor(3358), tensor(2265), tensor(3847), tensor(2305), tensor(814), tensor(2154), tensor(488), tensor(1282), tensor(2013), tensor(539), tensor(2813), tensor(835), tensor(4056), tensor(52), tensor(3086), tensor(2689), tensor(2163), tensor(631), tensor(291), tensor(2632), tensor(1601), tensor(3425), tensor(2753), tensor(1748), tensor(424), tensor(1522), tensor(224), tensor(181), tensor(2099), tensor(603), tensor(3747), tensor(1767), tensor(683), tensor(2080), tensor(4364), tensor(313), tensor(2279), tensor(3650), tensor(2176), tensor(3453), tensor(1081), tensor(587), tensor(253), tensor(1938), tensor(4014), tensor(2873), tensor(1065), tensor(4086), tensor(2519), tensor(706), tensor(2922), tensor(2794), tensor(516), tensor(4503), tensor(2946), tensor(128), tensor(4083), tensor(2121), tensor(366), tensor(2616), tensor(3199), tensor(2248), tensor(1640), tensor(1285), tensor(367), tensor(1136), tensor(3068), tensor(3588), tensor(2425), tensor(3501), tensor(2535), tensor(265), tensor(910), tensor(431), tensor(2180), tensor(586), tensor(74), tensor(2595), tensor(4398), tensor(2908), tensor(697), tensor(2604), tensor(3044), tensor(3184), tensor(1212), tensor(117), tensor(1), tensor(4255), tensor(4481), tensor(2802), tensor(4337), tensor(1311), tensor(2493), tensor(1878), tensor(3949), tensor(1672), tensor(2567), tensor(3194), tensor(555), tensor(3675), tensor(3447), tensor(404), tensor(2387), tensor(2408), tensor(2760), tensor(728), tensor(1470), tensor(2658), tensor(653), tensor(4407), tensor(906), tensor(2201), tensor(3877), tensor(2793), tensor(2043), tensor(2064), tensor(1570), tensor(2624), tensor(3554), tensor(1314), tensor(1230), tensor(3055), tensor(3866), tensor(1887), tensor(4173), tensor(4166), tensor(966), tensor(4367), tensor(3132), tensor(442), tensor(2023), tensor(2164), tensor(1618), tensor(4324), tensor(2701), tensor(3350), tensor(3794), tensor(3222), tensor(1082), tensor(4353), tensor(1506), tensor(2422), tensor(3173), tensor(2808), tensor(3468), tensor(583), tensor(2647), tensor(1194), tensor(1956), tensor(2772), tensor(11), tensor(869), tensor(4179), tensor(4015), tensor(1607), tensor(1978), tensor(3627), tensor(4306), tensor(598), tensor(2293), tensor(1880), tensor(868), tensor(322), tensor(175), tensor(3051), tensor(4046), tensor(3236), tensor(3703), tensor(2199), tensor(3981), tensor(3669), tensor(76), tensor(856), tensor(2365), tensor(4002), tensor(2401), tensor(2741), tensor(775), tensor(1014), tensor(1485), tensor(3133), tensor(3989), tensor(3827), tensor(2664), tensor(2292), tensor(2801), tensor(2603), tensor(759), tensor(4000), tensor(740), tensor(1623), tensor(292), tensor(208), tensor(1185), tensor(716), tensor(1545), tensor(2434), tensor(2319), tensor(456), tensor(4506), tensor(278), tensor(462), tensor(4101), tensor(84), tensor(4067), tensor(4098), tensor(3982), tensor(2456), tensor(926), tensor(2380), tensor(226)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5371, Accuracy: 0.7756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5397, Accuracy: 0.7813\n",
            "Epoch [3/15], Loss: 1.5355, Accuracy: 0.7813\n",
            "Epoch [4/15], Loss: 1.5558, Accuracy: 0.7569\n",
            "Epoch [5/15], Loss: 1.5409, Accuracy: 0.7821\n",
            "Epoch [6/15], Loss: 1.5655, Accuracy: 0.7390\n",
            "Epoch [7/15], Loss: 1.5344, Accuracy: 0.7951\n",
            "Epoch [8/15], Loss: 1.5436, Accuracy: 0.7748\n",
            "Epoch [9/15], Loss: 1.5346, Accuracy: 0.7789\n",
            "Epoch [10/15], Loss: 1.5297, Accuracy: 0.7870\n",
            "Epoch [11/15], Loss: 1.5177, Accuracy: 0.7919\n",
            "Epoch [12/15], Loss: 1.5177, Accuracy: 0.8041\n",
            "Epoch [13/15], Loss: 1.5170, Accuracy: 0.8016\n",
            "Epoch [14/15], Loss: 1.5276, Accuracy: 0.8033\n",
            "Epoch [15/15], Loss: 1.5142, Accuracy: 0.8098\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4004/1467103094.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for criterion in selection_criteria:\n",
        "    AL_class = ActiveLearningPipeline(model=model,\n",
        "                                      test_indices=test_indices,\n",
        "                                      available_pool_indices=available_pool_indices,\n",
        "                                      train_indices=train_indices,\n",
        "                                      selection_criterion=criterion,\n",
        "                                      iterations=iterations,\n",
        "                                      budget_per_iter=budget_per_iter,\n",
        "                                      num_epochs=num_epoch)\n",
        "    accuracy_scores_dict[criterion] = AL_class.run_pipeline()\n",
        "generate_plot(accuracy_scores_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5sKVVjSzEl7",
        "outputId": "2ec75b07-9550-4cc6-c3ca-d415c6dc9965"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'custom': [0.46825396825396826,\n",
              "              0.5727513227513227,\n",
              "              0.5932539682539683,\n",
              "              0.6203703703703703,\n",
              "              0.5978835978835979,\n",
              "              0.5866402116402116,\n",
              "              0.6183862433862434,\n",
              "              0.6084656084656084,\n",
              "              0.5773809523809523,\n",
              "              0.5939153439153438,\n",
              "              0.6243386243386243,\n",
              "              0.6263227513227513,\n",
              "              0.613095238095238,\n",
              "              0.6342592592592592,\n",
              "              0.6342592592592592,\n",
              "              0.6514550264550264,\n",
              "              0.6117724867724867,\n",
              "              0.6388888888888888,\n",
              "              0.6593915343915343,\n",
              "              0.6461640211640212]})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_scores_dict"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
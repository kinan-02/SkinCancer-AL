{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JjgyTgaRfy4k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import random, torch\n",
    "\n",
    "random.seed(0) # Set seed for NumPy\n",
    "np.random.seed(0) # Set seed for PyTorch (for both CPU and GPU)\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "train_df = pd.read_csv('train_dataset/metadata.csv')\n",
    "test_df = pd.read_csv('test_dataset/metadata.csv')\n",
    "val_df = pd.read_csv('validation_dataset/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "bcHCCahYj6B7",
    "outputId": "1facd164-3e8b-40f4-81d7-52fc3110e538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnosis\n",
      "nevus                         1205\n",
      "melanoma                      1113\n",
      "pigmented benign keratosis    1099\n",
      "basal cell carcinoma           514\n",
      "squamous cell carcinoma        197\n",
      "vascular lesion                142\n",
      "actinic keratosis              130\n",
      "dermatofibroma                 115\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actinic keratosis': 0,\n",
       " 'basal cell carcinoma': 1,\n",
       " 'dermatofibroma': 2,\n",
       " 'melanoma': 3,\n",
       " 'nevus': 4,\n",
       " 'pigmented benign keratosis': 5,\n",
       " 'squamous cell carcinoma': 6,\n",
       " 'vascular lesion': 7}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping = {\n",
    "    \"actinic keratosis\": 0,\n",
    "    \"basal cell carcinoma\": 1,\n",
    "    \"dermatofibroma\": 2,\n",
    "    \"melanoma\": 3,\n",
    "    \"nevus\": 4,\n",
    "    \"pigmented benign keratosis\": 5,\n",
    "    \"squamous cell carcinoma\": 6,\n",
    "    \"vascular lesion\":7\n",
    "}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BLruwetXmPem"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define image transformations (resize, convert to tensor, and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),          # Resize images to 224x224 (matching ResNet input size)\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, dataframe, transform, train='train'):\n",
    "        self.dataframe=dataframe\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.path_to_image=self._create_path_to_image_dict()\n",
    "        self.paths=list(self.path_to_image.keys())\n",
    "        self.labels=list(self.path_to_image.values())\n",
    "\n",
    "    def _create_path_to_image_dict(self):\n",
    "      path_to_image={}\n",
    "      for index,row in self.dataframe.iterrows():\n",
    "        if self.train == 'train':\n",
    "          img_path = os.path.join('train_dataset/',row['isic_id']+'.jpg')\n",
    "        elif self.train == 'test':\n",
    "          img_path = os.path.join('test_dataset/',row['isic_id']+'.jpg')\n",
    "        else:\n",
    "            img_path = os.path.join('validation_dataset/',row['isic_id']+'.jpg')\n",
    "        label=row['diagnosis']\n",
    "        path_to_image[img_path]=label\n",
    "      return path_to_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        img_path=self.paths[index]\n",
    "        img_label=self.labels[index]\n",
    "        image=Image.open(img_path)\n",
    "        image=self.transform(image)\n",
    "        if self.train == 'val':\n",
    "            return image, class_mapping[img_label], index\n",
    "        return image, img_label, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9yCvnprDoa_7"
   },
   "outputs": [],
   "source": [
    "train_df = Dataset(train_df, transform)\n",
    "val_df = Dataset(val_df, transform,train='val')\n",
    "test_df = Dataset(test_df, transform,train='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5L0NcpesnZI",
    "outputId": "22f97091-b571-4138-964f-95a509890583"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Load pre-trained ResNet50 model from torchvision\n",
    "base_model = models.resnet50(pretrained=True)\n",
    "\n",
    "num_classes = 8  # Adjust this based on your dataset\n",
    "base_model.fc = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(base_model.fc.in_features, 128),  # Add a fully connected layer\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, num_classes),  # Final layer with number of classes\n",
    "    nn.Softmax(dim=1)  # Softmax activation for multi-class classification\n",
    ")\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers except the fully connected ones\n",
    "\n",
    "# Unfreeze the final fully connected layer\n",
    "for param in base_model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# class_weights = torch.tensor([0.5, 2.0, 1.0, 0.8, 0.2, 3.0, 0.7, 1.2]).to(device)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(base_model.fc.parameters(), lr=0.0008)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "x = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 4  # Adjust based on your memory and hardware\n",
    "val_loader = DataLoader(val_df, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-Vk8kyZsgACa"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ActiveLearningPipeline:\n",
    "    def __init__(self, model,\n",
    "                 available_pool_indices,\n",
    "                 train_indices,\n",
    "                 test_indices,\n",
    "                 selection_criterion,\n",
    "                 iterations,\n",
    "                 budget_per_iter,\n",
    "                 num_epochs):\n",
    "        self.model = model\n",
    "        self.iterations = iterations\n",
    "        self.budget_per_iter = budget_per_iter\n",
    "        self.available_pool_indices = available_pool_indices\n",
    "        self.train_indices = train_indices\n",
    "        self.test_indices = test_indices\n",
    "        self.selection_criterion = selection_criterion\n",
    "        if self.selection_criterion == 'random':\n",
    "          self.train_indices = []\n",
    "        self.num_epochs = num_epochs\n",
    "        self.pool_features = []\n",
    "        self.pool_indices = []\n",
    "        self.train_features = []\n",
    "        # self.best_acc = 0\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the active learning pipeline\n",
    "        :return\n",
    "        accuracy_scores: list, accuracy scores at each iteration\n",
    "        \"\"\"\n",
    "        accuracy_scores = []\n",
    "        # auto_encoder =  Autoencoder(256)\n",
    "        # auto_encoder.load_state_dict(torch.load('vae_model.pth'))\n",
    "        self._get_features()\n",
    "        for iteration in range(self.iterations):\n",
    "            # if len(self.train_indices) > 600:\n",
    "            #     # raise error if the train set is larger than 600 samples\n",
    "            #     raise ValueError('The train set is larger than 600 samples')\n",
    "            print(f\"--------- Number of Iteration {iteration} ---------\")\n",
    "            if self.selection_criterion == 'random':\n",
    "                self._random_sampling()\n",
    "            elif self.selection_criterion == 'similarity_based':\n",
    "                self._similarity_based_sampling()\n",
    "            else:\n",
    "              self._custom_sampling(iteration)\n",
    "            \n",
    "            train_images = [train_df.__getitem__(index)[0] for index in self.train_indices]\n",
    "            label_df = [class_mapping[train_df.__getitem__(index)[1]] for index in self.train_indices]\n",
    "            self._train_model(train_images, label_df)\n",
    "            self.model.load_state_dict(torch.load(f\"best_{self.selection_criterion}_model.pth\"))\n",
    "            accuracy = self._evaluate_model()\n",
    "            accuracy_scores.append(accuracy)\n",
    "        return accuracy_scores\n",
    "        \n",
    "    def calculate_class_weights(self, label_counts, num_classes=8):\n",
    "        total_samples = sum(label_counts.values())\n",
    "        class_weights = torch.zeros(num_classes)\n",
    "        \n",
    "        for cls in range(num_classes):\n",
    "            if cls in label_counts:\n",
    "                class_weights[cls] = total_samples / (num_classes * label_counts[cls])\n",
    "            else:\n",
    "                class_weights[cls] = 1.0  # Handle the case where a class has zero samples in the current epoch\n",
    "    \n",
    "        return class_weights\n",
    "    \n",
    "    def _train_model(self, train_images, label_df):\n",
    "      label_counts = defaultdict(int)\n",
    "      for label in label_df:\n",
    "                label_counts[label] += 1\n",
    "      class_weights = self.calculate_class_weights(label_counts, 8).to(device)\n",
    "      loss_f = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "      train_images_tensor = torch.stack(train_images)\n",
    "      label_df_tensor = torch.tensor(label_df)\n",
    "      train_dataset = TensorDataset(train_images_tensor, label_df_tensor)\n",
    "\n",
    "      batch_size = 32\n",
    "      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "      best_acc = 0\n",
    "      for epoch in range(self.num_epochs):\n",
    "                self.model.train()\n",
    "                running_loss = 0.0  # Track the running loss\n",
    "                correct_predictions = 0\n",
    "                total_predictions = 0\n",
    "                # Training loop\n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs = inputs\n",
    "                    inputs= inputs.to(device)\n",
    "                    labels = torch.tensor(labels).to(device)\n",
    "                    \n",
    "                    # Zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = self.model(inputs)\n",
    "                    # outputs = outputs.logits\n",
    "                    loss = loss_f(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimization\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "        \n",
    "                    # Calculate accuracy\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    correct_predictions += torch.sum(preds == labels)\n",
    "                    total_predictions += inputs.shape[0]\n",
    "\n",
    "                # Print loss and accuracy at the end of each epoch\n",
    "                epoch_loss = running_loss / len(train_loader)\n",
    "                epoch_acc = correct_predictions.double() / total_predictions\n",
    "                print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "          \n",
    "                val_acc = self._check_model()\n",
    "                if val_acc > best_acc: \n",
    "                    best_acc = val_acc\n",
    "                    torch.save(self.model.state_dict(), f\"best_{self.selection_criterion}_model.pth\")\n",
    "      print(\"--\"*30)\n",
    "\n",
    "    def _check_model(self):\n",
    "        self.model.eval()\n",
    "        running_corrects = 0\n",
    "        total_predictions = 0.0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, _ in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = torch.tensor(labels).to(device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                # outputs = outputs.logits\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                total_predictions += inputs.shape[0]\n",
    "        val_acc = running_corrects.double() / total_predictions\n",
    "        return val_acc.item()\n",
    "        \n",
    "    def _evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate the model\n",
    "        :return:\n",
    "        accuracy: float, accuracy of the model\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        running_corrects = 0\n",
    "        test_images_tensor = torch.stack(test_images)\n",
    "        label_df_tensor = torch.tensor(test_label_df)\n",
    "        test_dataset = TensorDataset(test_images_tensor, label_df_tensor)\n",
    "        batch_size = 32  # Adjust based on your memory and hardware\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        total_predictions = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = torch.tensor(labels).to(device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                # outputs = outputs.logits\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                total_predictions += inputs.shape[0]\n",
    "        test_acc = running_corrects.double() / total_predictions\n",
    "        return test_acc.item()\n",
    "\n",
    "    def _random_sampling(self):\n",
    "      selected_indices = np.random.choice(self.available_pool_indices, self.budget_per_iter, replace=False)\n",
    "      selected_indices = selected_indices.tolist()\n",
    "      self.train_indices = self.train_indices + selected_indices\n",
    "\n",
    "      available_pool_set = set(self.available_pool_indices)\n",
    "      train_set = set(self.train_indices)\n",
    "      self.available_pool_indices = list(available_pool_set - train_set)\n",
    "\n",
    "    def extract_vae_features(self, dataloader, model, feature_extractor):\n",
    "        features_list = []\n",
    "        indices_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, indices in dataloader:\n",
    "                images = images.to(device)\n",
    "                images_list = [transforms.ToPILImage()(img) for img in images]\n",
    "                inputs = feature_extractor(images=images_list, return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    inputs = inputs.to(device)\n",
    "                    outputs = model(**inputs)\n",
    "                \n",
    "                x = outputs.last_hidden_state[:, 0, :]\n",
    "                features_list.append(x.cpu().numpy())\n",
    "                \n",
    "                # Collect indices\n",
    "                indices_list.extend(indices)\n",
    "                \n",
    "        # Stack all features into a 2D array (n_samples, hidden_dim)\n",
    "        features = np.vstack(features_list)\n",
    "        \n",
    "        return features, indices_list\n",
    "       \n",
    "    def _get_features(self):\n",
    "        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        # feature_extractor = feature_extractor.to(device)\n",
    "        model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        model = model.to(device)\n",
    "\n",
    "        train_images = [train_df.__getitem__(index)[0] for index in self.train_indices]\n",
    "        train_images_tensor = torch.stack(train_images)\n",
    "        label_df_tensor = torch.tensor(self.train_indices)\n",
    "        train_dataset = TensorDataset(train_images_tensor, label_df_tensor)\n",
    "        batch_size = 32\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.train_features, self.train_indices = self.extract_vae_features(train_loader, model, feature_extractor)\n",
    "        \n",
    "        X_unlabeled = [train_df.__getitem__(index)[0] for index in self.available_pool_indices]\n",
    "        pool_images_tensor = torch.stack(X_unlabeled)\n",
    "        pool_indices_tensor = torch.tensor(self.available_pool_indices)\n",
    "        pool_dataset = TensorDataset(pool_images_tensor, pool_indices_tensor)\n",
    "    \n",
    "        batch_size = 32\n",
    "        pool_loader = DataLoader(pool_dataset, batch_size=batch_size, shuffle=False)\n",
    "   \n",
    "        self.pool_features, self.pool_indices = self.extract_vae_features(pool_loader, model, feature_extractor)\n",
    "        \n",
    "    def _similarity_based_sampling(self):\n",
    "          max_similarities = []\n",
    "\n",
    "          for pool_feature in self.pool_features:\n",
    "                similarities = cosine_similarity([pool_feature], self.train_features)\n",
    "                max_similarity = np.max(similarities)\n",
    "                max_similarities.append(max_similarity)\n",
    "              \n",
    "          selected_indices = np.argsort(max_similarities)[:self.budget_per_iter]\n",
    "          temp = np.array(self.available_pool_indices)\n",
    "          selected_indices = temp[selected_indices]\n",
    "        \n",
    "          for i in selected_indices:\n",
    "              index = self.pool_indices.index(i)\n",
    "              \n",
    "              self.train_features = np.append(self.train_features, [self.pool_features[index]], axis=0)\n",
    "              self.train_indices.append(i)\n",
    "              \n",
    "              self.pool_features = np.delete(self.pool_features, index, axis=0)\n",
    "              self.pool_indices.pop(index)\n",
    "\n",
    "          available_pool_set = set(self.available_pool_indices)\n",
    "          train_set = set(self.train_indices)\n",
    "          self.available_pool_indices = list(available_pool_set - train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o5829ZYDh1Rp"
   },
   "outputs": [],
   "source": [
    "def generate_plot(accuracy_scores_dict):\n",
    "    \"\"\"\n",
    "    Generate a plot\n",
    "    \"\"\"\n",
    "    for criterion, accuracy_scores in accuracy_scores_dict.items():\n",
    "        plt.plot(range(1, len(accuracy_scores) + 1), accuracy_scores, label=criterion)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DQ2IZmfRuYgX"
   },
   "outputs": [],
   "source": [
    "available_pool_indices = []\n",
    "for i in range(len(train_df)):\n",
    "    image, label, index = train_df[i]\n",
    "    available_pool_indices.append(index)\n",
    "\n",
    "test_indices = []\n",
    "for i in range(len(test_df)):\n",
    "    image, label, index = test_df[i]\n",
    "    test_indices.append(index)\n",
    "test_images = [test_df.__getitem__(index)[0] for index in test_indices]\n",
    "test_label_df = [class_mapping[test_df.__getitem__(index)[1]] for index in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = [1372,\n",
    " 1277,\n",
    " 1255,\n",
    " 1423,\n",
    " 2925,\n",
    " 1963,\n",
    " 2335,\n",
    " 1923,\n",
    " 3791,\n",
    " 1239,\n",
    " 909,\n",
    " 134,\n",
    " 1547,\n",
    " 3931,\n",
    " 2467,\n",
    " 2832,\n",
    " 1789,\n",
    " 3022,\n",
    " 2424,\n",
    " 780,\n",
    " 2412,\n",
    " 3038,\n",
    " 2158,\n",
    " 3335,\n",
    " 1868,\n",
    " 1771,\n",
    " 2015,\n",
    " 1535,\n",
    " 710,\n",
    " 3007]\n",
    "available_pool_set = set(available_pool_indices)\n",
    "train_set = set(train_indices)\n",
    "available_pool_indices = list(available_pool_set - train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Pmcf2jc6h2S9"
   },
   "outputs": [],
   "source": [
    "# train_indices = []\n",
    "iterations = 20\n",
    "budget_per_iter = 60\n",
    "num_epoch = 15\n",
    "selection_criteria = ['similarity_based']\n",
    "accuracy_scores_dict = defaultdict(list)\n",
    "model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "4UGO2jW1h5Ql",
    "outputId": "0993cc7b-2fc2-4a84-a31a-ef45ed5febff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 18:12:58.295995: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-16 18:12:58.703328: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
      "2024-10-16 18:12:58.703418: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-16 18:13:00.490201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
      "2024-10-16 18:13:00.490494: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
      "2024-10-16 18:13:00.490515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/anaconda/envs/py38_default/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/py38_default/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 0 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 2.0687, Accuracy: 0.1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 2.0179, Accuracy: 0.2778\n",
      "Epoch [3/15], Loss: 1.9476, Accuracy: 0.3222\n",
      "Epoch [4/15], Loss: 1.9141, Accuracy: 0.4556\n",
      "Epoch [5/15], Loss: 1.8258, Accuracy: 0.6111\n",
      "Epoch [6/15], Loss: 1.7913, Accuracy: 0.5444\n",
      "Epoch [7/15], Loss: 1.7244, Accuracy: 0.6000\n",
      "Epoch [8/15], Loss: 1.6726, Accuracy: 0.7333\n",
      "Epoch [9/15], Loss: 1.6264, Accuracy: 0.8667\n",
      "Epoch [10/15], Loss: 1.5921, Accuracy: 0.8444\n",
      "Epoch [11/15], Loss: 1.5415, Accuracy: 0.8667\n",
      "Epoch [12/15], Loss: 1.5165, Accuracy: 0.8444\n",
      "Epoch [13/15], Loss: 1.4898, Accuracy: 0.9111\n",
      "Epoch [14/15], Loss: 1.4749, Accuracy: 0.8778\n",
      "Epoch [15/15], Loss: 1.4566, Accuracy: 0.8889\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 1 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.7002, Accuracy: 0.6733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6708, Accuracy: 0.6467\n",
      "Epoch [3/15], Loss: 1.6691, Accuracy: 0.7133\n",
      "Epoch [4/15], Loss: 1.6359, Accuracy: 0.7267\n",
      "Epoch [5/15], Loss: 1.6036, Accuracy: 0.7400\n",
      "Epoch [6/15], Loss: 1.6236, Accuracy: 0.7733\n",
      "Epoch [7/15], Loss: 1.5815, Accuracy: 0.8067\n",
      "Epoch [8/15], Loss: 1.5524, Accuracy: 0.8000\n",
      "Epoch [9/15], Loss: 1.5519, Accuracy: 0.8133\n",
      "Epoch [10/15], Loss: 1.5303, Accuracy: 0.8267\n",
      "Epoch [11/15], Loss: 1.5226, Accuracy: 0.8200\n",
      "Epoch [12/15], Loss: 1.5216, Accuracy: 0.8133\n",
      "Epoch [13/15], Loss: 1.5106, Accuracy: 0.8667\n",
      "Epoch [14/15], Loss: 1.5226, Accuracy: 0.8600\n",
      "Epoch [15/15], Loss: 1.5114, Accuracy: 0.8533\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 2 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6592, Accuracy: 0.7048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6719, Accuracy: 0.6714\n",
      "Epoch [3/15], Loss: 1.6188, Accuracy: 0.7238\n",
      "Epoch [4/15], Loss: 1.5766, Accuracy: 0.7667\n",
      "Epoch [5/15], Loss: 1.5863, Accuracy: 0.7667\n",
      "Epoch [6/15], Loss: 1.5864, Accuracy: 0.7905\n",
      "Epoch [7/15], Loss: 1.5671, Accuracy: 0.8000\n",
      "Epoch [8/15], Loss: 1.5323, Accuracy: 0.8333\n",
      "Epoch [9/15], Loss: 1.5488, Accuracy: 0.8000\n",
      "Epoch [10/15], Loss: 1.5318, Accuracy: 0.8333\n",
      "Epoch [11/15], Loss: 1.5438, Accuracy: 0.8333\n",
      "Epoch [12/15], Loss: 1.4977, Accuracy: 0.8381\n",
      "Epoch [13/15], Loss: 1.4889, Accuracy: 0.8571\n",
      "Epoch [14/15], Loss: 1.5165, Accuracy: 0.8810\n",
      "Epoch [15/15], Loss: 1.5114, Accuracy: 0.8571\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 3 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6730, Accuracy: 0.6593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6711, Accuracy: 0.6741\n",
      "Epoch [3/15], Loss: 1.6311, Accuracy: 0.7074\n",
      "Epoch [4/15], Loss: 1.6050, Accuracy: 0.7259\n",
      "Epoch [5/15], Loss: 1.5865, Accuracy: 0.7444\n",
      "Epoch [6/15], Loss: 1.6126, Accuracy: 0.7148\n",
      "Epoch [7/15], Loss: 1.6110, Accuracy: 0.7407\n",
      "Epoch [8/15], Loss: 1.5911, Accuracy: 0.7556\n",
      "Epoch [9/15], Loss: 1.5501, Accuracy: 0.8148\n",
      "Epoch [10/15], Loss: 1.5272, Accuracy: 0.8000\n",
      "Epoch [11/15], Loss: 1.5371, Accuracy: 0.8222\n",
      "Epoch [12/15], Loss: 1.5445, Accuracy: 0.8222\n",
      "Epoch [13/15], Loss: 1.5244, Accuracy: 0.8074\n",
      "Epoch [14/15], Loss: 1.5212, Accuracy: 0.8407\n",
      "Epoch [15/15], Loss: 1.4854, Accuracy: 0.8370\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 4 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6052, Accuracy: 0.7636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6412, Accuracy: 0.7394\n",
      "Epoch [3/15], Loss: 1.6210, Accuracy: 0.7000\n",
      "Epoch [4/15], Loss: 1.6104, Accuracy: 0.7455\n",
      "Epoch [5/15], Loss: 1.6028, Accuracy: 0.7424\n",
      "Epoch [6/15], Loss: 1.6336, Accuracy: 0.7394\n",
      "Epoch [7/15], Loss: 1.5932, Accuracy: 0.7424\n",
      "Epoch [8/15], Loss: 1.5584, Accuracy: 0.7727\n",
      "Epoch [9/15], Loss: 1.5641, Accuracy: 0.7636\n",
      "Epoch [10/15], Loss: 1.5749, Accuracy: 0.7818\n",
      "Epoch [11/15], Loss: 1.5576, Accuracy: 0.7970\n",
      "Epoch [12/15], Loss: 1.5673, Accuracy: 0.7909\n",
      "Epoch [13/15], Loss: 1.5252, Accuracy: 0.8121\n",
      "Epoch [14/15], Loss: 1.5512, Accuracy: 0.7818\n",
      "Epoch [15/15], Loss: 1.5156, Accuracy: 0.8364\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 5 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6678, Accuracy: 0.6949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6454, Accuracy: 0.7077\n",
      "Epoch [3/15], Loss: 1.6214, Accuracy: 0.7513\n",
      "Epoch [4/15], Loss: 1.5924, Accuracy: 0.7436\n",
      "Epoch [5/15], Loss: 1.5780, Accuracy: 0.7462\n",
      "Epoch [6/15], Loss: 1.5725, Accuracy: 0.7667\n",
      "Epoch [7/15], Loss: 1.6376, Accuracy: 0.7000\n",
      "Epoch [8/15], Loss: 1.6339, Accuracy: 0.7051\n",
      "Epoch [9/15], Loss: 1.6324, Accuracy: 0.6872\n",
      "Epoch [10/15], Loss: 1.5780, Accuracy: 0.7385\n",
      "Epoch [11/15], Loss: 1.6242, Accuracy: 0.6846\n",
      "Epoch [12/15], Loss: 1.5556, Accuracy: 0.7769\n",
      "Epoch [13/15], Loss: 1.5650, Accuracy: 0.7564\n",
      "Epoch [14/15], Loss: 1.5295, Accuracy: 0.7897\n",
      "Epoch [15/15], Loss: 1.5793, Accuracy: 0.7538\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 6 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6157, Accuracy: 0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6367, Accuracy: 0.7356\n",
      "Epoch [3/15], Loss: 1.6246, Accuracy: 0.6756\n",
      "Epoch [4/15], Loss: 1.6352, Accuracy: 0.7289\n",
      "Epoch [5/15], Loss: 1.6037, Accuracy: 0.7156\n",
      "Epoch [6/15], Loss: 1.6732, Accuracy: 0.6311\n",
      "Epoch [7/15], Loss: 1.5906, Accuracy: 0.7422\n",
      "Epoch [8/15], Loss: 1.6221, Accuracy: 0.7289\n",
      "Epoch [9/15], Loss: 1.5835, Accuracy: 0.7622\n",
      "Epoch [10/15], Loss: 1.6839, Accuracy: 0.6111\n",
      "Epoch [11/15], Loss: 1.6004, Accuracy: 0.6978\n",
      "Epoch [12/15], Loss: 1.5801, Accuracy: 0.7489\n",
      "Epoch [13/15], Loss: 1.6289, Accuracy: 0.6600\n",
      "Epoch [14/15], Loss: 1.6666, Accuracy: 0.7044\n",
      "Epoch [15/15], Loss: 1.5953, Accuracy: 0.7778\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 7 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6110, Accuracy: 0.7020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5842, Accuracy: 0.7373\n",
      "Epoch [3/15], Loss: 1.5656, Accuracy: 0.7392\n",
      "Epoch [4/15], Loss: 1.5690, Accuracy: 0.7314\n",
      "Epoch [5/15], Loss: 1.5671, Accuracy: 0.7686\n",
      "Epoch [6/15], Loss: 1.5789, Accuracy: 0.7392\n",
      "Epoch [7/15], Loss: 1.5504, Accuracy: 0.7647\n",
      "Epoch [8/15], Loss: 1.5537, Accuracy: 0.7725\n",
      "Epoch [9/15], Loss: 1.5505, Accuracy: 0.7725\n",
      "Epoch [10/15], Loss: 1.5407, Accuracy: 0.7882\n",
      "Epoch [11/15], Loss: 1.5401, Accuracy: 0.7784\n",
      "Epoch [12/15], Loss: 1.5448, Accuracy: 0.7843\n",
      "Epoch [13/15], Loss: 1.5288, Accuracy: 0.8157\n",
      "Epoch [14/15], Loss: 1.5323, Accuracy: 0.8078\n",
      "Epoch [15/15], Loss: 1.5265, Accuracy: 0.7941\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 8 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5749, Accuracy: 0.7596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5624, Accuracy: 0.7702\n",
      "Epoch [3/15], Loss: 1.5626, Accuracy: 0.7842\n",
      "Epoch [4/15], Loss: 1.5547, Accuracy: 0.7719\n",
      "Epoch [5/15], Loss: 1.5631, Accuracy: 0.7614\n",
      "Epoch [6/15], Loss: 1.5475, Accuracy: 0.7860\n",
      "Epoch [7/15], Loss: 1.5428, Accuracy: 0.7754\n",
      "Epoch [8/15], Loss: 1.5515, Accuracy: 0.7877\n",
      "Epoch [9/15], Loss: 1.5390, Accuracy: 0.7842\n",
      "Epoch [10/15], Loss: 1.5198, Accuracy: 0.8158\n",
      "Epoch [11/15], Loss: 1.5439, Accuracy: 0.7754\n",
      "Epoch [12/15], Loss: 1.5288, Accuracy: 0.8088\n",
      "Epoch [13/15], Loss: 1.5271, Accuracy: 0.8053\n",
      "Epoch [14/15], Loss: 1.5341, Accuracy: 0.8035\n",
      "Epoch [15/15], Loss: 1.5265, Accuracy: 0.8088\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 9 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5697, Accuracy: 0.7571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6036, Accuracy: 0.7127\n",
      "Epoch [3/15], Loss: 1.5645, Accuracy: 0.7635\n",
      "Epoch [4/15], Loss: 1.5545, Accuracy: 0.7603\n",
      "Epoch [5/15], Loss: 1.5515, Accuracy: 0.7698\n",
      "Epoch [6/15], Loss: 1.5567, Accuracy: 0.7683\n",
      "Epoch [7/15], Loss: 1.5597, Accuracy: 0.7683\n",
      "Epoch [8/15], Loss: 1.5405, Accuracy: 0.7889\n",
      "Epoch [9/15], Loss: 1.5391, Accuracy: 0.7825\n",
      "Epoch [10/15], Loss: 1.5461, Accuracy: 0.7714\n",
      "Epoch [11/15], Loss: 1.5383, Accuracy: 0.7889\n",
      "Epoch [12/15], Loss: 1.5392, Accuracy: 0.7810\n",
      "Epoch [13/15], Loss: 1.5479, Accuracy: 0.7635\n",
      "Epoch [14/15], Loss: 1.5554, Accuracy: 0.7587\n",
      "Epoch [15/15], Loss: 1.5366, Accuracy: 0.7825\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 10 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5847, Accuracy: 0.7406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5803, Accuracy: 0.7290\n",
      "Epoch [3/15], Loss: 1.5611, Accuracy: 0.7696\n",
      "Epoch [4/15], Loss: 1.5660, Accuracy: 0.7464\n",
      "Epoch [5/15], Loss: 1.5719, Accuracy: 0.7551\n",
      "Epoch [6/15], Loss: 1.5799, Accuracy: 0.7435\n",
      "Epoch [7/15], Loss: 1.5650, Accuracy: 0.7609\n",
      "Epoch [8/15], Loss: 1.5520, Accuracy: 0.7768\n",
      "Epoch [9/15], Loss: 1.5522, Accuracy: 0.7797\n",
      "Epoch [10/15], Loss: 1.5403, Accuracy: 0.7841\n",
      "Epoch [11/15], Loss: 1.5459, Accuracy: 0.7855\n",
      "Epoch [12/15], Loss: 1.5658, Accuracy: 0.7435\n",
      "Epoch [13/15], Loss: 1.5544, Accuracy: 0.7797\n",
      "Epoch [14/15], Loss: 1.5479, Accuracy: 0.7783\n",
      "Epoch [15/15], Loss: 1.5604, Accuracy: 0.7710\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 11 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6048, Accuracy: 0.7280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5661, Accuracy: 0.7627\n",
      "Epoch [3/15], Loss: 1.5802, Accuracy: 0.7253\n",
      "Epoch [4/15], Loss: 1.5776, Accuracy: 0.7533\n",
      "Epoch [5/15], Loss: 1.5859, Accuracy: 0.7320\n",
      "Epoch [6/15], Loss: 1.5727, Accuracy: 0.7507\n",
      "Epoch [7/15], Loss: 1.5668, Accuracy: 0.7467\n",
      "Epoch [8/15], Loss: 1.5589, Accuracy: 0.7640\n",
      "Epoch [9/15], Loss: 1.5604, Accuracy: 0.7507\n",
      "Epoch [10/15], Loss: 1.5901, Accuracy: 0.7347\n",
      "Epoch [11/15], Loss: 1.5497, Accuracy: 0.7707\n",
      "Epoch [12/15], Loss: 1.5418, Accuracy: 0.7653\n",
      "Epoch [13/15], Loss: 1.5618, Accuracy: 0.7653\n",
      "Epoch [14/15], Loss: 1.5287, Accuracy: 0.8053\n",
      "Epoch [15/15], Loss: 1.5431, Accuracy: 0.7947\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 12 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5808, Accuracy: 0.7383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6024, Accuracy: 0.7049\n",
      "Epoch [3/15], Loss: 1.5723, Accuracy: 0.7383\n",
      "Epoch [4/15], Loss: 1.5824, Accuracy: 0.7407\n",
      "Epoch [5/15], Loss: 1.5764, Accuracy: 0.7469\n",
      "Epoch [6/15], Loss: 1.5824, Accuracy: 0.7321\n",
      "Epoch [7/15], Loss: 1.5587, Accuracy: 0.7716\n",
      "Epoch [8/15], Loss: 1.5811, Accuracy: 0.7531\n",
      "Epoch [9/15], Loss: 1.5670, Accuracy: 0.7630\n",
      "Epoch [10/15], Loss: 1.5644, Accuracy: 0.7543\n",
      "Epoch [11/15], Loss: 1.5485, Accuracy: 0.7889\n",
      "Epoch [12/15], Loss: 1.5606, Accuracy: 0.7580\n",
      "Epoch [13/15], Loss: 1.5592, Accuracy: 0.7407\n",
      "Epoch [14/15], Loss: 1.5511, Accuracy: 0.7654\n",
      "Epoch [15/15], Loss: 1.5543, Accuracy: 0.7889\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 13 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5530, Accuracy: 0.7816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5765, Accuracy: 0.7322\n",
      "Epoch [3/15], Loss: 1.6028, Accuracy: 0.7276\n",
      "Epoch [4/15], Loss: 1.5749, Accuracy: 0.7667\n",
      "Epoch [5/15], Loss: 1.5713, Accuracy: 0.7575\n",
      "Epoch [6/15], Loss: 1.5427, Accuracy: 0.7724\n",
      "Epoch [7/15], Loss: 1.5573, Accuracy: 0.7609\n",
      "Epoch [8/15], Loss: 1.5751, Accuracy: 0.7517\n",
      "Epoch [9/15], Loss: 1.5766, Accuracy: 0.7552\n",
      "Epoch [10/15], Loss: 1.5677, Accuracy: 0.7552\n",
      "Epoch [11/15], Loss: 1.5593, Accuracy: 0.7460\n",
      "Epoch [12/15], Loss: 1.5373, Accuracy: 0.7897\n",
      "Epoch [13/15], Loss: 1.5513, Accuracy: 0.7552\n",
      "Epoch [14/15], Loss: 1.5423, Accuracy: 0.7874\n",
      "Epoch [15/15], Loss: 1.5378, Accuracy: 0.7782\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 14 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5805, Accuracy: 0.7570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5732, Accuracy: 0.7462\n",
      "Epoch [3/15], Loss: 1.5763, Accuracy: 0.7462\n",
      "Epoch [4/15], Loss: 1.5673, Accuracy: 0.7677\n",
      "Epoch [5/15], Loss: 1.5492, Accuracy: 0.7591\n",
      "Epoch [6/15], Loss: 1.5393, Accuracy: 0.7882\n",
      "Epoch [7/15], Loss: 1.6007, Accuracy: 0.7118\n",
      "Epoch [8/15], Loss: 1.6498, Accuracy: 0.6914\n",
      "Epoch [9/15], Loss: 1.5723, Accuracy: 0.7677\n",
      "Epoch [10/15], Loss: 1.5742, Accuracy: 0.7774\n",
      "Epoch [11/15], Loss: 1.5478, Accuracy: 0.7634\n",
      "Epoch [12/15], Loss: 1.5648, Accuracy: 0.7742\n",
      "Epoch [13/15], Loss: 1.6540, Accuracy: 0.6247\n",
      "Epoch [14/15], Loss: 1.6148, Accuracy: 0.7290\n",
      "Epoch [15/15], Loss: 1.5735, Accuracy: 0.7753\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 15 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5570, Accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5704, Accuracy: 0.7545\n",
      "Epoch [3/15], Loss: 1.5599, Accuracy: 0.7727\n",
      "Epoch [4/15], Loss: 1.5613, Accuracy: 0.7586\n",
      "Epoch [5/15], Loss: 1.5672, Accuracy: 0.7545\n",
      "Epoch [6/15], Loss: 1.5690, Accuracy: 0.7677\n",
      "Epoch [7/15], Loss: 1.5665, Accuracy: 0.7576\n",
      "Epoch [8/15], Loss: 1.5549, Accuracy: 0.7687\n",
      "Epoch [9/15], Loss: 1.5682, Accuracy: 0.7576\n",
      "Epoch [10/15], Loss: 1.5399, Accuracy: 0.7919\n",
      "Epoch [11/15], Loss: 1.5438, Accuracy: 0.7838\n",
      "Epoch [12/15], Loss: 1.5345, Accuracy: 0.7929\n",
      "Epoch [13/15], Loss: 1.5436, Accuracy: 0.7919\n",
      "Epoch [14/15], Loss: 1.5529, Accuracy: 0.7909\n",
      "Epoch [15/15], Loss: 1.5470, Accuracy: 0.7828\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 16 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5731, Accuracy: 0.7514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5665, Accuracy: 0.7486\n",
      "Epoch [3/15], Loss: 1.5862, Accuracy: 0.7371\n",
      "Epoch [4/15], Loss: 1.5485, Accuracy: 0.7762\n",
      "Epoch [5/15], Loss: 1.5479, Accuracy: 0.7743\n",
      "Epoch [6/15], Loss: 1.5605, Accuracy: 0.7562\n",
      "Epoch [7/15], Loss: 1.5559, Accuracy: 0.7638\n",
      "Epoch [8/15], Loss: 1.5563, Accuracy: 0.7610\n",
      "Epoch [9/15], Loss: 1.5510, Accuracy: 0.7543\n",
      "Epoch [10/15], Loss: 1.5483, Accuracy: 0.7781\n",
      "Epoch [11/15], Loss: 1.5653, Accuracy: 0.7476\n",
      "Epoch [12/15], Loss: 1.5346, Accuracy: 0.7810\n",
      "Epoch [13/15], Loss: 1.5396, Accuracy: 0.7724\n",
      "Epoch [14/15], Loss: 1.5369, Accuracy: 0.7867\n",
      "Epoch [15/15], Loss: 1.5283, Accuracy: 0.8010\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 17 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5463, Accuracy: 0.7703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5495, Accuracy: 0.7712\n",
      "Epoch [3/15], Loss: 1.5364, Accuracy: 0.7739\n",
      "Epoch [4/15], Loss: 1.5587, Accuracy: 0.7541\n",
      "Epoch [5/15], Loss: 1.5446, Accuracy: 0.7712\n",
      "Epoch [6/15], Loss: 1.5343, Accuracy: 0.7910\n",
      "Epoch [7/15], Loss: 1.5279, Accuracy: 0.7865\n",
      "Epoch [8/15], Loss: 1.5161, Accuracy: 0.8072\n",
      "Epoch [9/15], Loss: 1.5439, Accuracy: 0.7676\n",
      "Epoch [10/15], Loss: 1.5489, Accuracy: 0.7730\n",
      "Epoch [11/15], Loss: 1.5300, Accuracy: 0.7901\n",
      "Epoch [12/15], Loss: 1.5259, Accuracy: 0.8054\n",
      "Epoch [13/15], Loss: 1.5351, Accuracy: 0.7847\n",
      "Epoch [14/15], Loss: 1.5342, Accuracy: 0.7892\n",
      "Epoch [15/15], Loss: 1.5462, Accuracy: 0.7631\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 18 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5750, Accuracy: 0.7385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5565, Accuracy: 0.7701\n",
      "Epoch [3/15], Loss: 1.5419, Accuracy: 0.7692\n",
      "Epoch [4/15], Loss: 1.5512, Accuracy: 0.7607\n",
      "Epoch [5/15], Loss: 1.5459, Accuracy: 0.7684\n",
      "Epoch [6/15], Loss: 1.5450, Accuracy: 0.7675\n",
      "Epoch [7/15], Loss: 1.5537, Accuracy: 0.7675\n",
      "Epoch [8/15], Loss: 1.5419, Accuracy: 0.7735\n",
      "Epoch [9/15], Loss: 1.5460, Accuracy: 0.7735\n",
      "Epoch [10/15], Loss: 1.5463, Accuracy: 0.7573\n",
      "Epoch [11/15], Loss: 1.5352, Accuracy: 0.7889\n",
      "Epoch [12/15], Loss: 1.5335, Accuracy: 0.7795\n",
      "Epoch [13/15], Loss: 1.5420, Accuracy: 0.7658\n",
      "Epoch [14/15], Loss: 1.5468, Accuracy: 0.7684\n",
      "Epoch [15/15], Loss: 1.5380, Accuracy: 0.7632\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 19 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5483, Accuracy: 0.7732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5624, Accuracy: 0.7577\n",
      "Epoch [3/15], Loss: 1.5545, Accuracy: 0.7691\n",
      "Epoch [4/15], Loss: 1.5434, Accuracy: 0.7683\n",
      "Epoch [5/15], Loss: 1.5568, Accuracy: 0.7520\n",
      "Epoch [6/15], Loss: 1.5470, Accuracy: 0.7642\n",
      "Epoch [7/15], Loss: 1.5485, Accuracy: 0.7715\n",
      "Epoch [8/15], Loss: 1.5232, Accuracy: 0.7927\n",
      "Epoch [9/15], Loss: 1.5339, Accuracy: 0.7496\n",
      "Epoch [10/15], Loss: 1.5097, Accuracy: 0.7846\n",
      "Epoch [11/15], Loss: 1.5134, Accuracy: 0.7724\n",
      "Epoch [12/15], Loss: 1.4967, Accuracy: 0.7911\n",
      "Epoch [13/15], Loss: 1.4818, Accuracy: 0.8008\n",
      "Epoch [14/15], Loss: 1.5075, Accuracy: 0.7634\n",
      "Epoch [15/15], Loss: 1.5120, Accuracy: 0.7919\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_393239/2988917942.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "\n",
    "for criterion in selection_criteria:\n",
    "    AL_class = ActiveLearningPipeline(model=model,\n",
    "                                      test_indices=test_indices,\n",
    "                                      available_pool_indices=available_pool_indices,\n",
    "                                      train_indices=train_indices,\n",
    "                                      selection_criterion=criterion,\n",
    "                                      iterations=iterations,\n",
    "                                      budget_per_iter=budget_per_iter,\n",
    "                                      num_epochs=num_epoch)\n",
    "    accuracy_scores_dict[criterion] = AL_class.run_pipeline()\n",
    "generate_plot(accuracy_scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "j5sKVVjSzEl7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'similarity_based': [0.4821428571428571,\n",
       "              0.5621693121693121,\n",
       "              0.5582010582010581,\n",
       "              0.5595238095238095,\n",
       "              0.5667989417989417,\n",
       "              0.6197089947089947,\n",
       "              0.6481481481481481,\n",
       "              0.6395502645502645,\n",
       "              0.6177248677248677,\n",
       "              0.6296296296296295,\n",
       "              0.6375661375661376,\n",
       "              0.6501322751322751,\n",
       "              0.613095238095238,\n",
       "              0.6362433862433862,\n",
       "              0.5958994708994708,\n",
       "              0.6402116402116402,\n",
       "              0.6507936507936507,\n",
       "              0.6421957671957672,\n",
       "              0.6507936507936507,\n",
       "              0.6527777777777778]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_scores_dict"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

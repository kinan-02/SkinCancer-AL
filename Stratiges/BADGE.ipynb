{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjgyTgaRfy4k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import entropy\n",
        "from collections import defaultdict\n",
        "import argparse\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "random.seed(0) # Set seed for NumPy\n",
        "np.random.seed(0) # Set seed for PyTorch (for both CPU and GPU)\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "train_df = pd.read_csv('train_dataset/metadata.csv')\n",
        "test_df = pd.read_csv('test_dataset/metadata.csv')\n",
        "val_df = pd.read_csv('validation_dataset/metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "bcHCCahYj6B7",
        "outputId": "1facd164-3e8b-40f4-81d7-52fc3110e538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diagnosis\n",
            "nevus                         1205\n",
            "melanoma                      1113\n",
            "pigmented benign keratosis    1099\n",
            "basal cell carcinoma           514\n",
            "squamous cell carcinoma        197\n",
            "vascular lesion                142\n",
            "actinic keratosis              130\n",
            "dermatofibroma                 115\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_df['diagnosis'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGYWg_Tlg7d5",
        "outputId": "aed0e0a9-ae14-497d-d0af-d35079fc298d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'actinic keratosis': 0,\n",
              " 'basal cell carcinoma': 1,\n",
              " 'dermatofibroma': 2,\n",
              " 'melanoma': 3,\n",
              " 'nevus': 4,\n",
              " 'pigmented benign keratosis': 5,\n",
              " 'squamous cell carcinoma': 6,\n",
              " 'vascular lesion': 7}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_mapping = {\n",
        "    \"actinic keratosis\": 0,\n",
        "    \"basal cell carcinoma\": 1,\n",
        "    \"dermatofibroma\": 2,\n",
        "    \"melanoma\": 3,\n",
        "    \"nevus\": 4,\n",
        "    \"pigmented benign keratosis\": 5,\n",
        "    \"squamous cell carcinoma\": 6,\n",
        "    \"vascular lesion\":7\n",
        "}\n",
        "class_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLruwetXmPem"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define image transformations (resize, convert to tensor, and normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),          # Resize images to 224x224 (matching ResNet input size)\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\"\"\"\n",
        "Creating a Dataset class to read the data to pass it later to the data loader\n",
        "\"\"\"\n",
        "class Dataset():\n",
        "    def __init__(self, dataframe, transform, train='train'):\n",
        "        self.dataframe=dataframe\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.path_to_image=self._create_path_to_image_dict()\n",
        "        self.paths=list(self.path_to_image.keys())\n",
        "        self.labels=list(self.path_to_image.values())\n",
        "\n",
        "    def _create_path_to_image_dict(self):\n",
        "      path_to_image={}\n",
        "      for index,row in self.dataframe.iterrows():\n",
        "        if self.train == 'train':\n",
        "          img_path = os.path.join('train_dataset/',row['isic_id']+'.jpg')\n",
        "        elif self.train == 'test':\n",
        "          img_path = os.path.join('test_dataset/',row['isic_id']+'.jpg')\n",
        "        else:\n",
        "            img_path = os.path.join('validation_dataset/',row['isic_id']+'.jpg')\n",
        "        label=row['diagnosis']\n",
        "        path_to_image[img_path]=label\n",
        "      return path_to_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        img_path=self.paths[index]\n",
        "        img_label=self.labels[index]\n",
        "        image=Image.open(img_path)\n",
        "        image=self.transform(image)\n",
        "        if self.train == 'val':\n",
        "            return image, class_mapping[img_label], index\n",
        "        return image, img_label, index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yCvnprDoa_7"
      },
      "outputs": [],
      "source": [
        "train_df = Dataset(train_df, transform)\n",
        "val_df = Dataset(val_df, transform,train='val')\n",
        "test_df = Dataset(test_df, transform,train='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5L0NcpesnZI",
        "outputId": "22f97091-b571-4138-964f-95a509890583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Load pre-trained ResNet50 model from torchvision\n",
        "base_model = models.resnet50(pretrained=True)\n",
        "\n",
        "num_classes = 8\n",
        "\n",
        "\"\"\"\n",
        "replacing the final fc in the pretrained base model to adjust it to the number of classes we have\n",
        "\"\"\"\n",
        "base_model.fc = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(base_model.fc.in_features, 128),  # Add a fully connected layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, num_classes),  # Final layer with number of classes\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "for param in base_model.parameters():\n",
        "    param.requires_grad = False  # Freeze all layers except the fully connected ones\n",
        "\n",
        "# Unfreeze the final fully connected layer\n",
        "for param in base_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(base_model.fc.parameters(), lr=0.0008)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWQzIUrbg7eE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 4\n",
        "val_loader = DataLoader(val_df, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vk8kyZsgACa"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "class ActiveLearningPipeline:\n",
        "    def __init__(self, model,\n",
        "                 available_pool_indices,\n",
        "                 train_indices,\n",
        "                 test_indices,\n",
        "                 selection_criterion,\n",
        "                 iterations,\n",
        "                 budget_per_iter,\n",
        "                 num_epochs):\n",
        "        self.model = model\n",
        "        self.iterations = iterations\n",
        "        self.budget_per_iter = budget_per_iter\n",
        "        self.available_pool_indices = available_pool_indices\n",
        "        self.train_indices = train_indices\n",
        "        self.test_indices = test_indices\n",
        "        self.selection_criterion = selection_criterion\n",
        "        if self.selection_criterion == 'random':\n",
        "          self.train_indices = []\n",
        "        self.num_epochs = num_epochs\n",
        "        # self.best_acc = 0\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"\n",
        "        Run the active learning pipeline\n",
        "        :return\n",
        "        accuracy_scores: list, accuracy scores at each iteration\n",
        "        \"\"\"\n",
        "        accuracy_scores = []\n",
        "        for iteration in range(self.iterations):\n",
        "            print(f\"--------- Number of Iteration {iteration} ---------\")\n",
        "            if self.selection_criterion == 'random':\n",
        "                self._random_sampling()\n",
        "            elif self.selection_criterion == 'BADGE_30':\n",
        "                self._badge_sampling()\n",
        "            else:\n",
        "              self._custom_sampling(iteration)\n",
        "\n",
        "            train_images = [train_df.__getitem__(index)[0] for index in self.train_indices]\n",
        "            label_df = [class_mapping[train_df.__getitem__(index)[1]] for index in self.train_indices]\n",
        "            self._train_model(train_images, label_df)\n",
        "            #loading the best model weights in each iteration\n",
        "            self.model.load_state_dict(torch.load(f\"best_{self.selection_criterion}_model.pth\"))\n",
        "            accuracy = self._evaluate_model()\n",
        "            accuracy_scores.append(accuracy)\n",
        "        return accuracy_scores\n",
        "\n",
        "    def calculate_class_weights(self, label_counts, num_classes=8):\n",
        "        \"\"\"\n",
        "        this function is to caculate the inverse probability of each class in the data\n",
        "        \"\"\"\n",
        "        total_samples = sum(label_counts.values())\n",
        "        class_weights = torch.zeros(num_classes)\n",
        "\n",
        "        for cls in range(num_classes):\n",
        "            if cls in label_counts:\n",
        "                class_weights[cls] = total_samples / (num_classes * label_counts[cls])\n",
        "            else:\n",
        "                class_weights[cls] = 1.0  # Handle the case where a class has zero samples in the current epoch\n",
        "\n",
        "        return class_weights\n",
        "\n",
        "    def _train_model(self, train_images, label_df):\n",
        "      label_counts = defaultdict(int)\n",
        "      for label in label_df:\n",
        "                label_counts[label] += 1\n",
        "      class_weights = self.calculate_class_weights(label_counts, 8).to(device)\n",
        "      #giving higher weight for the loss of samples that their class is a minority in the data while giving less weight\n",
        "      #to the loss for classes that are majority\n",
        "      loss_f = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "      train_images_tensor = torch.stack(train_images)\n",
        "      label_df_tensor = torch.tensor(label_df)\n",
        "      train_dataset = TensorDataset(train_images_tensor, label_df_tensor)\n",
        "\n",
        "      batch_size = 32\n",
        "      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "      best_acc = 0\n",
        "      for epoch in range(self.num_epochs):\n",
        "                self.model.train()\n",
        "                running_loss = 0.0  # Track the running loss\n",
        "                correct_predictions = 0\n",
        "                total_predictions = 0\n",
        "                # Training loop\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs = inputs\n",
        "                    inputs= inputs.to(device)\n",
        "                    labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                    # Zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.model(inputs)\n",
        "                    # outputs = outputs.logits\n",
        "                    loss = loss_f(outputs, labels)\n",
        "\n",
        "                    # Backward pass and optimization\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    correct_predictions += torch.sum(preds == labels)\n",
        "                    total_predictions += inputs.shape[0]\n",
        "\n",
        "                # Print loss and accuracy at the end of each epoch\n",
        "                epoch_loss = running_loss / len(train_loader)\n",
        "                epoch_acc = correct_predictions.double() / total_predictions\n",
        "                print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "                #saving the best model weights on the validation\n",
        "                val_acc = self._check_model()\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "                    torch.save(self.model.state_dict(), f\"best_{self.selection_criterion}_model.pth\")\n",
        "      print(\"--\"*30)\n",
        "\n",
        "    def _check_model(self):\n",
        "        \"\"\"\n",
        "        this function is used to evaluate the model on the validation set\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        running_corrects = 0\n",
        "        total_predictions = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, _ in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                # outputs = outputs.logits\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                running_corrects += torch.sum(preds == labels)\n",
        "                total_predictions += inputs.shape[0]\n",
        "        val_acc = running_corrects.double() / total_predictions\n",
        "        return val_acc.item()\n",
        "\n",
        "    def _evaluate_model(self):\n",
        "        \"\"\"\n",
        "        Evaluate the model on the test set\n",
        "        :return:\n",
        "        accuracy: float, accuracy of the model\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        running_corrects = 0\n",
        "        test_images_tensor = torch.stack(test_images)\n",
        "        label_df_tensor = torch.tensor(test_label_df)\n",
        "        test_dataset = TensorDataset(test_images_tensor, label_df_tensor)\n",
        "        batch_size = 32\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "        total_predictions = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                running_corrects += torch.sum(preds == labels)\n",
        "                total_predictions += inputs.shape[0]\n",
        "        test_acc = running_corrects.double() / total_predictions\n",
        "        return test_acc.item()\n",
        "\n",
        "    def _random_sampling(self):\n",
        "      \"\"\"\n",
        "      random sampling strategy\n",
        "      \"\"\"\n",
        "      selected_indices = np.random.choice(self.available_pool_indices, self.budget_per_iter, replace=False)\n",
        "      selected_indices = selected_indices.tolist()\n",
        "      self.train_indices = self.train_indices + selected_indices\n",
        "\n",
        "      available_pool_set = set(self.available_pool_indices)\n",
        "      train_set = set(self.train_indices)\n",
        "      self.available_pool_indices = list(available_pool_set - train_set)\n",
        "\n",
        "\n",
        "    def _badge_sampling(self):\n",
        "      \"\"\"\n",
        "      BADGE sampling strategy\n",
        "      \"\"\"\n",
        "      model = self.model\n",
        "\n",
        "      X_unlabeled = [train_df.__getitem__(index)[0] for index in self.available_pool_indices]\n",
        "\n",
        "      pool_images_tensor = torch.stack(X_unlabeled)\n",
        "      pool_dataset = TensorDataset(pool_images_tensor)\n",
        "\n",
        "      batch_size = 32\n",
        "      pool_loader = DataLoader(pool_dataset, batch_size=batch_size, shuffle=False)\n",
        "      model.eval()\n",
        "      gradient_embeddings = []\n",
        "\n",
        "      def extract_grad_hook(module, grad_input, grad_output):\n",
        "        gradient_embeddings.append(grad_output[0].detach().cpu().numpy())\n",
        "      #calculating the gradients based on the last layer of the model\n",
        "      handle = model.fc[3].register_full_backward_hook(extract_grad_hook)\n",
        "\n",
        "      for inputs in pool_loader:\n",
        "        inputs = inputs[0].to(device)\n",
        "        inputs.requires_grad = False\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        predicted_labels = torch.argmax(outputs, dim=1).to(device)\n",
        "        loss = torch.nn.functional.cross_entropy(outputs, predicted_labels)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "      handle.remove()\n",
        "\n",
        "      gradient_embeddings = np.concatenate(gradient_embeddings, axis=0)\n",
        "      num_samples_to_select = self.budget_per_iter\n",
        "      #applying KMenas++ on the gradients embeddings\n",
        "      kmeans = KMeans(n_clusters=num_samples_to_select, init='k-means++', random_state=0).fit(gradient_embeddings)\n",
        "      #updating the pool and train indices\n",
        "      centroids = kmeans.cluster_centers_\n",
        "      selected_indices = []\n",
        "      for i in range(num_samples_to_select):\n",
        "        cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
        "        closest_sample_index = min(cluster_indices, key=lambda idx: np.linalg.norm(gradient_embeddings[idx] - centroids[i]))\n",
        "        selected_indices.append(closest_sample_index)\n",
        "\n",
        "      temp = np.array(self.available_pool_indices)\n",
        "      selected_indices = temp[selected_indices]\n",
        "\n",
        "      self.train_indices = self.train_indices + selected_indices.tolist()\n",
        "\n",
        "      available_pool_set = set(self.available_pool_indices)\n",
        "      train_set = set(self.train_indices)\n",
        "      self.available_pool_indices = list(available_pool_set - train_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5829ZYDh1Rp"
      },
      "outputs": [],
      "source": [
        "def generate_plot(accuracy_scores_dict):\n",
        "    \"\"\"\n",
        "    Generate a plot\n",
        "    \"\"\"\n",
        "    for criterion, accuracy_scores in accuracy_scores_dict.items():\n",
        "        plt.plot(range(1, len(accuracy_scores) + 1), accuracy_scores, label=criterion)\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ2IZmfRuYgX"
      },
      "outputs": [],
      "source": [
        "available_pool_indices = []\n",
        "for i in range(len(train_df)):\n",
        "    image, label, index = train_df[i]\n",
        "    available_pool_indices.append(index)\n",
        "\n",
        "test_indices = []\n",
        "for i in range(len(test_df)):\n",
        "    image, label, index = test_df[i]\n",
        "    test_indices.append(index)\n",
        "test_images = [test_df.__getitem__(index)[0] for index in test_indices]\n",
        "test_label_df = [class_mapping[test_df.__getitem__(index)[1]] for index in test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT_B3HUQg7eL"
      },
      "outputs": [],
      "source": [
        "#According to the initialization that we got using KMeans++ Code of the initialization could be found in the initialization file\n",
        "train_indices = [1372,\n",
        " 1277,\n",
        " 1255,\n",
        " 1423,\n",
        " 2925,\n",
        " 1963,\n",
        " 2335,\n",
        " 1923,\n",
        " 3791,\n",
        " 1239,\n",
        " 909,\n",
        " 134,\n",
        " 1547,\n",
        " 3931,\n",
        " 2467,\n",
        " 2832,\n",
        " 1789,\n",
        " 3022,\n",
        " 2424,\n",
        " 780,\n",
        " 2412,\n",
        " 3038,\n",
        " 2158,\n",
        " 3335,\n",
        " 1868,\n",
        " 1771,\n",
        " 2015,\n",
        " 1535,\n",
        " 710,\n",
        " 3007]\n",
        "available_pool_set = set(available_pool_indices)\n",
        "train_set = set(train_indices)\n",
        "available_pool_indices = list(available_pool_set - train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmcf2jc6h2S9"
      },
      "outputs": [],
      "source": [
        "iterations = 20\n",
        "budget_per_iter = 60 #30\n",
        "num_epoch = 15\n",
        "selection_criteria = ['BADGE'] # BADGE_30\n",
        "accuracy_scores_dict = defaultdict(list)\n",
        "model = base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "4UGO2jW1h5Ql",
        "outputId": "0993cc7b-2fc2-4a84-a31a-ef45ed5febff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 0 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 2.0804, Accuracy: 0.0500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 2.0274, Accuracy: 0.3167\n",
            "Epoch [3/15], Loss: 1.9875, Accuracy: 0.3500\n",
            "Epoch [4/15], Loss: 1.9434, Accuracy: 0.4333\n",
            "Epoch [5/15], Loss: 1.9091, Accuracy: 0.4167\n",
            "Epoch [6/15], Loss: 1.8228, Accuracy: 0.4500\n",
            "Epoch [7/15], Loss: 1.8067, Accuracy: 0.6500\n",
            "Epoch [8/15], Loss: 1.7432, Accuracy: 0.7667\n",
            "Epoch [9/15], Loss: 1.7027, Accuracy: 0.8667\n",
            "Epoch [10/15], Loss: 1.6765, Accuracy: 0.8500\n",
            "Epoch [11/15], Loss: 1.6206, Accuracy: 0.9000\n",
            "Epoch [12/15], Loss: 1.5688, Accuracy: 0.9000\n",
            "Epoch [13/15], Loss: 1.5271, Accuracy: 0.8833\n",
            "Epoch [14/15], Loss: 1.5257, Accuracy: 0.9000\n",
            "Epoch [15/15], Loss: 1.4706, Accuracy: 0.9333\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 1 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.7231, Accuracy: 0.7667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.7098, Accuracy: 0.7889\n",
            "Epoch [3/15], Loss: 1.6936, Accuracy: 0.7667\n",
            "Epoch [4/15], Loss: 1.6472, Accuracy: 0.7889\n",
            "Epoch [5/15], Loss: 1.6702, Accuracy: 0.7889\n",
            "Epoch [6/15], Loss: 1.6448, Accuracy: 0.8111\n",
            "Epoch [7/15], Loss: 1.6054, Accuracy: 0.8333\n",
            "Epoch [8/15], Loss: 1.6308, Accuracy: 0.8556\n",
            "Epoch [9/15], Loss: 1.6305, Accuracy: 0.8556\n",
            "Epoch [10/15], Loss: 1.6046, Accuracy: 0.8444\n",
            "Epoch [11/15], Loss: 1.6168, Accuracy: 0.8333\n",
            "Epoch [12/15], Loss: 1.5965, Accuracy: 0.8556\n",
            "Epoch [13/15], Loss: 1.6027, Accuracy: 0.8778\n",
            "Epoch [14/15], Loss: 1.5947, Accuracy: 0.8778\n",
            "Epoch [15/15], Loss: 1.5936, Accuracy: 0.8556\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 2 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6947, Accuracy: 0.7417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6659, Accuracy: 0.7333\n",
            "Epoch [3/15], Loss: 1.6554, Accuracy: 0.7500\n",
            "Epoch [4/15], Loss: 1.6518, Accuracy: 0.7667\n",
            "Epoch [5/15], Loss: 1.6304, Accuracy: 0.7833\n",
            "Epoch [6/15], Loss: 1.6158, Accuracy: 0.7833\n",
            "Epoch [7/15], Loss: 1.6334, Accuracy: 0.8000\n",
            "Epoch [8/15], Loss: 1.6260, Accuracy: 0.7667\n",
            "Epoch [9/15], Loss: 1.6116, Accuracy: 0.8250\n",
            "Epoch [10/15], Loss: 1.6217, Accuracy: 0.7917\n",
            "Epoch [11/15], Loss: 1.5990, Accuracy: 0.8000\n",
            "Epoch [12/15], Loss: 1.6027, Accuracy: 0.8250\n",
            "Epoch [13/15], Loss: 1.5945, Accuracy: 0.8500\n",
            "Epoch [14/15], Loss: 1.5944, Accuracy: 0.8083\n",
            "Epoch [15/15], Loss: 1.5577, Accuracy: 0.8333\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 3 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6392, Accuracy: 0.7000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6259, Accuracy: 0.7333\n",
            "Epoch [3/15], Loss: 1.6386, Accuracy: 0.7400\n",
            "Epoch [4/15], Loss: 1.6368, Accuracy: 0.7667\n",
            "Epoch [5/15], Loss: 1.6233, Accuracy: 0.7467\n",
            "Epoch [6/15], Loss: 1.5991, Accuracy: 0.7867\n",
            "Epoch [7/15], Loss: 1.6183, Accuracy: 0.7800\n",
            "Epoch [8/15], Loss: 1.6076, Accuracy: 0.8200\n",
            "Epoch [9/15], Loss: 1.6118, Accuracy: 0.8000\n",
            "Epoch [10/15], Loss: 1.6023, Accuracy: 0.8000\n",
            "Epoch [11/15], Loss: 1.6128, Accuracy: 0.7933\n",
            "Epoch [12/15], Loss: 1.5985, Accuracy: 0.8400\n",
            "Epoch [13/15], Loss: 1.5720, Accuracy: 0.8267\n",
            "Epoch [14/15], Loss: 1.5631, Accuracy: 0.8400\n",
            "Epoch [15/15], Loss: 1.6025, Accuracy: 0.8133\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 4 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6701, Accuracy: 0.7167\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6468, Accuracy: 0.7333\n",
            "Epoch [3/15], Loss: 1.6368, Accuracy: 0.7611\n",
            "Epoch [4/15], Loss: 1.6430, Accuracy: 0.7778\n",
            "Epoch [5/15], Loss: 1.6404, Accuracy: 0.7667\n",
            "Epoch [6/15], Loss: 1.6411, Accuracy: 0.8000\n",
            "Epoch [7/15], Loss: 1.6286, Accuracy: 0.7833\n",
            "Epoch [8/15], Loss: 1.6508, Accuracy: 0.7389\n",
            "Epoch [9/15], Loss: 1.6063, Accuracy: 0.8056\n",
            "Epoch [10/15], Loss: 1.6229, Accuracy: 0.7889\n",
            "Epoch [11/15], Loss: 1.6116, Accuracy: 0.8278\n",
            "Epoch [12/15], Loss: 1.5990, Accuracy: 0.7778\n",
            "Epoch [13/15], Loss: 1.6221, Accuracy: 0.8000\n",
            "Epoch [14/15], Loss: 1.6107, Accuracy: 0.8278\n",
            "Epoch [15/15], Loss: 1.5974, Accuracy: 0.8111\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 5 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6589, Accuracy: 0.7286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6826, Accuracy: 0.6952\n",
            "Epoch [3/15], Loss: 1.6571, Accuracy: 0.7381\n",
            "Epoch [4/15], Loss: 1.6378, Accuracy: 0.7571\n",
            "Epoch [5/15], Loss: 1.6388, Accuracy: 0.7857\n",
            "Epoch [6/15], Loss: 1.6270, Accuracy: 0.7524\n",
            "Epoch [7/15], Loss: 1.6075, Accuracy: 0.7952\n",
            "Epoch [8/15], Loss: 1.6100, Accuracy: 0.7952\n",
            "Epoch [9/15], Loss: 1.6041, Accuracy: 0.8238\n",
            "Epoch [10/15], Loss: 1.6274, Accuracy: 0.8190\n",
            "Epoch [11/15], Loss: 1.6034, Accuracy: 0.8095\n",
            "Epoch [12/15], Loss: 1.5797, Accuracy: 0.8429\n",
            "Epoch [13/15], Loss: 1.5735, Accuracy: 0.8381\n",
            "Epoch [14/15], Loss: 1.5629, Accuracy: 0.8571\n",
            "Epoch [15/15], Loss: 1.5702, Accuracy: 0.8619\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 6 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6710, Accuracy: 0.7458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6803, Accuracy: 0.7167\n",
            "Epoch [3/15], Loss: 1.6746, Accuracy: 0.7375\n",
            "Epoch [4/15], Loss: 1.6491, Accuracy: 0.7583\n",
            "Epoch [5/15], Loss: 1.6394, Accuracy: 0.7583\n",
            "Epoch [6/15], Loss: 1.6364, Accuracy: 0.8042\n",
            "Epoch [7/15], Loss: 1.6282, Accuracy: 0.7458\n",
            "Epoch [8/15], Loss: 1.6138, Accuracy: 0.8042\n",
            "Epoch [9/15], Loss: 1.6229, Accuracy: 0.8042\n",
            "Epoch [10/15], Loss: 1.5916, Accuracy: 0.7875\n",
            "Epoch [11/15], Loss: 1.5819, Accuracy: 0.7667\n",
            "Epoch [12/15], Loss: 1.5739, Accuracy: 0.8417\n",
            "Epoch [13/15], Loss: 1.5687, Accuracy: 0.8417\n",
            "Epoch [14/15], Loss: 1.5581, Accuracy: 0.7917\n",
            "Epoch [15/15], Loss: 1.5377, Accuracy: 0.8208\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 7 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6372, Accuracy: 0.7370\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6351, Accuracy: 0.7667\n",
            "Epoch [3/15], Loss: 1.6035, Accuracy: 0.7741\n",
            "Epoch [4/15], Loss: 1.5867, Accuracy: 0.7815\n",
            "Epoch [5/15], Loss: 1.5983, Accuracy: 0.7667\n",
            "Epoch [6/15], Loss: 1.5712, Accuracy: 0.7963\n",
            "Epoch [7/15], Loss: 1.5685, Accuracy: 0.7481\n",
            "Epoch [8/15], Loss: 1.5641, Accuracy: 0.8000\n",
            "Epoch [9/15], Loss: 1.5294, Accuracy: 0.8148\n",
            "Epoch [10/15], Loss: 1.5227, Accuracy: 0.8556\n",
            "Epoch [11/15], Loss: 1.5219, Accuracy: 0.8259\n",
            "Epoch [12/15], Loss: 1.5350, Accuracy: 0.8259\n",
            "Epoch [13/15], Loss: 1.5167, Accuracy: 0.7852\n",
            "Epoch [14/15], Loss: 1.5229, Accuracy: 0.8444\n",
            "Epoch [15/15], Loss: 1.4988, Accuracy: 0.8185\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 8 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6496, Accuracy: 0.7000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6248, Accuracy: 0.7633\n",
            "Epoch [3/15], Loss: 1.6421, Accuracy: 0.7200\n",
            "Epoch [4/15], Loss: 1.6077, Accuracy: 0.7833\n",
            "Epoch [5/15], Loss: 1.6089, Accuracy: 0.7100\n",
            "Epoch [6/15], Loss: 1.5761, Accuracy: 0.7500\n",
            "Epoch [7/15], Loss: 1.5662, Accuracy: 0.7700\n",
            "Epoch [8/15], Loss: 1.5563, Accuracy: 0.7600\n",
            "Epoch [9/15], Loss: 1.5389, Accuracy: 0.8000\n",
            "Epoch [10/15], Loss: 1.5110, Accuracy: 0.7900\n",
            "Epoch [11/15], Loss: 1.5174, Accuracy: 0.7700\n",
            "Epoch [12/15], Loss: 1.5196, Accuracy: 0.8167\n",
            "Epoch [13/15], Loss: 1.5572, Accuracy: 0.7367\n",
            "Epoch [14/15], Loss: 1.5038, Accuracy: 0.8233\n",
            "Epoch [15/15], Loss: 1.4922, Accuracy: 0.8300\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 9 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5818, Accuracy: 0.7606\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5730, Accuracy: 0.7576\n",
            "Epoch [3/15], Loss: 1.5920, Accuracy: 0.7545\n",
            "Epoch [4/15], Loss: 1.5628, Accuracy: 0.7545\n",
            "Epoch [5/15], Loss: 1.5662, Accuracy: 0.7333\n",
            "Epoch [6/15], Loss: 1.5697, Accuracy: 0.7879\n",
            "Epoch [7/15], Loss: 1.5514, Accuracy: 0.7667\n",
            "Epoch [8/15], Loss: 1.5577, Accuracy: 0.7758\n",
            "Epoch [9/15], Loss: 1.5423, Accuracy: 0.7576\n",
            "Epoch [10/15], Loss: 1.5571, Accuracy: 0.8030\n",
            "Epoch [11/15], Loss: 1.5336, Accuracy: 0.8121\n",
            "Epoch [12/15], Loss: 1.5380, Accuracy: 0.8061\n",
            "Epoch [13/15], Loss: 1.5067, Accuracy: 0.8061\n",
            "Epoch [14/15], Loss: 1.5294, Accuracy: 0.8182\n",
            "Epoch [15/15], Loss: 1.5447, Accuracy: 0.7970\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 10 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5888, Accuracy: 0.7639\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5680, Accuracy: 0.7861\n",
            "Epoch [3/15], Loss: 1.5878, Accuracy: 0.7472\n",
            "Epoch [4/15], Loss: 1.5520, Accuracy: 0.7806\n",
            "Epoch [5/15], Loss: 1.5227, Accuracy: 0.7889\n",
            "Epoch [6/15], Loss: 1.4759, Accuracy: 0.8306\n",
            "Epoch [7/15], Loss: 1.5094, Accuracy: 0.8056\n",
            "Epoch [8/15], Loss: 1.5348, Accuracy: 0.7639\n",
            "Epoch [9/15], Loss: 1.5360, Accuracy: 0.7417\n",
            "Epoch [10/15], Loss: 1.4653, Accuracy: 0.7889\n",
            "Epoch [11/15], Loss: 1.4511, Accuracy: 0.8417\n",
            "Epoch [12/15], Loss: 1.4651, Accuracy: 0.7889\n",
            "Epoch [13/15], Loss: 1.4293, Accuracy: 0.8444\n",
            "Epoch [14/15], Loss: 1.4144, Accuracy: 0.8444\n",
            "Epoch [15/15], Loss: 1.4393, Accuracy: 0.8306\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 11 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5718, Accuracy: 0.7513\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5226, Accuracy: 0.7821\n",
            "Epoch [3/15], Loss: 1.4779, Accuracy: 0.8282\n",
            "Epoch [4/15], Loss: 1.4938, Accuracy: 0.8308\n",
            "Epoch [5/15], Loss: 1.4620, Accuracy: 0.7769\n",
            "Epoch [6/15], Loss: 1.4985, Accuracy: 0.8205\n",
            "Epoch [7/15], Loss: 1.5060, Accuracy: 0.8000\n",
            "Epoch [8/15], Loss: 1.4579, Accuracy: 0.8103\n",
            "Epoch [9/15], Loss: 1.5031, Accuracy: 0.7692\n",
            "Epoch [10/15], Loss: 1.4783, Accuracy: 0.7615\n",
            "Epoch [11/15], Loss: 1.4379, Accuracy: 0.8385\n",
            "Epoch [12/15], Loss: 1.4568, Accuracy: 0.7615\n",
            "Epoch [13/15], Loss: 1.4213, Accuracy: 0.8436\n",
            "Epoch [14/15], Loss: 1.4501, Accuracy: 0.8308\n",
            "Epoch [15/15], Loss: 1.4178, Accuracy: 0.8513\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 12 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4968, Accuracy: 0.7905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5288, Accuracy: 0.7286\n",
            "Epoch [3/15], Loss: 1.5495, Accuracy: 0.7452\n",
            "Epoch [4/15], Loss: 1.5397, Accuracy: 0.7690\n",
            "Epoch [5/15], Loss: 1.5054, Accuracy: 0.7952\n",
            "Epoch [6/15], Loss: 1.4969, Accuracy: 0.8190\n",
            "Epoch [7/15], Loss: 1.4921, Accuracy: 0.8167\n",
            "Epoch [8/15], Loss: 1.5033, Accuracy: 0.7667\n",
            "Epoch [9/15], Loss: 1.4437, Accuracy: 0.8119\n",
            "Epoch [10/15], Loss: 1.4394, Accuracy: 0.8262\n",
            "Epoch [11/15], Loss: 1.5086, Accuracy: 0.7452\n",
            "Epoch [12/15], Loss: 1.4766, Accuracy: 0.7810\n",
            "Epoch [13/15], Loss: 1.4580, Accuracy: 0.8167\n",
            "Epoch [14/15], Loss: 1.4587, Accuracy: 0.8095\n",
            "Epoch [15/15], Loss: 1.4857, Accuracy: 0.7976\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 13 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5221, Accuracy: 0.7489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5579, Accuracy: 0.7756\n",
            "Epoch [3/15], Loss: 1.5113, Accuracy: 0.7933\n",
            "Epoch [4/15], Loss: 1.5145, Accuracy: 0.7933\n",
            "Epoch [5/15], Loss: 1.5113, Accuracy: 0.7467\n",
            "Epoch [6/15], Loss: 1.4938, Accuracy: 0.8422\n",
            "Epoch [7/15], Loss: 1.5477, Accuracy: 0.6889\n",
            "Epoch [8/15], Loss: 1.6212, Accuracy: 0.6289\n",
            "Epoch [9/15], Loss: 1.4961, Accuracy: 0.7489\n",
            "Epoch [10/15], Loss: 1.5068, Accuracy: 0.7778\n",
            "Epoch [11/15], Loss: 1.4846, Accuracy: 0.8333\n",
            "Epoch [12/15], Loss: 1.4842, Accuracy: 0.8289\n",
            "Epoch [13/15], Loss: 1.4209, Accuracy: 0.8267\n",
            "Epoch [14/15], Loss: 1.4642, Accuracy: 0.8333\n",
            "Epoch [15/15], Loss: 1.4990, Accuracy: 0.7400\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 14 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5148, Accuracy: 0.7167\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4857, Accuracy: 0.7625\n",
            "Epoch [3/15], Loss: 1.4751, Accuracy: 0.7833\n",
            "Epoch [4/15], Loss: 1.4545, Accuracy: 0.7917\n",
            "Epoch [5/15], Loss: 1.4582, Accuracy: 0.7979\n",
            "Epoch [6/15], Loss: 1.4668, Accuracy: 0.7792\n",
            "Epoch [7/15], Loss: 1.4471, Accuracy: 0.7958\n",
            "Epoch [8/15], Loss: 1.4469, Accuracy: 0.8083\n",
            "Epoch [9/15], Loss: 1.4515, Accuracy: 0.8063\n",
            "Epoch [10/15], Loss: 1.4378, Accuracy: 0.8167\n",
            "Epoch [11/15], Loss: 1.4438, Accuracy: 0.8208\n",
            "Epoch [12/15], Loss: 1.4326, Accuracy: 0.8292\n",
            "Epoch [13/15], Loss: 1.4255, Accuracy: 0.8208\n",
            "Epoch [14/15], Loss: 1.4212, Accuracy: 0.8458\n",
            "Epoch [15/15], Loss: 1.4193, Accuracy: 0.8312\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 15 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4491, Accuracy: 0.8020\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4593, Accuracy: 0.7824\n",
            "Epoch [3/15], Loss: 1.4478, Accuracy: 0.8039\n",
            "Epoch [4/15], Loss: 1.4494, Accuracy: 0.7922\n",
            "Epoch [5/15], Loss: 1.4494, Accuracy: 0.8020\n",
            "Epoch [6/15], Loss: 1.4295, Accuracy: 0.8353\n",
            "Epoch [7/15], Loss: 1.4540, Accuracy: 0.7882\n",
            "Epoch [8/15], Loss: 1.4369, Accuracy: 0.8059\n",
            "Epoch [9/15], Loss: 1.4346, Accuracy: 0.8157\n",
            "Epoch [10/15], Loss: 1.4347, Accuracy: 0.8137\n",
            "Epoch [11/15], Loss: 1.4183, Accuracy: 0.8333\n",
            "Epoch [12/15], Loss: 1.4294, Accuracy: 0.8196\n",
            "Epoch [13/15], Loss: 1.4198, Accuracy: 0.8333\n",
            "Epoch [14/15], Loss: 1.4245, Accuracy: 0.8098\n",
            "Epoch [15/15], Loss: 1.4247, Accuracy: 0.8314\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 16 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4444, Accuracy: 0.8148\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4416, Accuracy: 0.8148\n",
            "Epoch [3/15], Loss: 1.4447, Accuracy: 0.7704\n",
            "Epoch [4/15], Loss: 1.4551, Accuracy: 0.7889\n",
            "Epoch [5/15], Loss: 1.4293, Accuracy: 0.8074\n",
            "Epoch [6/15], Loss: 1.4265, Accuracy: 0.8111\n",
            "Epoch [7/15], Loss: 1.4254, Accuracy: 0.8315\n",
            "Epoch [8/15], Loss: 1.4156, Accuracy: 0.8130\n",
            "Epoch [9/15], Loss: 1.4270, Accuracy: 0.8296\n",
            "Epoch [10/15], Loss: 1.4183, Accuracy: 0.8278\n",
            "Epoch [11/15], Loss: 1.4457, Accuracy: 0.8037\n",
            "Epoch [12/15], Loss: 1.4128, Accuracy: 0.8296\n",
            "Epoch [13/15], Loss: 1.4222, Accuracy: 0.8167\n",
            "Epoch [14/15], Loss: 1.3983, Accuracy: 0.8593\n",
            "Epoch [15/15], Loss: 1.3975, Accuracy: 0.8500\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 17 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4625, Accuracy: 0.7895\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4614, Accuracy: 0.7930\n",
            "Epoch [3/15], Loss: 1.4699, Accuracy: 0.7614\n",
            "Epoch [4/15], Loss: 1.4636, Accuracy: 0.7789\n",
            "Epoch [5/15], Loss: 1.4634, Accuracy: 0.7825\n",
            "Epoch [6/15], Loss: 1.4569, Accuracy: 0.7965\n",
            "Epoch [7/15], Loss: 1.4618, Accuracy: 0.7789\n",
            "Epoch [8/15], Loss: 1.4384, Accuracy: 0.8140\n",
            "Epoch [9/15], Loss: 1.4215, Accuracy: 0.8281\n",
            "Epoch [10/15], Loss: 1.4488, Accuracy: 0.7754\n",
            "Epoch [11/15], Loss: 1.4319, Accuracy: 0.8140\n",
            "Epoch [12/15], Loss: 1.4075, Accuracy: 0.8368\n",
            "Epoch [13/15], Loss: 1.4110, Accuracy: 0.8316\n",
            "Epoch [14/15], Loss: 1.4130, Accuracy: 0.8333\n",
            "Epoch [15/15], Loss: 1.4163, Accuracy: 0.8263\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 18 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4689, Accuracy: 0.7650\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4791, Accuracy: 0.7600\n",
            "Epoch [3/15], Loss: 1.4702, Accuracy: 0.7883\n",
            "Epoch [4/15], Loss: 1.4549, Accuracy: 0.7817\n",
            "Epoch [5/15], Loss: 1.4609, Accuracy: 0.8017\n",
            "Epoch [6/15], Loss: 1.4377, Accuracy: 0.8000\n",
            "Epoch [7/15], Loss: 1.4588, Accuracy: 0.7783\n",
            "Epoch [8/15], Loss: 1.4500, Accuracy: 0.7717\n",
            "Epoch [9/15], Loss: 1.4428, Accuracy: 0.7933\n",
            "Epoch [10/15], Loss: 1.4414, Accuracy: 0.8117\n",
            "Epoch [11/15], Loss: 1.4458, Accuracy: 0.7850\n",
            "Epoch [12/15], Loss: 1.4242, Accuracy: 0.8267\n",
            "Epoch [13/15], Loss: 1.4462, Accuracy: 0.7783\n",
            "Epoch [14/15], Loss: 1.4423, Accuracy: 0.7750\n",
            "Epoch [15/15], Loss: 1.4147, Accuracy: 0.8217\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 19 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4719, Accuracy: 0.7825\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4615, Accuracy: 0.7937\n",
            "Epoch [3/15], Loss: 1.4686, Accuracy: 0.7492\n",
            "Epoch [4/15], Loss: 1.4664, Accuracy: 0.7540\n",
            "Epoch [5/15], Loss: 1.4627, Accuracy: 0.7587\n",
            "Epoch [6/15], Loss: 1.4451, Accuracy: 0.7825\n",
            "Epoch [7/15], Loss: 1.4449, Accuracy: 0.8063\n",
            "Epoch [8/15], Loss: 1.4421, Accuracy: 0.8063\n",
            "Epoch [9/15], Loss: 1.4220, Accuracy: 0.7937\n",
            "Epoch [10/15], Loss: 1.4528, Accuracy: 0.8032\n",
            "Epoch [11/15], Loss: 1.4383, Accuracy: 0.8016\n",
            "Epoch [12/15], Loss: 1.4541, Accuracy: 0.7794\n",
            "Epoch [13/15], Loss: 1.4155, Accuracy: 0.8270\n",
            "Epoch [14/15], Loss: 1.4350, Accuracy: 0.8063\n",
            "Epoch [15/15], Loss: 1.4247, Accuracy: 0.8175\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20979/2865371390.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for criterion in selection_criteria:\n",
        "    AL_class = ActiveLearningPipeline(model=model,\n",
        "                                      test_indices=test_indices,\n",
        "                                      available_pool_indices=available_pool_indices,\n",
        "                                      train_indices=train_indices,\n",
        "                                      selection_criterion=criterion,\n",
        "                                      iterations=iterations,\n",
        "                                      budget_per_iter=budget_per_iter,\n",
        "                                      num_epochs=num_epoch)\n",
        "    accuracy_scores_dict[criterion] = AL_class.run_pipeline()\n",
        "generate_plot(accuracy_scores_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5sKVVjSzEl7",
        "outputId": "5633e42a-8c46-4ad8-c5ef-c81919e76cc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'BADGE_30': [0.5158730158730158,\n",
              "              0.6071428571428571,\n",
              "              0.6388888888888888,\n",
              "              0.611111111111111,\n",
              "              0.6197089947089947,\n",
              "              0.6441798941798942,\n",
              "              0.6362433862433862,\n",
              "              0.6448412698412698,\n",
              "              0.6276455026455026,\n",
              "              0.6011904761904762,\n",
              "              0.6322751322751322,\n",
              "              0.6276455026455026,\n",
              "              0.6104497354497355,\n",
              "              0.6329365079365079,\n",
              "              0.6223544973544973,\n",
              "              0.6428571428571428,\n",
              "              0.6276455026455026,\n",
              "              0.6501322751322751,\n",
              "              0.6157407407407407,\n",
              "              0.6507936507936507]})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_scores_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN47g0f2g7eO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "JjgyTgaRfy4k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import entropy\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import random, torch\n",
    "\n",
    "random.seed(0) # Set seed for NumPy\n",
    "np.random.seed(0) # Set seed for PyTorch (for both CPU and GPU)\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "train_df = pd.read_csv('train_dataset/metadata.csv')\n",
    "test_df = pd.read_csv('test_dataset/metadata.csv')\n",
    "val_df = pd.read_csv('validation_dataset/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actinic keratosis': 0,\n",
       " 'basal cell carcinoma': 1,\n",
       " 'dermatofibroma': 2,\n",
       " 'melanoma': 3,\n",
       " 'nevus': 4,\n",
       " 'pigmented benign keratosis': 5,\n",
       " 'squamous cell carcinoma': 6,\n",
       " 'vascular lesion': 7}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping = {\n",
    "    \"actinic keratosis\": 0,\n",
    "    \"basal cell carcinoma\": 1,\n",
    "    \"dermatofibroma\": 2,\n",
    "    \"melanoma\": 3,\n",
    "    \"nevus\": 4,\n",
    "    \"pigmented benign keratosis\": 5,\n",
    "    \"squamous cell carcinoma\": 6,\n",
    "    \"vascular lesion\":7\n",
    "}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "BLruwetXmPem"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define image transformations (resize, convert to tensor, and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),          # Resize images to 224x224 (matching ResNet input size)\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, dataframe, transform, train='train'):\n",
    "        self.dataframe=dataframe\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.path_to_image=self._create_path_to_image_dict()\n",
    "        self.paths=list(self.path_to_image.keys())\n",
    "        self.labels=list(self.path_to_image.values())\n",
    "\n",
    "    def _create_path_to_image_dict(self):\n",
    "      path_to_image={}\n",
    "      for index,row in self.dataframe.iterrows():\n",
    "        if self.train == 'train':\n",
    "          img_path = os.path.join('train_dataset/',row['isic_id']+'.jpg')\n",
    "        elif self.train == 'test':\n",
    "          img_path = os.path.join('test_dataset/',row['isic_id']+'.jpg')\n",
    "        else:\n",
    "            img_path = os.path.join('validation_dataset/',row['isic_id']+'.jpg')\n",
    "        label=row['diagnosis']\n",
    "        path_to_image[img_path]=label\n",
    "      return path_to_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        img_path=self.paths[index]\n",
    "        img_label=self.labels[index]\n",
    "        image=Image.open(img_path)\n",
    "        image=self.transform(image)\n",
    "        if self.train == 'val':\n",
    "            return image, class_mapping[img_label], index\n",
    "        return image, img_label, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9yCvnprDoa_7"
   },
   "outputs": [],
   "source": [
    "train_df = Dataset(train_df, transform)\n",
    "val_df = Dataset(val_df, transform,train='val')\n",
    "test_df = Dataset(test_df, transform,train='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5L0NcpesnZI",
    "outputId": "22f97091-b571-4138-964f-95a509890583"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Load pre-trained ResNet50 model from torchvision\n",
    "base_model = models.resnet50(pretrained=True)\n",
    "\n",
    "num_classes = 8  # Adjust this based on your dataset\n",
    "base_model.fc = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(base_model.fc.in_features, 128),  # Add a fully connected layer\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, num_classes),  # Final layer with number of classes\n",
    "    nn.Softmax(dim=1)  # Softmax activation for multi-class classification\n",
    ")\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers except the fully connected ones\n",
    "\n",
    "# Unfreeze the final fully connected layer\n",
    "for param in base_model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(base_model.fc.parameters(), lr=0.0008)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "x = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 4  # Adjust based on your memory and hardware\n",
    "val_loader = DataLoader(val_df, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-Vk8kyZsgACa"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "\n",
    "class ActiveLearningPipeline:\n",
    "    def __init__(self, model,\n",
    "                 available_pool_indices,\n",
    "                 train_indices,\n",
    "                 test_indices,\n",
    "                 selection_criterion,\n",
    "                 iterations,\n",
    "                 budget_per_iter,\n",
    "                 num_epochs):\n",
    "        self.model = model\n",
    "        self.iterations = iterations\n",
    "        self.budget_per_iter = budget_per_iter\n",
    "        self.available_pool_indices = available_pool_indices\n",
    "        self.train_indices = train_indices\n",
    "        self.test_indices = test_indices\n",
    "        self.selection_criterion = selection_criterion\n",
    "        if self.selection_criterion == 'random':\n",
    "          self.train_indices = []\n",
    "        self.num_epochs = num_epochs\n",
    "        self.pool_features = []\n",
    "        self.pool_indices = []\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the active learning pipeline\n",
    "        :return\n",
    "        accuracy_scores: list, accuracy scores at each iteration\n",
    "        \"\"\"\n",
    "        accuracy_scores = []\n",
    "        self._get_features()\n",
    "        for iteration in range(self.iterations):\n",
    "            print(f\"--------- Number of Iteration {iteration} ---------\")\n",
    "            if self.selection_criterion == 'random':\n",
    "                self._random_sampling()\n",
    "            elif self.selection_criterion == 'kmeans_num_classes':\n",
    "                self._kmeans_sampling()\n",
    "            else:\n",
    "              self._custom_sampling(iteration)\n",
    "            \n",
    "            train_images = [train_df.__getitem__(index)[0] for index in self.train_indices]\n",
    "            label_df = [class_mapping[train_df.__getitem__(index)[1]] for index in self.train_indices]\n",
    "            self._train_model(train_images, label_df)\n",
    "            self.model.load_state_dict(torch.load(f\"best_{self.selection_criterion}_model.pth\"))\n",
    "            accuracy = self._evaluate_model()\n",
    "            accuracy_scores.append(accuracy)\n",
    "        return accuracy_scores\n",
    "        \n",
    "    def calculate_class_weights(self, label_counts, num_classes=8):\n",
    "        total_samples = sum(label_counts.values())\n",
    "        class_weights = torch.zeros(num_classes)\n",
    "        \n",
    "        for cls in range(num_classes):\n",
    "            if cls in label_counts:\n",
    "                class_weights[cls] = total_samples / (num_classes * label_counts[cls])\n",
    "            else:\n",
    "                class_weights[cls] = 1.0  # Handle the case where a class has zero samples in the current epoch\n",
    "    \n",
    "        return class_weights\n",
    "    \n",
    "    def _train_model(self, train_images, label_df):\n",
    "      label_counts = defaultdict(int)\n",
    "      for label in label_df:\n",
    "                label_counts[label] += 1\n",
    "      class_weights = self.calculate_class_weights(label_counts, 8).to(device)\n",
    "      loss_f = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "      train_images_tensor = torch.stack(train_images)\n",
    "      label_df_tensor = torch.tensor(label_df)\n",
    "      train_dataset = TensorDataset(train_images_tensor, label_df_tensor)\n",
    "\n",
    "      batch_size = 32\n",
    "      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "      best_acc = 0\n",
    "      for epoch in range(self.num_epochs):\n",
    "                self.model.train()\n",
    "                running_loss = 0.0  # Track the running loss\n",
    "                correct_predictions = 0\n",
    "                total_predictions = 0\n",
    "                # Training loop\n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs = inputs\n",
    "                    inputs= inputs.to(device)\n",
    "                    labels = torch.tensor(labels).to(device)\n",
    "                    \n",
    "                    # Zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = loss_f(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimization\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "        \n",
    "                    # Calculate accuracy\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    correct_predictions += torch.sum(preds == labels)\n",
    "                    total_predictions += inputs.shape[0]\n",
    "\n",
    "                # Print loss and accuracy at the end of each epoch\n",
    "                epoch_loss = running_loss / len(train_loader)\n",
    "                epoch_acc = correct_predictions.double() / total_predictions\n",
    "                print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "          \n",
    "                val_acc = self._check_model()\n",
    "                if val_acc > best_acc: \n",
    "                    best_acc = val_acc\n",
    "                    torch.save(self.model.state_dict(), f\"best_{self.selection_criterion}_model.pth\")\n",
    "      print(\"--\"*30)\n",
    "        \n",
    "    def _check_model(self):\n",
    "        self.model.eval()\n",
    "        running_corrects = 0\n",
    "        total_predictions = 0.0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, _ in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = torch.tensor(labels).to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                total_predictions += inputs.shape[0]\n",
    "        val_acc = running_corrects.double() / total_predictions\n",
    "        return val_acc.item()\n",
    "        \n",
    "    def _evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate the model\n",
    "        :return:\n",
    "        accuracy: float, accuracy of the model\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        running_corrects = 0\n",
    "        test_images_tensor = torch.stack(test_images)\n",
    "        label_df_tensor = torch.tensor(test_label_df)\n",
    "        test_dataset = TensorDataset(test_images_tensor, label_df_tensor)\n",
    "        batch_size = 32  # Adjust based on your memory and hardware\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        total_predictions = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = torch.tensor(labels).to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                # outputs = outputs.logits\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                total_predictions += inputs.shape[0]\n",
    "        test_acc = running_corrects.double() / total_predictions\n",
    "        return test_acc.item()\n",
    "\n",
    "    def _random_sampling(self):\n",
    "      selected_indices = np.random.choice(self.available_pool_indices, self.budget_per_iter, replace=False)\n",
    "      selected_indices = selected_indices.tolist()\n",
    "      self.train_indices = self.train_indices + selected_indices\n",
    "\n",
    "      available_pool_set = set(self.available_pool_indices)\n",
    "      train_set = set(self.train_indices)\n",
    "      self.available_pool_indices = list(available_pool_set - train_set)\n",
    "\n",
    "    def extract_vae_features(self, dataloader, model, feature_extractor):\n",
    "        features_list = []\n",
    "        indices_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, indices in dataloader:\n",
    "                # images = images.to(device)  # Move images to GPU if available\n",
    "                images_list = [transforms.ToPILImage()(img) for img in images]\n",
    "                inputs = feature_extractor(images=images_list, return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                \n",
    "                x = outputs.last_hidden_state[:, 0, :]\n",
    "                features_list.append(x.cpu().numpy())\n",
    "                \n",
    "                # Collect indices\n",
    "                indices_list.extend(indices)\n",
    "                \n",
    "        # Stack all features into a 2D array (n_samples, hidden_dim)\n",
    "        features = np.vstack(features_list)\n",
    "        \n",
    "        return features, indices_list\n",
    "\n",
    "    def get_representative_images(self, kmeans, pool_features, pool_indices):\n",
    "        cluster_to_images = {}\n",
    "        for i in range(kmeans.n_clusters):\n",
    "            # Get the indices of all images in the current cluster\n",
    "            cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "            \n",
    "            # Extract features of the images in the current cluster\n",
    "            cluster_features = pool_features[cluster_indices]\n",
    "            \n",
    "            # Compute distances between each feature and the cluster centroid\n",
    "            distances = np.linalg.norm(cluster_features - kmeans.cluster_centers_[i], axis=1)\n",
    "            \n",
    "            # Map the cluster number to the index of the representative image\n",
    "            # nearest_indices = cluster_indices[np.argsort(distances)[:7]]\n",
    "\n",
    "            # Another approach\n",
    "            nearest_indices = cluster_indices[np.argsort(distances)[:4]]\n",
    "            farthest_indices = cluster_indices[np.argsort(distances)[-3:]]\n",
    "            \n",
    "            # Map the cluster number to the indices of the top k nearest images\n",
    "            # cluster_to_images[i] = [pool_indices[idx] for idx in nearest_indices]\n",
    "\n",
    "            # Another approach\n",
    "            cluster_to_images[i] = [pool_indices[idx] for idx in nearest_indices] + [pool_indices[idx] for idx in farthest_indices]\n",
    "        return cluster_to_images\n",
    "       \n",
    "    def _get_features(self):\n",
    "        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        \n",
    "        X_unlabeled = [train_df.__getitem__(index)[0] for index in self.available_pool_indices]\n",
    "        # Extract latent features using the VAE model\n",
    "        pool_images_tensor = torch.stack(X_unlabeled)\n",
    "        pool_indices_tensor = torch.tensor(self.available_pool_indices)\n",
    "        pool_dataset = TensorDataset(pool_images_tensor, pool_indices_tensor)\n",
    "    \n",
    "        batch_size = 32\n",
    "        pool_loader = DataLoader(pool_dataset, batch_size=batch_size, shuffle=False)\n",
    "   \n",
    "        self.pool_features, self.pool_indices = self.extract_vae_features(pool_loader, model, feature_extractor)\n",
    "        \n",
    "    def _kmeans_sampling(self):\n",
    "          \n",
    "          n_clusters = 8 \n",
    "          kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=0)\n",
    "          kmeans.fit(self.pool_features)\n",
    "            \n",
    "          representative_images = self.get_representative_images(kmeans, self.pool_features, self.pool_indices)\n",
    "          selected_indices = list(ids.item() for l in representative_images.values() for ids in l) \n",
    "\n",
    "          for i in selected_indices:\n",
    "              index = self.pool_indices.index(i)\n",
    "              self.pool_features = np.delete(self.pool_features, index, axis=0)\n",
    "              self.pool_indices.pop(index)\n",
    "          \n",
    "          self.train_indices = self.train_indices + selected_indices\n",
    "            \n",
    "          available_pool_set = set(self.available_pool_indices)\n",
    "          train_set = set(self.train_indices)\n",
    "          self.available_pool_indices = list(available_pool_set - train_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "o5829ZYDh1Rp"
   },
   "outputs": [],
   "source": [
    "def generate_plot(accuracy_scores_dict):\n",
    "    \"\"\"\n",
    "    Generate a plot\n",
    "    \"\"\"\n",
    "    for criterion, accuracy_scores in accuracy_scores_dict.items():\n",
    "        plt.plot(range(1, len(accuracy_scores) + 1), accuracy_scores, label=criterion)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "DQ2IZmfRuYgX"
   },
   "outputs": [],
   "source": [
    "available_pool_indices = []\n",
    "for i in range(len(train_df)):\n",
    "    image, label, index = train_df[i]\n",
    "    available_pool_indices.append(index)\n",
    "\n",
    "test_indices = []\n",
    "for i in range(len(test_df)):\n",
    "    image, label, index = test_df[i]\n",
    "    test_indices.append(index)\n",
    "test_images = [test_df.__getitem__(index)[0] for index in test_indices]\n",
    "test_label_df = [class_mapping[test_df.__getitem__(index)[1]] for index in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = [1372,\n",
    " 1277,\n",
    " 1255,\n",
    " 1423,\n",
    " 2925,\n",
    " 1963,\n",
    " 2335,\n",
    " 1923,\n",
    " 3791,\n",
    " 1239,\n",
    " 909,\n",
    " 134,\n",
    " 1547,\n",
    " 3931,\n",
    " 2467,\n",
    " 2832,\n",
    " 1789,\n",
    " 3022,\n",
    " 2424,\n",
    " 780,\n",
    " 2412,\n",
    " 3038,\n",
    " 2158,\n",
    " 3335,\n",
    " 1868,\n",
    " 1771,\n",
    " 2015,\n",
    " 1535,\n",
    " 710,\n",
    " 3007]\n",
    "available_pool_set = set(available_pool_indices)\n",
    "train_set = set(train_indices)\n",
    "available_pool_indices = list(available_pool_set - train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Pmcf2jc6h2S9"
   },
   "outputs": [],
   "source": [
    "# train_indices = []\n",
    "iterations = 20\n",
    "budget_per_iter = 60\n",
    "num_epoch = 15\n",
    "selection_criteria = ['kmeans_num_classes']\n",
    "accuracy_scores_dict = defaultdict(list)\n",
    "model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "4UGO2jW1h5Ql",
    "outputId": "0993cc7b-2fc2-4a84-a31a-ef45ed5febff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_default/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/py38_default/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 0 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 2.0967, Accuracy: 0.1163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 2.0566, Accuracy: 0.3023\n",
      "Epoch [3/15], Loss: 2.0177, Accuracy: 0.6512\n",
      "Epoch [4/15], Loss: 1.9716, Accuracy: 0.7093\n",
      "Epoch [5/15], Loss: 1.9195, Accuracy: 0.7558\n",
      "Epoch [6/15], Loss: 1.8475, Accuracy: 0.6977\n",
      "Epoch [7/15], Loss: 1.7747, Accuracy: 0.7674\n",
      "Epoch [8/15], Loss: 1.7464, Accuracy: 0.8140\n",
      "Epoch [9/15], Loss: 1.7231, Accuracy: 0.7907\n",
      "Epoch [10/15], Loss: 1.6887, Accuracy: 0.8023\n",
      "Epoch [11/15], Loss: 1.6067, Accuracy: 0.8721\n",
      "Epoch [12/15], Loss: 1.6513, Accuracy: 0.8256\n",
      "Epoch [13/15], Loss: 1.5940, Accuracy: 0.8721\n",
      "Epoch [14/15], Loss: 1.5405, Accuracy: 0.8372\n",
      "Epoch [15/15], Loss: 1.5358, Accuracy: 0.8605\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 1 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.7240, Accuracy: 0.7183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.7111, Accuracy: 0.7042\n",
      "Epoch [3/15], Loss: 1.7274, Accuracy: 0.7324\n",
      "Epoch [4/15], Loss: 1.6821, Accuracy: 0.7394\n",
      "Epoch [5/15], Loss: 1.6544, Accuracy: 0.7746\n",
      "Epoch [6/15], Loss: 1.6763, Accuracy: 0.7676\n",
      "Epoch [7/15], Loss: 1.6543, Accuracy: 0.7817\n",
      "Epoch [8/15], Loss: 1.6265, Accuracy: 0.7606\n",
      "Epoch [9/15], Loss: 1.6705, Accuracy: 0.7042\n",
      "Epoch [10/15], Loss: 1.6092, Accuracy: 0.7465\n",
      "Epoch [11/15], Loss: 1.6207, Accuracy: 0.8028\n",
      "Epoch [12/15], Loss: 1.5828, Accuracy: 0.8099\n",
      "Epoch [13/15], Loss: 1.5776, Accuracy: 0.8239\n",
      "Epoch [14/15], Loss: 1.5988, Accuracy: 0.8169\n",
      "Epoch [15/15], Loss: 1.5656, Accuracy: 0.8028\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 2 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6951, Accuracy: 0.6364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.7499, Accuracy: 0.6616\n",
      "Epoch [3/15], Loss: 1.6747, Accuracy: 0.7525\n",
      "Epoch [4/15], Loss: 1.6684, Accuracy: 0.7020\n",
      "Epoch [5/15], Loss: 1.7010, Accuracy: 0.7273\n",
      "Epoch [6/15], Loss: 1.6635, Accuracy: 0.6717\n",
      "Epoch [7/15], Loss: 1.6886, Accuracy: 0.6616\n",
      "Epoch [8/15], Loss: 1.6647, Accuracy: 0.7222\n",
      "Epoch [9/15], Loss: 1.6520, Accuracy: 0.7273\n",
      "Epoch [10/15], Loss: 1.5838, Accuracy: 0.7727\n",
      "Epoch [11/15], Loss: 1.6017, Accuracy: 0.7576\n",
      "Epoch [12/15], Loss: 1.6214, Accuracy: 0.7525\n",
      "Epoch [13/15], Loss: 1.5665, Accuracy: 0.7626\n",
      "Epoch [14/15], Loss: 1.6023, Accuracy: 0.7576\n",
      "Epoch [15/15], Loss: 1.6570, Accuracy: 0.7172\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 3 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6805, Accuracy: 0.6693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.7187, Accuracy: 0.6850\n",
      "Epoch [3/15], Loss: 1.6779, Accuracy: 0.7165\n",
      "Epoch [4/15], Loss: 1.6699, Accuracy: 0.7047\n",
      "Epoch [5/15], Loss: 1.6297, Accuracy: 0.7402\n",
      "Epoch [6/15], Loss: 1.6475, Accuracy: 0.7441\n",
      "Epoch [7/15], Loss: 1.6659, Accuracy: 0.7520\n",
      "Epoch [8/15], Loss: 1.6416, Accuracy: 0.7441\n",
      "Epoch [9/15], Loss: 1.6196, Accuracy: 0.7638\n",
      "Epoch [10/15], Loss: 1.6073, Accuracy: 0.7874\n",
      "Epoch [11/15], Loss: 1.5973, Accuracy: 0.7835\n",
      "Epoch [12/15], Loss: 1.6271, Accuracy: 0.7559\n",
      "Epoch [13/15], Loss: 1.6140, Accuracy: 0.7953\n",
      "Epoch [14/15], Loss: 1.6010, Accuracy: 0.8031\n",
      "Epoch [15/15], Loss: 1.5925, Accuracy: 0.8150\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 4 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6940, Accuracy: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6916, Accuracy: 0.7032\n",
      "Epoch [3/15], Loss: 1.6717, Accuracy: 0.7032\n",
      "Epoch [4/15], Loss: 1.6991, Accuracy: 0.6355\n",
      "Epoch [5/15], Loss: 1.6714, Accuracy: 0.7129\n",
      "Epoch [6/15], Loss: 1.6458, Accuracy: 0.7452\n",
      "Epoch [7/15], Loss: 1.6669, Accuracy: 0.7226\n",
      "Epoch [8/15], Loss: 1.6383, Accuracy: 0.7226\n",
      "Epoch [9/15], Loss: 1.6452, Accuracy: 0.7581\n",
      "Epoch [10/15], Loss: 1.6174, Accuracy: 0.7645\n",
      "Epoch [11/15], Loss: 1.6353, Accuracy: 0.7677\n",
      "Epoch [12/15], Loss: 1.6243, Accuracy: 0.7677\n",
      "Epoch [13/15], Loss: 1.6185, Accuracy: 0.7645\n",
      "Epoch [14/15], Loss: 1.5850, Accuracy: 0.7355\n",
      "Epoch [15/15], Loss: 1.5507, Accuracy: 0.7710\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 5 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6976, Accuracy: 0.7077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.7052, Accuracy: 0.6967\n",
      "Epoch [3/15], Loss: 1.6933, Accuracy: 0.7186\n",
      "Epoch [4/15], Loss: 1.6826, Accuracy: 0.7268\n",
      "Epoch [5/15], Loss: 1.6841, Accuracy: 0.7240\n",
      "Epoch [6/15], Loss: 1.6736, Accuracy: 0.6940\n",
      "Epoch [7/15], Loss: 1.6464, Accuracy: 0.7049\n",
      "Epoch [8/15], Loss: 1.6544, Accuracy: 0.7295\n",
      "Epoch [9/15], Loss: 1.6601, Accuracy: 0.7322\n",
      "Epoch [10/15], Loss: 1.6377, Accuracy: 0.7268\n",
      "Epoch [11/15], Loss: 1.6239, Accuracy: 0.7650\n",
      "Epoch [12/15], Loss: 1.6478, Accuracy: 0.7350\n",
      "Epoch [13/15], Loss: 1.6194, Accuracy: 0.7541\n",
      "Epoch [14/15], Loss: 1.6511, Accuracy: 0.7295\n",
      "Epoch [15/15], Loss: 1.6673, Accuracy: 0.7186\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 6 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6546, Accuracy: 0.7133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6473, Accuracy: 0.7464\n",
      "Epoch [3/15], Loss: 1.6077, Accuracy: 0.7701\n",
      "Epoch [4/15], Loss: 1.6160, Accuracy: 0.7275\n",
      "Epoch [5/15], Loss: 1.6241, Accuracy: 0.7393\n",
      "Epoch [6/15], Loss: 1.6311, Accuracy: 0.7227\n",
      "Epoch [7/15], Loss: 1.6333, Accuracy: 0.7607\n",
      "Epoch [8/15], Loss: 1.6027, Accuracy: 0.7678\n",
      "Epoch [9/15], Loss: 1.6083, Accuracy: 0.7678\n",
      "Epoch [10/15], Loss: 1.6285, Accuracy: 0.7346\n",
      "Epoch [11/15], Loss: 1.5903, Accuracy: 0.7417\n",
      "Epoch [12/15], Loss: 1.6135, Accuracy: 0.7085\n",
      "Epoch [13/15], Loss: 1.5900, Accuracy: 0.7678\n",
      "Epoch [14/15], Loss: 1.5406, Accuracy: 0.8009\n",
      "Epoch [15/15], Loss: 1.5796, Accuracy: 0.7725\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 7 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.6338, Accuracy: 0.6799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.6037, Accuracy: 0.7510\n",
      "Epoch [3/15], Loss: 1.5932, Accuracy: 0.7699\n",
      "Epoch [4/15], Loss: 1.5512, Accuracy: 0.7531\n",
      "Epoch [5/15], Loss: 1.5494, Accuracy: 0.7364\n",
      "Epoch [6/15], Loss: 1.5283, Accuracy: 0.7762\n",
      "Epoch [7/15], Loss: 1.4890, Accuracy: 0.7803\n",
      "Epoch [8/15], Loss: 1.4881, Accuracy: 0.7845\n",
      "Epoch [9/15], Loss: 1.4888, Accuracy: 0.7824\n",
      "Epoch [10/15], Loss: 1.5024, Accuracy: 0.7615\n",
      "Epoch [11/15], Loss: 1.4786, Accuracy: 0.7845\n",
      "Epoch [12/15], Loss: 1.4906, Accuracy: 0.7762\n",
      "Epoch [13/15], Loss: 1.4601, Accuracy: 0.8033\n",
      "Epoch [14/15], Loss: 1.4435, Accuracy: 0.8180\n",
      "Epoch [15/15], Loss: 1.4469, Accuracy: 0.8222\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 8 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5131, Accuracy: 0.7584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5226, Accuracy: 0.7734\n",
      "Epoch [3/15], Loss: 1.5041, Accuracy: 0.7734\n",
      "Epoch [4/15], Loss: 1.5058, Accuracy: 0.7715\n",
      "Epoch [5/15], Loss: 1.4844, Accuracy: 0.8034\n",
      "Epoch [6/15], Loss: 1.4967, Accuracy: 0.7697\n",
      "Epoch [7/15], Loss: 1.4793, Accuracy: 0.7828\n",
      "Epoch [8/15], Loss: 1.4852, Accuracy: 0.7659\n",
      "Epoch [9/15], Loss: 1.4564, Accuracy: 0.8052\n",
      "Epoch [10/15], Loss: 1.4643, Accuracy: 0.8090\n",
      "Epoch [11/15], Loss: 1.4635, Accuracy: 0.8277\n",
      "Epoch [12/15], Loss: 1.4509, Accuracy: 0.8090\n",
      "Epoch [13/15], Loss: 1.4425, Accuracy: 0.8202\n",
      "Epoch [14/15], Loss: 1.4434, Accuracy: 0.8333\n",
      "Epoch [15/15], Loss: 1.4412, Accuracy: 0.8296\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 9 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5079, Accuracy: 0.7831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5337, Accuracy: 0.7576\n",
      "Epoch [3/15], Loss: 1.5265, Accuracy: 0.7339\n",
      "Epoch [4/15], Loss: 1.5080, Accuracy: 0.7695\n",
      "Epoch [5/15], Loss: 1.5052, Accuracy: 0.7593\n",
      "Epoch [6/15], Loss: 1.4909, Accuracy: 0.8085\n",
      "Epoch [7/15], Loss: 1.4932, Accuracy: 0.7966\n",
      "Epoch [8/15], Loss: 1.5130, Accuracy: 0.7610\n",
      "Epoch [9/15], Loss: 1.5198, Accuracy: 0.6932\n",
      "Epoch [10/15], Loss: 1.4971, Accuracy: 0.7576\n",
      "Epoch [11/15], Loss: 1.4768, Accuracy: 0.7983\n",
      "Epoch [12/15], Loss: 1.4846, Accuracy: 0.7915\n",
      "Epoch [13/15], Loss: 1.4625, Accuracy: 0.8136\n",
      "Epoch [14/15], Loss: 1.4795, Accuracy: 0.7797\n",
      "Epoch [15/15], Loss: 1.4777, Accuracy: 0.7847\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 10 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5651, Accuracy: 0.7229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5607, Accuracy: 0.7508\n",
      "Epoch [3/15], Loss: 1.5398, Accuracy: 0.7523\n",
      "Epoch [4/15], Loss: 1.5550, Accuracy: 0.7461\n",
      "Epoch [5/15], Loss: 1.5563, Accuracy: 0.7198\n",
      "Epoch [6/15], Loss: 1.5219, Accuracy: 0.7570\n",
      "Epoch [7/15], Loss: 1.5869, Accuracy: 0.6053\n",
      "Epoch [8/15], Loss: 1.5388, Accuracy: 0.7229\n",
      "Epoch [9/15], Loss: 1.5088, Accuracy: 0.7771\n",
      "Epoch [10/15], Loss: 1.5060, Accuracy: 0.7941\n",
      "Epoch [11/15], Loss: 1.5048, Accuracy: 0.7663\n",
      "Epoch [12/15], Loss: 1.5204, Accuracy: 0.7647\n",
      "Epoch [13/15], Loss: 1.5234, Accuracy: 0.7167\n",
      "Epoch [14/15], Loss: 1.4812, Accuracy: 0.8003\n",
      "Epoch [15/15], Loss: 1.4756, Accuracy: 0.8065\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 11 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5681, Accuracy: 0.7208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5772, Accuracy: 0.6909\n",
      "Epoch [3/15], Loss: 1.5432, Accuracy: 0.7650\n",
      "Epoch [4/15], Loss: 1.5311, Accuracy: 0.7507\n",
      "Epoch [5/15], Loss: 1.5257, Accuracy: 0.7792\n",
      "Epoch [6/15], Loss: 1.5100, Accuracy: 0.7821\n",
      "Epoch [7/15], Loss: 1.5245, Accuracy: 0.7536\n",
      "Epoch [8/15], Loss: 1.5374, Accuracy: 0.7707\n",
      "Epoch [9/15], Loss: 1.5060, Accuracy: 0.7735\n",
      "Epoch [10/15], Loss: 1.4886, Accuracy: 0.7906\n",
      "Epoch [11/15], Loss: 1.5034, Accuracy: 0.7664\n",
      "Epoch [12/15], Loss: 1.5060, Accuracy: 0.7792\n",
      "Epoch [13/15], Loss: 1.4883, Accuracy: 0.7863\n",
      "Epoch [14/15], Loss: 1.4903, Accuracy: 0.7991\n",
      "Epoch [15/15], Loss: 1.4935, Accuracy: 0.7877\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 12 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5178, Accuracy: 0.7718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.4990, Accuracy: 0.7784\n",
      "Epoch [3/15], Loss: 1.4946, Accuracy: 0.7968\n",
      "Epoch [4/15], Loss: 1.4959, Accuracy: 0.7876\n",
      "Epoch [5/15], Loss: 1.4894, Accuracy: 0.7995\n",
      "Epoch [6/15], Loss: 1.4955, Accuracy: 0.7955\n",
      "Epoch [7/15], Loss: 1.4992, Accuracy: 0.7902\n",
      "Epoch [8/15], Loss: 1.4711, Accuracy: 0.8127\n",
      "Epoch [9/15], Loss: 1.4729, Accuracy: 0.8179\n",
      "Epoch [10/15], Loss: 1.5000, Accuracy: 0.7982\n",
      "Epoch [11/15], Loss: 1.4859, Accuracy: 0.7850\n",
      "Epoch [12/15], Loss: 1.4906, Accuracy: 0.8127\n",
      "Epoch [13/15], Loss: 1.4666, Accuracy: 0.8166\n",
      "Epoch [14/15], Loss: 1.4710, Accuracy: 0.8021\n",
      "Epoch [15/15], Loss: 1.5013, Accuracy: 0.7942\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 13 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.4927, Accuracy: 0.7973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.4839, Accuracy: 0.8170\n",
      "Epoch [3/15], Loss: 1.4880, Accuracy: 0.7973\n",
      "Epoch [4/15], Loss: 1.4896, Accuracy: 0.7899\n",
      "Epoch [5/15], Loss: 1.4889, Accuracy: 0.7912\n",
      "Epoch [6/15], Loss: 1.4834, Accuracy: 0.8145\n",
      "Epoch [7/15], Loss: 1.4626, Accuracy: 0.8292\n",
      "Epoch [8/15], Loss: 1.4747, Accuracy: 0.8071\n",
      "Epoch [9/15], Loss: 1.5026, Accuracy: 0.7948\n",
      "Epoch [10/15], Loss: 1.4742, Accuracy: 0.8145\n",
      "Epoch [11/15], Loss: 1.4748, Accuracy: 0.8157\n",
      "Epoch [12/15], Loss: 1.4732, Accuracy: 0.8034\n",
      "Epoch [13/15], Loss: 1.4648, Accuracy: 0.8378\n",
      "Epoch [14/15], Loss: 1.4858, Accuracy: 0.7985\n",
      "Epoch [15/15], Loss: 1.4593, Accuracy: 0.8243\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 14 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5086, Accuracy: 0.7678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5033, Accuracy: 0.8034\n",
      "Epoch [3/15], Loss: 1.4812, Accuracy: 0.7690\n",
      "Epoch [4/15], Loss: 1.4865, Accuracy: 0.7782\n",
      "Epoch [5/15], Loss: 1.4838, Accuracy: 0.7885\n",
      "Epoch [6/15], Loss: 1.4868, Accuracy: 0.7609\n",
      "Epoch [7/15], Loss: 1.4648, Accuracy: 0.8023\n",
      "Epoch [8/15], Loss: 1.4807, Accuracy: 0.7954\n",
      "Epoch [9/15], Loss: 1.4964, Accuracy: 0.7609\n",
      "Epoch [10/15], Loss: 1.4772, Accuracy: 0.8046\n",
      "Epoch [11/15], Loss: 1.4634, Accuracy: 0.8115\n",
      "Epoch [12/15], Loss: 1.4683, Accuracy: 0.7805\n",
      "Epoch [13/15], Loss: 1.4502, Accuracy: 0.8046\n",
      "Epoch [14/15], Loss: 1.4674, Accuracy: 0.8184\n",
      "Epoch [15/15], Loss: 1.4894, Accuracy: 0.7828\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 15 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5016, Accuracy: 0.7711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.4923, Accuracy: 0.7927\n",
      "Epoch [3/15], Loss: 1.4742, Accuracy: 0.8013\n",
      "Epoch [4/15], Loss: 1.4867, Accuracy: 0.7991\n",
      "Epoch [5/15], Loss: 1.4953, Accuracy: 0.7851\n",
      "Epoch [6/15], Loss: 1.4783, Accuracy: 0.7937\n",
      "Epoch [7/15], Loss: 1.4683, Accuracy: 0.8121\n",
      "Epoch [8/15], Loss: 1.4720, Accuracy: 0.8121\n",
      "Epoch [9/15], Loss: 1.4715, Accuracy: 0.7905\n",
      "Epoch [10/15], Loss: 1.4752, Accuracy: 0.8002\n",
      "Epoch [11/15], Loss: 1.4555, Accuracy: 0.8153\n",
      "Epoch [12/15], Loss: 1.4658, Accuracy: 0.8110\n",
      "Epoch [13/15], Loss: 1.4605, Accuracy: 0.8251\n",
      "Epoch [14/15], Loss: 1.4488, Accuracy: 0.8207\n",
      "Epoch [15/15], Loss: 1.4527, Accuracy: 0.8186\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 16 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5071, Accuracy: 0.7770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5010, Accuracy: 0.7566\n",
      "Epoch [3/15], Loss: 1.4897, Accuracy: 0.7902\n",
      "Epoch [4/15], Loss: 1.4815, Accuracy: 0.7811\n",
      "Epoch [5/15], Loss: 1.4992, Accuracy: 0.7770\n",
      "Epoch [6/15], Loss: 1.4864, Accuracy: 0.7770\n",
      "Epoch [7/15], Loss: 1.4764, Accuracy: 0.8035\n",
      "Epoch [8/15], Loss: 1.4627, Accuracy: 0.8116\n",
      "Epoch [9/15], Loss: 1.4623, Accuracy: 0.8075\n",
      "Epoch [10/15], Loss: 1.4761, Accuracy: 0.7851\n",
      "Epoch [11/15], Loss: 1.4548, Accuracy: 0.8065\n",
      "Epoch [12/15], Loss: 1.4722, Accuracy: 0.7953\n",
      "Epoch [13/15], Loss: 1.4570, Accuracy: 0.8065\n",
      "Epoch [14/15], Loss: 1.4577, Accuracy: 0.8218\n",
      "Epoch [15/15], Loss: 1.4979, Accuracy: 0.7760\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 17 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5005, Accuracy: 0.7649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.4792, Accuracy: 0.7948\n",
      "Epoch [3/15], Loss: 1.4873, Accuracy: 0.7784\n",
      "Epoch [4/15], Loss: 1.4987, Accuracy: 0.7909\n",
      "Epoch [5/15], Loss: 1.4609, Accuracy: 0.8112\n",
      "Epoch [6/15], Loss: 1.4661, Accuracy: 0.8054\n",
      "Epoch [7/15], Loss: 1.4651, Accuracy: 0.8150\n",
      "Epoch [8/15], Loss: 1.4756, Accuracy: 0.7881\n",
      "Epoch [9/15], Loss: 1.4891, Accuracy: 0.7890\n",
      "Epoch [10/15], Loss: 1.4664, Accuracy: 0.8015\n",
      "Epoch [11/15], Loss: 1.4665, Accuracy: 0.7900\n",
      "Epoch [12/15], Loss: 1.4687, Accuracy: 0.8170\n",
      "Epoch [13/15], Loss: 1.4691, Accuracy: 0.8073\n",
      "Epoch [14/15], Loss: 1.4959, Accuracy: 0.7803\n",
      "Epoch [15/15], Loss: 1.4609, Accuracy: 0.8092\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 18 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.5101, Accuracy: 0.7642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.5142, Accuracy: 0.7651\n",
      "Epoch [3/15], Loss: 1.5066, Accuracy: 0.7724\n",
      "Epoch [4/15], Loss: 1.5119, Accuracy: 0.7495\n",
      "Epoch [5/15], Loss: 1.4948, Accuracy: 0.7697\n",
      "Epoch [6/15], Loss: 1.5160, Accuracy: 0.7386\n",
      "Epoch [7/15], Loss: 1.4901, Accuracy: 0.7761\n",
      "Epoch [8/15], Loss: 1.4792, Accuracy: 0.8044\n",
      "Epoch [9/15], Loss: 1.4857, Accuracy: 0.7687\n",
      "Epoch [10/15], Loss: 1.4852, Accuracy: 0.7934\n",
      "Epoch [11/15], Loss: 1.4877, Accuracy: 0.7907\n",
      "Epoch [12/15], Loss: 1.4936, Accuracy: 0.7633\n",
      "Epoch [13/15], Loss: 1.4665, Accuracy: 0.8108\n",
      "Epoch [14/15], Loss: 1.4829, Accuracy: 0.7916\n",
      "Epoch [15/15], Loss: 1.4597, Accuracy: 0.8062\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Number of Iteration 19 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.4891, Accuracy: 0.7791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 1.4787, Accuracy: 0.8026\n",
      "Epoch [3/15], Loss: 1.4687, Accuracy: 0.7948\n",
      "Epoch [4/15], Loss: 1.4867, Accuracy: 0.7843\n",
      "Epoch [5/15], Loss: 1.4682, Accuracy: 0.8078\n",
      "Epoch [6/15], Loss: 1.4983, Accuracy: 0.7713\n",
      "Epoch [7/15], Loss: 1.4787, Accuracy: 0.7965\n",
      "Epoch [8/15], Loss: 1.4693, Accuracy: 0.8113\n",
      "Epoch [9/15], Loss: 1.4553, Accuracy: 0.8243\n",
      "Epoch [10/15], Loss: 1.4695, Accuracy: 0.8043\n",
      "Epoch [11/15], Loss: 1.4703, Accuracy: 0.8139\n",
      "Epoch [12/15], Loss: 1.4651, Accuracy: 0.8122\n",
      "Epoch [13/15], Loss: 1.4598, Accuracy: 0.8078\n",
      "Epoch [14/15], Loss: 1.4679, Accuracy: 0.8122\n",
      "Epoch [15/15], Loss: 1.4600, Accuracy: 0.8113\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_303787/3124605072.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for criterion in selection_criteria:\n",
    "    AL_class = ActiveLearningPipeline(model=model,\n",
    "                                      test_indices=test_indices,\n",
    "                                      available_pool_indices=available_pool_indices,\n",
    "                                      train_indices=train_indices,\n",
    "                                      selection_criterion=criterion,\n",
    "                                      iterations=iterations,\n",
    "                                      budget_per_iter=budget_per_iter,\n",
    "                                      num_epochs=num_epoch)\n",
    "    accuracy_scores_dict[criterion] = AL_class.run_pipeline()\n",
    "generate_plot(accuracy_scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "j5sKVVjSzEl7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'kmeans_num_classes': [0.5608465608465608,\n",
       "              0.6355820105820106,\n",
       "              0.6335978835978836,\n",
       "              0.6382275132275131,\n",
       "              0.625,\n",
       "              0.6395502645502645,\n",
       "              0.6461640211640212,\n",
       "              0.6699735449735449,\n",
       "              0.6124338624338624,\n",
       "              0.6693121693121693,\n",
       "              0.6554232804232804,\n",
       "              0.6157407407407407,\n",
       "              0.6402116402116402,\n",
       "              0.6699735449735449,\n",
       "              0.6633597883597884,\n",
       "              0.6329365079365079,\n",
       "              0.6309523809523809,\n",
       "              0.6283068783068783,\n",
       "              0.6283068783068783,\n",
       "              0.6435185185185185]})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans 8 nearst 7 0.6355820105820106\n",
    "# kmeans 8 nearst 4 farthest 3 0.669"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

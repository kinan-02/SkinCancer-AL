{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjgyTgaRfy4k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import entropy\n",
        "from collections import defaultdict\n",
        "import argparse\n",
        "from matplotlib import pyplot as plt\n",
        "import random, torch\n",
        "\n",
        "random.seed(0) # Set seed for NumPy\n",
        "np.random.seed(0) # Set seed for PyTorch (for both CPU and GPU)\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "train_df = pd.read_csv('train_dataset/metadata.csv')\n",
        "test_df = pd.read_csv('test_dataset/metadata.csv')\n",
        "val_df = pd.read_csv('validation_dataset/metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "bcHCCahYj6B7",
        "outputId": "1facd164-3e8b-40f4-81d7-52fc3110e538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diagnosis\n",
            "nevus                         1205\n",
            "melanoma                      1113\n",
            "pigmented benign keratosis    1099\n",
            "basal cell carcinoma           514\n",
            "squamous cell carcinoma        197\n",
            "vascular lesion                142\n",
            "actinic keratosis              130\n",
            "dermatofibroma                 115\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_df['diagnosis'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkUCttr0qzCw",
        "outputId": "7810745d-6f48-4902-9cd2-33d72026652c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'actinic keratosis': 0,\n",
              " 'basal cell carcinoma': 1,\n",
              " 'dermatofibroma': 2,\n",
              " 'melanoma': 3,\n",
              " 'nevus': 4,\n",
              " 'pigmented benign keratosis': 5,\n",
              " 'squamous cell carcinoma': 6,\n",
              " 'vascular lesion': 7}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_mapping = {\n",
        "    \"actinic keratosis\": 0,\n",
        "    \"basal cell carcinoma\": 1,\n",
        "    \"dermatofibroma\": 2,\n",
        "    \"melanoma\": 3,\n",
        "    \"nevus\": 4,\n",
        "    \"pigmented benign keratosis\": 5,\n",
        "    \"squamous cell carcinoma\": 6,\n",
        "    \"vascular lesion\":7\n",
        "}\n",
        "class_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLruwetXmPem"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define image transformations (resize, convert to tensor, and normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),          # Resize images to 224x224 (matching ResNet input size)\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class Dataset():\n",
        "    \"\"\"\n",
        "    This class is to read the data and pass an instance of it later to the Dataloader\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe, transform, train='train'):\n",
        "        self.dataframe=dataframe\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.path_to_image=self._create_path_to_image_dict()\n",
        "        self.paths=list(self.path_to_image.keys())\n",
        "        self.labels=list(self.path_to_image.values())\n",
        "\n",
        "    def _create_path_to_image_dict(self):\n",
        "      path_to_image={}\n",
        "      for index,row in self.dataframe.iterrows():\n",
        "        if self.train == 'train':\n",
        "          img_path = os.path.join('train_dataset/',row['isic_id']+'.jpg')\n",
        "        elif self.train == 'test':\n",
        "          img_path = os.path.join('test_dataset/',row['isic_id']+'.jpg')\n",
        "        else:\n",
        "            img_path = os.path.join('validation_dataset/',row['isic_id']+'.jpg')\n",
        "        label=row['diagnosis']\n",
        "        path_to_image[img_path]=label\n",
        "      return path_to_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        img_path=self.paths[index]\n",
        "        img_label=self.labels[index]\n",
        "        image=Image.open(img_path)\n",
        "        image=self.transform(image)\n",
        "        if self.train == 'val':\n",
        "            return image, class_mapping[img_label], index\n",
        "        return image, img_label, index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yCvnprDoa_7"
      },
      "outputs": [],
      "source": [
        "train_df = Dataset(train_df, transform)\n",
        "val_df = Dataset(val_df, transform,train='val')\n",
        "test_df = Dataset(test_df, transform,train='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5L0NcpesnZI",
        "outputId": "22f97091-b571-4138-964f-95a509890583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Load pre-trained ResNet50 model from torchvision\n",
        "base_model = models.resnet50(pretrained=True)\n",
        "\n",
        "num_classes = 8\n",
        "#replacing the last fc in the pretrained base model\n",
        "base_model.fc = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(base_model.fc.in_features, 128),  # Add a fully connected layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, num_classes),  # Final layer with number of classes\n",
        "    nn.Softmax(dim=1)  # Softmax activation for multi-class classification\n",
        ")\n",
        "\n",
        "for param in base_model.parameters():\n",
        "    param.requires_grad = False  # Freeze all layers except the fully connected ones\n",
        "\n",
        "# Unfreeze the final fully connected layer\n",
        "for param in base_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(base_model.fc.parameters(), lr=0.0008)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tETv7INHqzCy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 4\n",
        "val_loader = DataLoader(val_df, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vk8kyZsgACa"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "class ActiveLearningPipeline:\n",
        "    def __init__(self, model,\n",
        "                 available_pool_indices,\n",
        "                 train_indices,\n",
        "                 test_indices,\n",
        "                 selection_criterion,\n",
        "                 iterations,\n",
        "                 budget_per_iter,\n",
        "                 num_epochs):\n",
        "        self.model = model\n",
        "        self.iterations = iterations\n",
        "        self.budget_per_iter = budget_per_iter\n",
        "        self.available_pool_indices = available_pool_indices\n",
        "        self.train_indices = train_indices\n",
        "        self.test_indices = test_indices\n",
        "        self.selection_criterion = selection_criterion\n",
        "        if self.selection_criterion == 'random':\n",
        "          self.train_indices = []\n",
        "        self.num_epochs = num_epochs\n",
        "        self.pool_features = []\n",
        "        self.pool_indices = []\n",
        "        # self.best_acc = 0\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"\n",
        "        Run the active learning pipeline\n",
        "        :return\n",
        "        accuracy_scores: list, accuracy scores at each iteration\n",
        "        \"\"\"\n",
        "        accuracy_scores = []\n",
        "        # auto_encoder =  Autoencoder(256)\n",
        "        # auto_encoder.load_state_dict(torch.load('vae_model.pth'))\n",
        "        self._get_features()\n",
        "        for iteration in range(self.iterations):\n",
        "            # if len(self.train_indices) > 600:\n",
        "            #     # raise error if the train set is larger than 600 samples\n",
        "            #     raise ValueError('The train set is larger than 600 samples')\n",
        "            print(f\"--------- Number of Iteration {iteration} ---------\")\n",
        "            if self.selection_criterion == 'random':\n",
        "                self._random_sampling()\n",
        "            elif self.selection_criterion == 'kmeans_budget':\n",
        "                self._kmeans_sampling()\n",
        "            else:\n",
        "              self._custom_sampling(iteration)\n",
        "\n",
        "            train_images = [train_df.__getitem__(index)[0] for index in self.train_indices]\n",
        "            label_df = [class_mapping[train_df.__getitem__(index)[1]] for index in self.train_indices]\n",
        "            self._train_model(train_images, label_df)\n",
        "            #loading the best model weights on the validation set in each iteration\n",
        "            self.model.load_state_dict(torch.load(f\"best_{self.selection_criterion}_model.pth\"))\n",
        "            accuracy = self._evaluate_model()\n",
        "            accuracy_scores.append(accuracy)\n",
        "        return accuracy_scores\n",
        "\n",
        "    def calculate_class_weights(self, label_counts, num_classes=8):\n",
        "        \"\"\"\n",
        "        Calculating the inverse of the probability of each class in the dataset\n",
        "        \"\"\"\n",
        "        total_samples = sum(label_counts.values())\n",
        "        class_weights = torch.zeros(num_classes)\n",
        "\n",
        "        for cls in range(num_classes):\n",
        "            if cls in label_counts:\n",
        "                class_weights[cls] = total_samples / (num_classes * label_counts[cls])\n",
        "            else:\n",
        "                class_weights[cls] = 1.0  # Handle the case where a class has zero samples in the current epoch\n",
        "\n",
        "        return class_weights\n",
        "\n",
        "    def _train_model(self, train_images, label_df):\n",
        "      label_counts = defaultdict(int)\n",
        "      for label in label_df:\n",
        "                label_counts[label] += 1\n",
        "      class_weights = self.calculate_class_weights(label_counts, 8).to(device)\n",
        "      loss_f = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "      train_images_tensor = torch.stack(train_images)\n",
        "      label_df_tensor = torch.tensor(label_df)\n",
        "      train_dataset = TensorDataset(train_images_tensor, label_df_tensor)\n",
        "\n",
        "      batch_size = 32\n",
        "      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "      best_acc = 0\n",
        "      for epoch in range(self.num_epochs):\n",
        "                self.model.train()\n",
        "                running_loss = 0.0  # Track the running loss\n",
        "                correct_predictions = 0\n",
        "                total_predictions = 0\n",
        "                # Training loop\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs = inputs\n",
        "                    inputs= inputs.to(device)\n",
        "                    labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                    # Zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = loss_f(outputs, labels)\n",
        "\n",
        "                    # Backward pass and optimization\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    correct_predictions += torch.sum(preds == labels)\n",
        "                    total_predictions += inputs.shape[0]\n",
        "\n",
        "                # Print loss and accuracy at the end of each epoch\n",
        "                epoch_loss = running_loss / len(train_loader)\n",
        "                epoch_acc = correct_predictions.double() / total_predictions\n",
        "                print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "                #saving the best model on the validation\n",
        "                val_acc = self._check_model()\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "                    torch.save(self.model.state_dict(), f\"best_{self.selection_criterion}_model.pth\")\n",
        "      print(\"--\"*30)\n",
        "\n",
        "    def _check_model(self):\n",
        "        \"\"\"\n",
        "        Evaluating the model on the validation set\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        running_corrects = 0\n",
        "        total_predictions = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, _ in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                running_corrects += torch.sum(preds == labels)\n",
        "                total_predictions += inputs.shape[0]\n",
        "        val_acc = running_corrects.double() / total_predictions\n",
        "        return val_acc.item()\n",
        "\n",
        "    def _evaluate_model(self):\n",
        "        \"\"\"\n",
        "        Evaluate the model on the test set\n",
        "        :return:\n",
        "        accuracy: float, accuracy of the model\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        running_corrects = 0\n",
        "        test_images_tensor = torch.stack(test_images)\n",
        "        label_df_tensor = torch.tensor(test_label_df)\n",
        "        test_dataset = TensorDataset(test_images_tensor, label_df_tensor)\n",
        "        batch_size = 32\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "        total_predictions = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                running_corrects += torch.sum(preds == labels)\n",
        "                total_predictions += inputs.shape[0]\n",
        "        test_acc = running_corrects.double() / total_predictions\n",
        "        return test_acc.item()\n",
        "\n",
        "    def _random_sampling(self):\n",
        "      selected_indices = np.random.choice(self.available_pool_indices, self.budget_per_iter, replace=False)\n",
        "      selected_indices = selected_indices.tolist()\n",
        "      self.train_indices = self.train_indices + selected_indices\n",
        "\n",
        "      available_pool_set = set(self.available_pool_indices)\n",
        "      train_set = set(self.train_indices)\n",
        "      self.available_pool_indices = list(available_pool_set - train_set)\n",
        "\n",
        "    def extract_vae_features(self, dataloader, model, feature_extractor):\n",
        "        \"\"\"\n",
        "        Extracting the Representation of the images as a victor based on the feature_extractor which is the embedding model\n",
        "        return the victors of the images we extracted and the indices\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "        indices_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, indices in dataloader:\n",
        "                images = images.to(device)  # Move images to GPU if available\n",
        "                images_list = [transforms.ToPILImage()(img) for img in images]\n",
        "                inputs = feature_extractor(images=images_list, return_tensors=\"pt\")\n",
        "                inputs = inputs.to(device)\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**inputs)\n",
        "\n",
        "                x = outputs.last_hidden_state[:, 0, :]\n",
        "                features_list.append(x.cpu().numpy())\n",
        "\n",
        "                # Collect indices\n",
        "                indices_list.extend(indices)\n",
        "\n",
        "        # Stack all features into a 2D array (n_samples, hidden_dim)\n",
        "        features = np.vstack(features_list)\n",
        "\n",
        "        return features, indices_list\n",
        "\n",
        "\n",
        "    def get_representative_images(self, kmeans, pool_features, pool_indices):\n",
        "        \"\"\"\n",
        "         returns a dictionary where the keys are the index of the cluster and the values are the closest images to\n",
        "         each centroid note that the K of the KMeans is the budget per iteration.\n",
        "        \"\"\"\n",
        "        cluster_to_images = {}\n",
        "        for i in range(kmeans.n_clusters):\n",
        "            # Get the indices of all images in the current cluster\n",
        "            cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
        "\n",
        "            # Extract features of the images in the current cluster\n",
        "            cluster_features = pool_features[cluster_indices]\n",
        "\n",
        "            # Compute distances between each feature and the cluster centroid\n",
        "            distances = np.linalg.norm(cluster_features - kmeans.cluster_centers_[i], axis=1)\n",
        "\n",
        "            # Map the cluster number to the index of the representative image\n",
        "            nearest_indices = cluster_indices[np.argsort(distances)[:1]]\n",
        "\n",
        "            # Map the cluster number to the indices of the top k nearest images\n",
        "            cluster_to_images[i] = [pool_indices[idx] for idx in nearest_indices]\n",
        "\n",
        "        return cluster_to_images\n",
        "\n",
        "    def _get_features(self):\n",
        "        #loading the pretrained Vit model to get images embeddings\n",
        "        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        model = model.to(device)\n",
        "\n",
        "        X_unlabeled = [train_df.__getitem__(index)[0] for index in self.available_pool_indices]\n",
        "        # Extract latent features using the Vit model\n",
        "        pool_images_tensor = torch.stack(X_unlabeled)\n",
        "        pool_indices_tensor = torch.tensor(self.available_pool_indices)\n",
        "        pool_dataset = TensorDataset(pool_images_tensor, pool_indices_tensor)\n",
        "\n",
        "        batch_size = 32\n",
        "        pool_loader = DataLoader(pool_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        self.pool_features, self.pool_indices = self.extract_vae_features(pool_loader, model, feature_extractor)\n",
        "\n",
        "    def _kmeans_sampling(self):\n",
        "          n_clusters = self.budget_per_iter\n",
        "          kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=0)\n",
        "          kmeans.fit(self.pool_features)\n",
        "\n",
        "          representative_images = self.get_representative_images(kmeans, self.pool_features, self.pool_indices)\n",
        "          selected_indices = list(ids.item() for l in representative_images.values() for ids in l)\n",
        "\n",
        "          for i in selected_indices:\n",
        "              index = self.pool_indices.index(i)\n",
        "              self.pool_features = np.delete(self.pool_features, index, axis=0)\n",
        "              self.pool_indices.pop(index)\n",
        "          self.train_indices = self.train_indices + selected_indices\n",
        "\n",
        "          available_pool_set = set(self.available_pool_indices)\n",
        "          train_set = set(self.train_indices)\n",
        "          self.available_pool_indices = list(available_pool_set - train_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5829ZYDh1Rp"
      },
      "outputs": [],
      "source": [
        "def generate_plot(accuracy_scores_dict):\n",
        "    \"\"\"\n",
        "    Generate a plot\n",
        "    \"\"\"\n",
        "    for criterion, accuracy_scores in accuracy_scores_dict.items():\n",
        "        plt.plot(range(1, len(accuracy_scores) + 1), accuracy_scores, label=criterion)\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ2IZmfRuYgX"
      },
      "outputs": [],
      "source": [
        "available_pool_indices = []\n",
        "for i in range(len(train_df)):\n",
        "    image, label, index = train_df[i]\n",
        "    available_pool_indices.append(index)\n",
        "\n",
        "test_indices = []\n",
        "for i in range(len(test_df)):\n",
        "    image, label, index = test_df[i]\n",
        "    test_indices.append(index)\n",
        "test_images = [test_df.__getitem__(index)[0] for index in test_indices]\n",
        "test_label_df = [class_mapping[test_df.__getitem__(index)[1]] for index in test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeFwF0yzqzC1"
      },
      "outputs": [],
      "source": [
        "#According to the initialization that we got using KMeans++ Code of the initialization could be found in the initials file.\n",
        "train_indices = [1372,\n",
        " 1277,\n",
        " 1255,\n",
        " 1423,\n",
        " 2925,\n",
        " 1963,\n",
        " 2335,\n",
        " 1923,\n",
        " 3791,\n",
        " 1239,\n",
        " 909,\n",
        " 134,\n",
        " 1547,\n",
        " 3931,\n",
        " 2467,\n",
        " 2832,\n",
        " 1789,\n",
        " 3022,\n",
        " 2424,\n",
        " 780,\n",
        " 2412,\n",
        " 3038,\n",
        " 2158,\n",
        " 3335,\n",
        " 1868,\n",
        " 1771,\n",
        " 2015,\n",
        " 1535,\n",
        " 710,\n",
        " 3007]\n",
        "available_pool_set = set(available_pool_indices)\n",
        "train_set = set(train_indices)\n",
        "available_pool_indices = list(available_pool_set - train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmcf2jc6h2S9"
      },
      "outputs": [],
      "source": [
        "iterations = 20\n",
        "budget_per_iter = 60\n",
        "num_epoch = 15\n",
        "selection_criteria = ['kmeans_budget']\n",
        "accuracy_scores_dict = defaultdict(list)\n",
        "model = base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "4UGO2jW1h5Ql",
        "outputId": "0993cc7b-2fc2-4a84-a31a-ef45ed5febff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-16 17:29:14.382560: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-16 17:29:14.654865: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
            "2024-10-16 17:29:14.654936: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-10-16 17:29:16.034751: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
            "2024-10-16 17:29:16.034999: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64_lin/gcc4.7:/opt/intel/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64_lin::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
            "2024-10-16 17:29:16.035019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 0 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 2.0904, Accuracy: 0.0889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 2.0600, Accuracy: 0.2889\n",
            "Epoch [3/15], Loss: 2.0226, Accuracy: 0.4444\n",
            "Epoch [4/15], Loss: 1.9662, Accuracy: 0.4889\n",
            "Epoch [5/15], Loss: 1.8840, Accuracy: 0.6444\n",
            "Epoch [6/15], Loss: 1.8495, Accuracy: 0.4889\n",
            "Epoch [7/15], Loss: 1.7900, Accuracy: 0.4778\n",
            "Epoch [8/15], Loss: 1.7278, Accuracy: 0.5222\n",
            "Epoch [9/15], Loss: 1.6565, Accuracy: 0.7222\n",
            "Epoch [10/15], Loss: 1.6248, Accuracy: 0.7333\n",
            "Epoch [11/15], Loss: 1.5672, Accuracy: 0.8222\n",
            "Epoch [12/15], Loss: 1.5571, Accuracy: 0.8333\n",
            "Epoch [13/15], Loss: 1.5124, Accuracy: 0.8667\n",
            "Epoch [14/15], Loss: 1.4906, Accuracy: 0.8889\n",
            "Epoch [15/15], Loss: 1.4616, Accuracy: 0.9111\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 1 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.7010, Accuracy: 0.7133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6895, Accuracy: 0.7467\n",
            "Epoch [3/15], Loss: 1.6661, Accuracy: 0.7600\n",
            "Epoch [4/15], Loss: 1.6367, Accuracy: 0.7400\n",
            "Epoch [5/15], Loss: 1.6026, Accuracy: 0.7933\n",
            "Epoch [6/15], Loss: 1.5980, Accuracy: 0.7933\n",
            "Epoch [7/15], Loss: 1.5620, Accuracy: 0.8000\n",
            "Epoch [8/15], Loss: 1.5557, Accuracy: 0.7933\n",
            "Epoch [9/15], Loss: 1.5146, Accuracy: 0.8267\n",
            "Epoch [10/15], Loss: 1.5154, Accuracy: 0.8533\n",
            "Epoch [11/15], Loss: 1.4863, Accuracy: 0.8467\n",
            "Epoch [12/15], Loss: 1.4857, Accuracy: 0.8467\n",
            "Epoch [13/15], Loss: 1.4758, Accuracy: 0.8533\n",
            "Epoch [14/15], Loss: 1.4841, Accuracy: 0.8467\n",
            "Epoch [15/15], Loss: 1.4482, Accuracy: 0.8533\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 2 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5779, Accuracy: 0.7619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5735, Accuracy: 0.7619\n",
            "Epoch [3/15], Loss: 1.5459, Accuracy: 0.8048\n",
            "Epoch [4/15], Loss: 1.5301, Accuracy: 0.7905\n",
            "Epoch [5/15], Loss: 1.5580, Accuracy: 0.8238\n",
            "Epoch [6/15], Loss: 1.4967, Accuracy: 0.8238\n",
            "Epoch [7/15], Loss: 1.5033, Accuracy: 0.7952\n",
            "Epoch [8/15], Loss: 1.4956, Accuracy: 0.8619\n",
            "Epoch [9/15], Loss: 1.4605, Accuracy: 0.8333\n",
            "Epoch [10/15], Loss: 1.4634, Accuracy: 0.8381\n",
            "Epoch [11/15], Loss: 1.4567, Accuracy: 0.8476\n",
            "Epoch [12/15], Loss: 1.4608, Accuracy: 0.8143\n",
            "Epoch [13/15], Loss: 1.4595, Accuracy: 0.8333\n",
            "Epoch [14/15], Loss: 1.4563, Accuracy: 0.8619\n",
            "Epoch [15/15], Loss: 1.4477, Accuracy: 0.8571\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 3 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5298, Accuracy: 0.8037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5505, Accuracy: 0.7704\n",
            "Epoch [3/15], Loss: 1.5185, Accuracy: 0.8185\n",
            "Epoch [4/15], Loss: 1.5148, Accuracy: 0.8185\n",
            "Epoch [5/15], Loss: 1.5323, Accuracy: 0.7815\n",
            "Epoch [6/15], Loss: 1.4797, Accuracy: 0.8259\n",
            "Epoch [7/15], Loss: 1.4709, Accuracy: 0.8667\n",
            "Epoch [8/15], Loss: 1.5007, Accuracy: 0.8185\n",
            "Epoch [9/15], Loss: 1.4579, Accuracy: 0.8444\n",
            "Epoch [10/15], Loss: 1.4556, Accuracy: 0.8667\n",
            "Epoch [11/15], Loss: 1.4571, Accuracy: 0.8667\n",
            "Epoch [12/15], Loss: 1.4223, Accuracy: 0.8889\n",
            "Epoch [13/15], Loss: 1.4585, Accuracy: 0.8667\n",
            "Epoch [14/15], Loss: 1.4321, Accuracy: 0.8889\n",
            "Epoch [15/15], Loss: 1.4489, Accuracy: 0.8741\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 4 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4836, Accuracy: 0.8394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4817, Accuracy: 0.8424\n",
            "Epoch [3/15], Loss: 1.4972, Accuracy: 0.8152\n",
            "Epoch [4/15], Loss: 1.5168, Accuracy: 0.8303\n",
            "Epoch [5/15], Loss: 1.5065, Accuracy: 0.7970\n",
            "Epoch [6/15], Loss: 1.4770, Accuracy: 0.8515\n",
            "Epoch [7/15], Loss: 1.4641, Accuracy: 0.8636\n",
            "Epoch [8/15], Loss: 1.4431, Accuracy: 0.8545\n",
            "Epoch [9/15], Loss: 1.4174, Accuracy: 0.8364\n",
            "Epoch [10/15], Loss: 1.4142, Accuracy: 0.8667\n",
            "Epoch [11/15], Loss: 1.4227, Accuracy: 0.8364\n",
            "Epoch [12/15], Loss: 1.4119, Accuracy: 0.8636\n",
            "Epoch [13/15], Loss: 1.4221, Accuracy: 0.8576\n",
            "Epoch [14/15], Loss: 1.4159, Accuracy: 0.8303\n",
            "Epoch [15/15], Loss: 1.3859, Accuracy: 0.8667\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 5 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4805, Accuracy: 0.8077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5175, Accuracy: 0.8128\n",
            "Epoch [3/15], Loss: 1.4584, Accuracy: 0.7769\n",
            "Epoch [4/15], Loss: 1.4641, Accuracy: 0.8205\n",
            "Epoch [5/15], Loss: 1.4540, Accuracy: 0.8205\n",
            "Epoch [6/15], Loss: 1.4345, Accuracy: 0.8128\n",
            "Epoch [7/15], Loss: 1.4226, Accuracy: 0.8513\n",
            "Epoch [8/15], Loss: 1.4483, Accuracy: 0.7974\n",
            "Epoch [9/15], Loss: 1.4785, Accuracy: 0.8308\n",
            "Epoch [10/15], Loss: 1.4344, Accuracy: 0.8205\n",
            "Epoch [11/15], Loss: 1.4090, Accuracy: 0.8692\n",
            "Epoch [12/15], Loss: 1.4135, Accuracy: 0.8487\n",
            "Epoch [13/15], Loss: 1.3855, Accuracy: 0.8667\n",
            "Epoch [14/15], Loss: 1.4437, Accuracy: 0.8538\n",
            "Epoch [15/15], Loss: 1.4062, Accuracy: 0.8590\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 6 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6306, Accuracy: 0.6489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6101, Accuracy: 0.7200\n",
            "Epoch [3/15], Loss: 1.5364, Accuracy: 0.7356\n",
            "Epoch [4/15], Loss: 1.5424, Accuracy: 0.7333\n",
            "Epoch [5/15], Loss: 1.5064, Accuracy: 0.7267\n",
            "Epoch [6/15], Loss: 1.4905, Accuracy: 0.7289\n",
            "Epoch [7/15], Loss: 1.5587, Accuracy: 0.7178\n",
            "Epoch [8/15], Loss: 1.4780, Accuracy: 0.7422\n",
            "Epoch [9/15], Loss: 1.4483, Accuracy: 0.8244\n",
            "Epoch [10/15], Loss: 1.4482, Accuracy: 0.7867\n",
            "Epoch [11/15], Loss: 1.4856, Accuracy: 0.7733\n",
            "Epoch [12/15], Loss: 1.4659, Accuracy: 0.8289\n",
            "Epoch [13/15], Loss: 1.4889, Accuracy: 0.7511\n",
            "Epoch [14/15], Loss: 1.5040, Accuracy: 0.7489\n",
            "Epoch [15/15], Loss: 1.4614, Accuracy: 0.8200\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 7 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4762, Accuracy: 0.7431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4705, Accuracy: 0.7824\n",
            "Epoch [3/15], Loss: 1.4358, Accuracy: 0.7824\n",
            "Epoch [4/15], Loss: 1.4222, Accuracy: 0.8078\n",
            "Epoch [5/15], Loss: 1.4557, Accuracy: 0.7941\n",
            "Epoch [6/15], Loss: 1.4110, Accuracy: 0.8275\n",
            "Epoch [7/15], Loss: 1.4169, Accuracy: 0.8078\n",
            "Epoch [8/15], Loss: 1.4026, Accuracy: 0.8275\n",
            "Epoch [9/15], Loss: 1.4017, Accuracy: 0.8431\n",
            "Epoch [10/15], Loss: 1.3827, Accuracy: 0.8588\n",
            "Epoch [11/15], Loss: 1.3792, Accuracy: 0.8490\n",
            "Epoch [12/15], Loss: 1.3731, Accuracy: 0.8804\n",
            "Epoch [13/15], Loss: 1.4156, Accuracy: 0.8196\n",
            "Epoch [14/15], Loss: 1.3825, Accuracy: 0.8745\n",
            "Epoch [15/15], Loss: 1.3946, Accuracy: 0.8412\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 8 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4558, Accuracy: 0.7965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4425, Accuracy: 0.7930\n",
            "Epoch [3/15], Loss: 1.4622, Accuracy: 0.7649\n",
            "Epoch [4/15], Loss: 1.4131, Accuracy: 0.8333\n",
            "Epoch [5/15], Loss: 1.4269, Accuracy: 0.7912\n",
            "Epoch [6/15], Loss: 1.4295, Accuracy: 0.8193\n",
            "Epoch [7/15], Loss: 1.4118, Accuracy: 0.8386\n",
            "Epoch [8/15], Loss: 1.3976, Accuracy: 0.8456\n",
            "Epoch [9/15], Loss: 1.4065, Accuracy: 0.8333\n",
            "Epoch [10/15], Loss: 1.3949, Accuracy: 0.8456\n",
            "Epoch [11/15], Loss: 1.4016, Accuracy: 0.8456\n",
            "Epoch [12/15], Loss: 1.3882, Accuracy: 0.8526\n",
            "Epoch [13/15], Loss: 1.3902, Accuracy: 0.8544\n",
            "Epoch [14/15], Loss: 1.3883, Accuracy: 0.8614\n",
            "Epoch [15/15], Loss: 1.3855, Accuracy: 0.8614\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 9 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4403, Accuracy: 0.8222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4587, Accuracy: 0.7587\n",
            "Epoch [3/15], Loss: 1.4514, Accuracy: 0.8079\n",
            "Epoch [4/15], Loss: 1.4520, Accuracy: 0.7905\n",
            "Epoch [5/15], Loss: 1.4242, Accuracy: 0.8127\n",
            "Epoch [6/15], Loss: 1.4090, Accuracy: 0.8460\n",
            "Epoch [7/15], Loss: 1.4209, Accuracy: 0.8206\n",
            "Epoch [8/15], Loss: 1.4451, Accuracy: 0.8000\n",
            "Epoch [9/15], Loss: 1.4186, Accuracy: 0.8159\n",
            "Epoch [10/15], Loss: 1.4118, Accuracy: 0.8302\n",
            "Epoch [11/15], Loss: 1.3990, Accuracy: 0.8476\n",
            "Epoch [12/15], Loss: 1.4120, Accuracy: 0.8270\n",
            "Epoch [13/15], Loss: 1.4040, Accuracy: 0.8333\n",
            "Epoch [14/15], Loss: 1.3967, Accuracy: 0.8429\n",
            "Epoch [15/15], Loss: 1.3943, Accuracy: 0.8492\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 10 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4973, Accuracy: 0.7623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4774, Accuracy: 0.7971\n",
            "Epoch [3/15], Loss: 1.4814, Accuracy: 0.7638\n",
            "Epoch [4/15], Loss: 1.4570, Accuracy: 0.7797\n",
            "Epoch [5/15], Loss: 1.4530, Accuracy: 0.7913\n",
            "Epoch [6/15], Loss: 1.4479, Accuracy: 0.7884\n",
            "Epoch [7/15], Loss: 1.4489, Accuracy: 0.8130\n",
            "Epoch [8/15], Loss: 1.4443, Accuracy: 0.8014\n",
            "Epoch [9/15], Loss: 1.4482, Accuracy: 0.8000\n",
            "Epoch [10/15], Loss: 1.4465, Accuracy: 0.7870\n",
            "Epoch [11/15], Loss: 1.4351, Accuracy: 0.8232\n",
            "Epoch [12/15], Loss: 1.4353, Accuracy: 0.8087\n",
            "Epoch [13/15], Loss: 1.4450, Accuracy: 0.8072\n",
            "Epoch [14/15], Loss: 1.4504, Accuracy: 0.7826\n",
            "Epoch [15/15], Loss: 1.4225, Accuracy: 0.8188\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 11 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4684, Accuracy: 0.7787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4487, Accuracy: 0.7987\n",
            "Epoch [3/15], Loss: 1.4403, Accuracy: 0.8093\n",
            "Epoch [4/15], Loss: 1.4358, Accuracy: 0.8040\n",
            "Epoch [5/15], Loss: 1.4320, Accuracy: 0.8120\n",
            "Epoch [6/15], Loss: 1.4613, Accuracy: 0.7893\n",
            "Epoch [7/15], Loss: 1.4320, Accuracy: 0.8173\n",
            "Epoch [8/15], Loss: 1.4542, Accuracy: 0.7960\n",
            "Epoch [9/15], Loss: 1.4160, Accuracy: 0.8307\n",
            "Epoch [10/15], Loss: 1.4357, Accuracy: 0.8027\n",
            "Epoch [11/15], Loss: 1.4400, Accuracy: 0.8027\n",
            "Epoch [12/15], Loss: 1.4460, Accuracy: 0.8187\n",
            "Epoch [13/15], Loss: 1.4484, Accuracy: 0.8147\n",
            "Epoch [14/15], Loss: 1.4123, Accuracy: 0.8240\n",
            "Epoch [15/15], Loss: 1.4254, Accuracy: 0.8240\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 12 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4422, Accuracy: 0.8025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4545, Accuracy: 0.8074\n",
            "Epoch [3/15], Loss: 1.4400, Accuracy: 0.8000\n",
            "Epoch [4/15], Loss: 1.4502, Accuracy: 0.8000\n",
            "Epoch [5/15], Loss: 1.4603, Accuracy: 0.8074\n",
            "Epoch [6/15], Loss: 1.4581, Accuracy: 0.8012\n",
            "Epoch [7/15], Loss: 1.4386, Accuracy: 0.8136\n",
            "Epoch [8/15], Loss: 1.4463, Accuracy: 0.8123\n",
            "Epoch [9/15], Loss: 1.4515, Accuracy: 0.7728\n",
            "Epoch [10/15], Loss: 1.4552, Accuracy: 0.7988\n",
            "Epoch [11/15], Loss: 1.4292, Accuracy: 0.8346\n",
            "Epoch [12/15], Loss: 1.4229, Accuracy: 0.8259\n",
            "Epoch [13/15], Loss: 1.4393, Accuracy: 0.8395\n",
            "Epoch [14/15], Loss: 1.4294, Accuracy: 0.8136\n",
            "Epoch [15/15], Loss: 1.4322, Accuracy: 0.8185\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 13 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4822, Accuracy: 0.7862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4669, Accuracy: 0.7931\n",
            "Epoch [3/15], Loss: 1.4767, Accuracy: 0.8023\n",
            "Epoch [4/15], Loss: 1.4892, Accuracy: 0.7931\n",
            "Epoch [5/15], Loss: 1.4787, Accuracy: 0.7713\n",
            "Epoch [6/15], Loss: 1.4527, Accuracy: 0.8023\n",
            "Epoch [7/15], Loss: 1.4858, Accuracy: 0.7782\n",
            "Epoch [8/15], Loss: 1.4575, Accuracy: 0.8023\n",
            "Epoch [9/15], Loss: 1.4613, Accuracy: 0.8046\n",
            "Epoch [10/15], Loss: 1.4835, Accuracy: 0.7425\n",
            "Epoch [11/15], Loss: 1.5077, Accuracy: 0.7575\n",
            "Epoch [12/15], Loss: 1.4960, Accuracy: 0.7609\n",
            "Epoch [13/15], Loss: 1.4776, Accuracy: 0.7885\n",
            "Epoch [14/15], Loss: 1.5500, Accuracy: 0.6287\n",
            "Epoch [15/15], Loss: 1.4668, Accuracy: 0.7678\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 14 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4869, Accuracy: 0.7828\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5268, Accuracy: 0.7634\n",
            "Epoch [3/15], Loss: 1.5276, Accuracy: 0.7634\n",
            "Epoch [4/15], Loss: 1.4905, Accuracy: 0.7796\n",
            "Epoch [5/15], Loss: 1.5012, Accuracy: 0.7903\n",
            "Epoch [6/15], Loss: 1.4914, Accuracy: 0.7892\n",
            "Epoch [7/15], Loss: 1.6047, Accuracy: 0.5753\n",
            "Epoch [8/15], Loss: 1.6749, Accuracy: 0.4462\n",
            "Epoch [9/15], Loss: 1.6098, Accuracy: 0.5473\n",
            "Epoch [10/15], Loss: 1.5343, Accuracy: 0.7204\n",
            "Epoch [11/15], Loss: 1.6513, Accuracy: 0.5731\n",
            "Epoch [12/15], Loss: 1.4828, Accuracy: 0.7774\n",
            "Epoch [13/15], Loss: 1.4817, Accuracy: 0.7871\n",
            "Epoch [14/15], Loss: 1.5507, Accuracy: 0.6140\n",
            "Epoch [15/15], Loss: 1.5404, Accuracy: 0.6161\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 15 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5002, Accuracy: 0.7636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4734, Accuracy: 0.7848\n",
            "Epoch [3/15], Loss: 1.4657, Accuracy: 0.7919\n",
            "Epoch [4/15], Loss: 1.4634, Accuracy: 0.8010\n",
            "Epoch [5/15], Loss: 1.4800, Accuracy: 0.7838\n",
            "Epoch [6/15], Loss: 1.4545, Accuracy: 0.8040\n",
            "Epoch [7/15], Loss: 1.4457, Accuracy: 0.8182\n",
            "Epoch [8/15], Loss: 1.4661, Accuracy: 0.8081\n",
            "Epoch [9/15], Loss: 1.4650, Accuracy: 0.7788\n",
            "Epoch [10/15], Loss: 1.4612, Accuracy: 0.7970\n",
            "Epoch [11/15], Loss: 1.4504, Accuracy: 0.8152\n",
            "Epoch [12/15], Loss: 1.4645, Accuracy: 0.7939\n",
            "Epoch [13/15], Loss: 1.4505, Accuracy: 0.8263\n",
            "Epoch [14/15], Loss: 1.4400, Accuracy: 0.8232\n",
            "Epoch [15/15], Loss: 1.4358, Accuracy: 0.8232\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 16 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4861, Accuracy: 0.7933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5019, Accuracy: 0.7848\n",
            "Epoch [3/15], Loss: 1.4670, Accuracy: 0.7876\n",
            "Epoch [4/15], Loss: 1.4786, Accuracy: 0.7886\n",
            "Epoch [5/15], Loss: 1.4849, Accuracy: 0.7886\n",
            "Epoch [6/15], Loss: 1.4604, Accuracy: 0.8162\n",
            "Epoch [7/15], Loss: 1.4610, Accuracy: 0.8048\n",
            "Epoch [8/15], Loss: 1.4962, Accuracy: 0.7838\n",
            "Epoch [9/15], Loss: 1.4553, Accuracy: 0.8162\n",
            "Epoch [10/15], Loss: 1.4575, Accuracy: 0.8076\n",
            "Epoch [11/15], Loss: 1.4604, Accuracy: 0.8067\n",
            "Epoch [12/15], Loss: 1.4558, Accuracy: 0.8114\n",
            "Epoch [13/15], Loss: 1.4451, Accuracy: 0.8190\n",
            "Epoch [14/15], Loss: 1.4433, Accuracy: 0.8276\n",
            "Epoch [15/15], Loss: 1.4480, Accuracy: 0.8219\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 17 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4956, Accuracy: 0.7901\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4797, Accuracy: 0.7946\n",
            "Epoch [3/15], Loss: 1.4751, Accuracy: 0.8018\n",
            "Epoch [4/15], Loss: 1.4673, Accuracy: 0.8000\n",
            "Epoch [5/15], Loss: 1.4573, Accuracy: 0.8108\n",
            "Epoch [6/15], Loss: 1.4668, Accuracy: 0.8009\n",
            "Epoch [7/15], Loss: 1.4613, Accuracy: 0.7928\n",
            "Epoch [8/15], Loss: 1.5175, Accuracy: 0.7387\n",
            "Epoch [9/15], Loss: 1.4593, Accuracy: 0.7991\n",
            "Epoch [10/15], Loss: 1.4707, Accuracy: 0.7910\n",
            "Epoch [11/15], Loss: 1.4622, Accuracy: 0.7955\n",
            "Epoch [12/15], Loss: 1.4741, Accuracy: 0.7982\n",
            "Epoch [13/15], Loss: 1.4460, Accuracy: 0.8153\n",
            "Epoch [14/15], Loss: 1.4612, Accuracy: 0.7901\n",
            "Epoch [15/15], Loss: 1.4303, Accuracy: 0.8315\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 18 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4732, Accuracy: 0.7957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4860, Accuracy: 0.7803\n",
            "Epoch [3/15], Loss: 1.4739, Accuracy: 0.7966\n",
            "Epoch [4/15], Loss: 1.4653, Accuracy: 0.8043\n",
            "Epoch [5/15], Loss: 1.4772, Accuracy: 0.7846\n",
            "Epoch [6/15], Loss: 1.4637, Accuracy: 0.8111\n",
            "Epoch [7/15], Loss: 1.4558, Accuracy: 0.8154\n",
            "Epoch [8/15], Loss: 1.4470, Accuracy: 0.8308\n",
            "Epoch [9/15], Loss: 1.4514, Accuracy: 0.8256\n",
            "Epoch [10/15], Loss: 1.4513, Accuracy: 0.8154\n",
            "Epoch [11/15], Loss: 1.4651, Accuracy: 0.8026\n",
            "Epoch [12/15], Loss: 1.4510, Accuracy: 0.8043\n",
            "Epoch [13/15], Loss: 1.4635, Accuracy: 0.8094\n",
            "Epoch [14/15], Loss: 1.4591, Accuracy: 0.7974\n",
            "Epoch [15/15], Loss: 1.4482, Accuracy: 0.8197\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 19 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.4511, Accuracy: 0.8146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.4506, Accuracy: 0.8106\n",
            "Epoch [3/15], Loss: 1.4503, Accuracy: 0.8138\n",
            "Epoch [4/15], Loss: 1.4763, Accuracy: 0.7959\n",
            "Epoch [5/15], Loss: 1.4381, Accuracy: 0.8211\n",
            "Epoch [6/15], Loss: 1.4390, Accuracy: 0.8268\n",
            "Epoch [7/15], Loss: 1.4394, Accuracy: 0.8033\n",
            "Epoch [8/15], Loss: 1.4345, Accuracy: 0.8211\n",
            "Epoch [9/15], Loss: 1.4394, Accuracy: 0.8106\n",
            "Epoch [10/15], Loss: 1.4413, Accuracy: 0.8236\n",
            "Epoch [11/15], Loss: 1.4487, Accuracy: 0.7992\n",
            "Epoch [12/15], Loss: 1.4377, Accuracy: 0.8268\n",
            "Epoch [13/15], Loss: 1.4301, Accuracy: 0.8285\n",
            "Epoch [14/15], Loss: 1.4193, Accuracy: 0.8528\n",
            "Epoch [15/15], Loss: 1.4352, Accuracy: 0.8130\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_357819/1963945088.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "\n",
        "for criterion in selection_criteria:\n",
        "    AL_class = ActiveLearningPipeline(model=model,\n",
        "                                      test_indices=test_indices,\n",
        "                                      available_pool_indices=available_pool_indices,\n",
        "                                      train_indices=train_indices,\n",
        "                                      selection_criterion=criterion,\n",
        "                                      iterations=iterations,\n",
        "                                      budget_per_iter=budget_per_iter,\n",
        "                                      num_epochs=num_epoch)\n",
        "    accuracy_scores_dict[criterion] = AL_class.run_pipeline()\n",
        "generate_plot(accuracy_scores_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5sKVVjSzEl7",
        "outputId": "d0b48ce8-3088-4918-cd6b-feedd8b38e96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'kmeans_budget': [0.5277777777777778,\n",
              "              0.5687830687830687,\n",
              "              0.6064814814814814,\n",
              "              0.5853174603174602,\n",
              "              0.6236772486772486,\n",
              "              0.621031746031746,\n",
              "              0.583994708994709,\n",
              "              0.5767195767195767,\n",
              "              0.566137566137566,\n",
              "              0.5965608465608465,\n",
              "              0.5687830687830687,\n",
              "              0.5939153439153438,\n",
              "              0.6216931216931216,\n",
              "              0.5945767195767195,\n",
              "              0.6223544973544973,\n",
              "              0.5813492063492063,\n",
              "              0.6064814814814814,\n",
              "              0.6375661375661376,\n",
              "              0.6005291005291005,\n",
              "              0.6461640211640212]})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_scores_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwY_UQ4XqzC2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
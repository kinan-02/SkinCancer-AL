{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinan-02/SkinCancer-AL/blob/main/Stratiges/uncertinty_kmeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjgyTgaRfy4k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import entropy\n",
        "from collections import defaultdict\n",
        "import argparse\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "train_df = pd.read_csv('train_dataset/metadata.csv')\n",
        "test_df = pd.read_csv('test_dataset/metadata.csv')\n",
        "val_df = pd.read_csv('validation_dataset/metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "bcHCCahYj6B7",
        "outputId": "1facd164-3e8b-40f4-81d7-52fc3110e538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diagnosis\n",
            "nevus                         1205\n",
            "melanoma                      1113\n",
            "pigmented benign keratosis    1099\n",
            "basal cell carcinoma           514\n",
            "squamous cell carcinoma        197\n",
            "vascular lesion                142\n",
            "actinic keratosis              130\n",
            "dermatofibroma                 115\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_df['diagnosis'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hSsk5IJwdls",
        "outputId": "88cd0103-a2c1-44f3-d61c-e5c23dc698e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'actinic keratosis': 0,\n",
              " 'basal cell carcinoma': 1,\n",
              " 'dermatofibroma': 2,\n",
              " 'melanoma': 3,\n",
              " 'nevus': 4,\n",
              " 'pigmented benign keratosis': 5,\n",
              " 'squamous cell carcinoma': 6,\n",
              " 'vascular lesion': 7}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_mapping = {\n",
        "    \"actinic keratosis\": 0,\n",
        "    \"basal cell carcinoma\": 1,\n",
        "    \"dermatofibroma\": 2,\n",
        "    \"melanoma\": 3,\n",
        "    \"nevus\": 4,\n",
        "    \"pigmented benign keratosis\": 5,\n",
        "    \"squamous cell carcinoma\": 6,\n",
        "    \"vascular lesion\":7\n",
        "}\n",
        "class_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLruwetXmPem"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define image transformations (resize, convert to tensor)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),          # Resize images to 224x224 (matching ResNet input size)\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# we made this class to read the data from the file and to use it later in the dataloader\n",
        "class Dataset():\n",
        "    def __init__(self, dataframe, transform, train='train'):\n",
        "        self.dataframe=dataframe\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.path_to_image=self._create_path_to_image_dict()\n",
        "        self.paths=list(self.path_to_image.keys())\n",
        "        self.labels=list(self.path_to_image.values())\n",
        "\n",
        "    def _create_path_to_image_dict(self):\n",
        "      path_to_image={}\n",
        "      for index,row in self.dataframe.iterrows():\n",
        "        if self.train == 'train':\n",
        "          img_path = os.path.join('train_dataset/',row['isic_id']+'.jpg')\n",
        "        elif self.train == 'test':\n",
        "          img_path = os.path.join('test_dataset/',row['isic_id']+'.jpg')\n",
        "        else:\n",
        "            img_path = os.path.join('validation_dataset/',row['isic_id']+'.jpg')\n",
        "        label=row['diagnosis']\n",
        "        path_to_image[img_path]=label\n",
        "      return path_to_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        img_path=self.paths[index]\n",
        "        img_label=self.labels[index]\n",
        "        image=Image.open(img_path)\n",
        "        image=self.transform(image)\n",
        "        if self.train == 'val':\n",
        "            return image, class_mapping[img_label], index\n",
        "        return image, img_label, index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yCvnprDoa_7"
      },
      "outputs": [],
      "source": [
        "train_df = Dataset(train_df, transform)\n",
        "val_df = Dataset(val_df, transform,train='val')\n",
        "test_df = Dataset(test_df, transform,train='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5L0NcpesnZI",
        "outputId": "22f97091-b571-4138-964f-95a509890583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Load pre-trained ResNet50 model from torchvision\n",
        "base_model = models.resnet50(pretrained=True)\n",
        "\n",
        "num_classes = 8\n",
        "base_model.fc = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(base_model.fc.in_features, 128),  # Add a fully connected layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, num_classes),  # Final layer with number of classes\n",
        "    nn.Softmax(dim=1)  # Softmax activation for multi-class classification\n",
        ")\n",
        "\n",
        "for param in base_model.parameters():\n",
        "    param.requires_grad = False  # Freeze all layers except the fully connected ones\n",
        "\n",
        "# Unfreeze the final fully connected layer\n",
        "for param in base_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(base_model.fc.parameters(), lr=0.0008)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model.to(device)\n",
        "x = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrqnCnhHwdlu"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 4\n",
        "val_loader = DataLoader(val_df, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Vk8kyZsgACa"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "\n",
        "class ActiveLearningPipeline:\n",
        "    def __init__(self, model,\n",
        "                 available_pool_indices,\n",
        "                 train_indices,\n",
        "                 test_indices,\n",
        "                 selection_criterion,\n",
        "                 iterations,\n",
        "                 budget_per_iter,\n",
        "                 num_epochs):\n",
        "        self.model = model\n",
        "        self.iterations = iterations\n",
        "        self.budget_per_iter = budget_per_iter\n",
        "        self.available_pool_indices = available_pool_indices\n",
        "        self.train_indices = train_indices\n",
        "        self.test_indices = test_indices\n",
        "        self.selection_criterion = selection_criterion\n",
        "        if self.selection_criterion == 'random':\n",
        "          self.train_indices = []\n",
        "        self.num_epochs = num_epochs\n",
        "        # self.best_acc = 0\n",
        "        self.pool_features = []\n",
        "        self.pool_indices = []\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"\n",
        "        Run the active learning pipeline\n",
        "        :return\n",
        "        accuracy_scores: list, accuracy scores at each iteration\n",
        "        \"\"\"\n",
        "        accuracy_scores = []\n",
        "        self._get_features()\n",
        "        for iteration in range(self.iterations):\n",
        "            print(f\"--------- Number of Iteration {iteration} ---------\")\n",
        "            if self.selection_criterion == 'random':\n",
        "                self._random_sampling()\n",
        "            elif self.selection_criterion == 'uncertainty_kmeans':\n",
        "                self._uncertainty_kmeans_sampling(iteration)\n",
        "            else:\n",
        "              self._custom_sampling(iteration)\n",
        "\n",
        "            train_images = [train_df.__getitem__(index)[0] for index in self.train_indices]\n",
        "            label_df = [class_mapping[train_df.__getitem__(index)[1]] for index in self.train_indices]\n",
        "            self._train_model(train_images, label_df)\n",
        "            self.model.load_state_dict(torch.load(f\"best_{self.selection_criterion}_model.pth\"))\n",
        "            accuracy = self._evaluate_model()\n",
        "            accuracy_scores.append(accuracy)\n",
        "        return accuracy_scores\n",
        "\n",
        "    def calculate_class_weights(self, label_counts, num_classes=8):\n",
        "      \"\"\"\n",
        "      calculate class weights to handle data imbalance for the loss function.\n",
        "      \"\"\"\n",
        "        total_samples = sum(label_counts.values())\n",
        "        class_weights = torch.zeros(num_classes)\n",
        "\n",
        "        for cls in range(num_classes):\n",
        "            if cls in label_counts:\n",
        "                class_weights[cls] = total_samples / (num_classes * label_counts[cls])\n",
        "            else:\n",
        "                class_weights[cls] = 1.0\n",
        "\n",
        "        return class_weights\n",
        "\n",
        "    def _train_model(self, train_images, label_df):\n",
        "      \"\"\"\n",
        "      This function trains the base model on the provided training set and saves the best-performing model.\n",
        "      \"\"\"\n",
        "      label_counts = defaultdict(int)\n",
        "      for label in label_df:\n",
        "                label_counts[label] += 1\n",
        "      class_weights = self.calculate_class_weights(label_counts, 8).to(device)\n",
        "      loss_f = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "      train_images_tensor = torch.stack(train_images)\n",
        "      label_df_tensor = torch.tensor(label_df)\n",
        "      train_dataset = TensorDataset(train_images_tensor, label_df_tensor)\n",
        "\n",
        "      batch_size = 32\n",
        "      train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "      best_acc = 0\n",
        "      for epoch in range(self.num_epochs):\n",
        "                self.model.train()\n",
        "                running_loss = 0.0  # Track the running loss\n",
        "                correct_predictions = 0\n",
        "                total_predictions = 0\n",
        "                # Training loop\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs = inputs\n",
        "                    inputs= inputs.to(device)\n",
        "                    labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                    # Zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = loss_f(outputs, labels)\n",
        "\n",
        "                    # Backward pass and optimization\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    correct_predictions += torch.sum(preds == labels)\n",
        "                    total_predictions += inputs.shape[0]\n",
        "\n",
        "                # Print loss and accuracy at the end of each epoch\n",
        "                epoch_loss = running_loss / len(train_loader)\n",
        "                epoch_acc = correct_predictions.double() / total_predictions\n",
        "                print(f'Epoch [{epoch+1}/{self.num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "\n",
        "                val_acc = self._check_model()\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "                    torch.save(self.model.state_dict(), f\"best_{self.selection_criterion}_model.pth\")\n",
        "      print(\"--\"*30)\n",
        "\n",
        "    def _check_model(self):\n",
        "      \"\"\"\n",
        "      Returns the accuracy of the base model on the validation set.\n",
        "      \"\"\"\n",
        "        self.model.eval()\n",
        "        running_corrects = 0\n",
        "        total_predictions = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, _ in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                running_corrects += torch.sum(preds == labels)\n",
        "                total_predictions += inputs.shape[0]\n",
        "        val_acc = running_corrects.double() / total_predictions\n",
        "        return val_acc.item()\n",
        "\n",
        "    def _evaluate_model(self):\n",
        "        \"\"\"\n",
        "        Returns the accuracy of the base model on the test set.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        running_corrects = 0\n",
        "        test_images_tensor = torch.stack(test_images)\n",
        "        label_df_tensor = torch.tensor(test_label_df)\n",
        "        test_dataset = TensorDataset(test_images_tensor, label_df_tensor)\n",
        "        batch_size = 32  # Adjust based on your memory and hardware\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "        total_predictions = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = torch.tensor(labels).to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                running_corrects += torch.sum(preds == labels)\n",
        "                total_predictions += inputs.shape[0]\n",
        "        test_acc = running_corrects.double() / total_predictions\n",
        "        return test_acc.item()\n",
        "\n",
        "    def _random_sampling(self):\n",
        "      \"\"\"\n",
        "      Adds samples to the training set using a random sampling strategy.\n",
        "      \"\"\"\n",
        "      selected_indices = np.random.choice(self.available_pool_indices, self.budget_per_iter, replace=False)\n",
        "      selected_indices = selected_indices.tolist()\n",
        "      self.train_indices = self.train_indices + selected_indices\n",
        "\n",
        "      available_pool_set = set(self.available_pool_indices)\n",
        "      train_set = set(self.train_indices)\n",
        "      self.available_pool_indices = list(available_pool_set - train_set)\n",
        "\n",
        "\n",
        "    def extract_vae_features(self, dataloader, model, feature_extractor):\n",
        "        \"\"\"\n",
        "        Return the latent vector for each image and the corresponding indices.\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "        indices_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, indices in dataloader:\n",
        "                # images = images.to(device)  # Move images to GPU if available\n",
        "                images_list = [transforms.ToPILImage()(img) for img in images]\n",
        "                inputs = feature_extractor(images=images_list, return_tensors=\"pt\")\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**inputs)\n",
        "\n",
        "                x = outputs.last_hidden_state[:, 0, :]\n",
        "                features_list.append(x.cpu().numpy())\n",
        "\n",
        "                # Collect indices\n",
        "                indices_list.extend(indices)\n",
        "\n",
        "        # Stack all features into a 2D array (n_samples, hidden_dim)\n",
        "        features = np.vstack(features_list)\n",
        "\n",
        "        return features, indices_list\n",
        "\n",
        "\n",
        "    def get_representative_images(self, kmeans, pool_features, pool_indices):\n",
        "          \"\"\"\n",
        "         returns a dictionary where the keys are the index of the cluster and the values are the closest images to\n",
        "         each centroid note that the K of the KMeans is the budget per iteration.\n",
        "        \"\"\"\n",
        "        cluster_to_images = {}\n",
        "        for i in range(kmeans.n_clusters):\n",
        "            # Get the indices of all images in the current cluster\n",
        "            cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
        "\n",
        "            # Extract features of the images in the current cluster\n",
        "            cluster_features = pool_features[cluster_indices]\n",
        "\n",
        "            # Compute distances between each feature and the cluster centroid\n",
        "            distances = np.linalg.norm(cluster_features - kmeans.cluster_centers_[i], axis=1)\n",
        "\n",
        "            # Map the cluster number to the index of the representative image\n",
        "            nearest_indices = cluster_indices[np.argsort(distances)[:1]]\n",
        "\n",
        "\n",
        "            # Map the cluster number to the indices of the top k nearest images\n",
        "            cluster_to_images[i] = [pool_indices[idx] for idx in nearest_indices]\n",
        "\n",
        "        return cluster_to_images\n",
        "\n",
        "    def _get_features(self):\n",
        "      \"\"\"\n",
        "      Creates latent feature vectors for each image in the available pool using a pre-trained Vision Transformer (ViT) model from Google.\n",
        "      \"\"\"\n",
        "        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "        X_unlabeled = [train_df.__getitem__(index)[0] for index in self.available_pool_indices]\n",
        "        # Extract latent features using the VAE model\n",
        "        pool_images_tensor = torch.stack(X_unlabeled)\n",
        "        pool_indices_tensor = torch.tensor(self.available_pool_indices)\n",
        "        pool_dataset = TensorDataset(pool_images_tensor, pool_indices_tensor)\n",
        "\n",
        "        batch_size = 32\n",
        "        pool_loader = DataLoader(pool_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        self.pool_features, self.pool_indices = self.extract_vae_features(pool_loader, model, feature_extractor)\n",
        "\n",
        "    def _kmean_uncertin_samples(self, selected_indices):\n",
        "      \"\"\"\n",
        "      returns the selected indices\n",
        "      \"\"\"\n",
        "        n_clusters = self.budget_per_iter\n",
        "        kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=0)\n",
        "        kmeans.fit(self.pool_features[selected_indices])\n",
        "\n",
        "        pool_indices_np = np.array(self.pool_indices)\n",
        "        representative_images = self.get_representative_images(kmeans, self.pool_features[selected_indices], pool_indices_np[selected_indices])\n",
        "        selected_indices = list(ids.item() for l in representative_images.values() for ids in l)\n",
        "\n",
        "        for i in selected_indices:\n",
        "              index = self.pool_indices.index(i)\n",
        "              self.pool_features = np.delete(self.pool_features, index, axis=0)\n",
        "              self.pool_indices.pop(index)\n",
        "        return selected_indices\n",
        "\n",
        "    def _uncertainty_kmeans_sampling(self, itr):\n",
        "      \"\"\"\n",
        "      Adds samples to the training set using a uncertainty-kmeans sampling strategy.\n",
        "      \"\"\"\n",
        "      model = self.model\n",
        "\n",
        "      X_unlabeled = [train_df.__getitem__(index)[0] for index in self.available_pool_indices]\n",
        "\n",
        "      pool_images_tensor = torch.stack(X_unlabeled)\n",
        "      pool_dataset = TensorDataset(pool_images_tensor)\n",
        "\n",
        "      batch_size = 32\n",
        "      pool_loader = DataLoader(pool_dataset, batch_size=batch_size, shuffle=False)\n",
        "      model.eval()\n",
        "      outputs = []\n",
        "      with torch.no_grad():\n",
        "                for inputs in pool_loader:\n",
        "                    inputs = inputs[0]\n",
        "                    inputs = inputs.to(device)\n",
        "                    x = model(inputs)\n",
        "                    if x.shape[0] != batch_size:\n",
        "                        padding_tensor = torch.full((batch_size - x.shape[0], 8), 0).to(device)\n",
        "                        c = batch_size - x.shape[0]\n",
        "                        x = torch.cat([x, padding_tensor])\n",
        "                    outputs.append(x)\n",
        "      probabilities = torch.cat(outputs, dim=0)\n",
        "      probabilities = probabilities[:-c]\n",
        "      probabilities_cpu = probabilities.cpu().numpy()\n",
        "      uncertainties = entropy(probabilities_cpu, axis=1)\n",
        "\n",
        "      selected_indices = np.argsort(uncertainties)[-(self.budget_per_iter*self.iterations):]\n",
        "\n",
        "      selected_indices = self._kmean_uncertin_samples(selected_indices)\n",
        "\n",
        "      self.train_indices = self.train_indices + selected_indices\n",
        "\n",
        "      available_pool_set = set(self.available_pool_indices)\n",
        "      train_set = set(self.train_indices)\n",
        "      self.available_pool_indices = list(available_pool_set - train_set)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5829ZYDh1Rp"
      },
      "outputs": [],
      "source": [
        "def generate_plot(accuracy_scores_dict):\n",
        "    \"\"\"\n",
        "    Generate a plot\n",
        "    \"\"\"\n",
        "    for criterion, accuracy_scores in accuracy_scores_dict.items():\n",
        "        plt.plot(range(1, len(accuracy_scores) + 1), accuracy_scores, label=criterion)\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ2IZmfRuYgX"
      },
      "outputs": [],
      "source": [
        "available_pool_indices = []\n",
        "for i in range(len(train_df)):\n",
        "  \"\"\" Initialize the available pool indices \"\"\"\n",
        "    image, label, index = train_df[i]\n",
        "    available_pool_indices.append(index)\n",
        "\n",
        "test_indices = []\n",
        "for i in range(len(test_df)):\n",
        "  \"\"\" Initialize the  test set \"\"\"\n",
        "    image, label, index = test_df[i]\n",
        "    test_indices.append(index)\n",
        "\n",
        "# Extract the images and labels for the collected test indices\n",
        "test_images = [test_df.__getitem__(index)[0] for index in test_indices]\n",
        "test_label_df = [class_mapping[test_df.__getitem__(index)[1]] for index in test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVImPDXSwdlw"
      },
      "outputs": [],
      "source": [
        "# This is the initial training set extracted using KMeans++ clustering with ViT (Vision Transformer) feature extraction.\n",
        "train_indices = [1372,\n",
        " 1277,\n",
        " 1255,\n",
        " 1423,\n",
        " 2925,\n",
        " 1963,\n",
        " 2335,\n",
        " 1923,\n",
        " 3791,\n",
        " 1239,\n",
        " 909,\n",
        " 134,\n",
        " 1547,\n",
        " 3931,\n",
        " 2467,\n",
        " 2832,\n",
        " 1789,\n",
        " 3022,\n",
        " 2424,\n",
        " 780,\n",
        " 2412,\n",
        " 3038,\n",
        " 2158,\n",
        " 3335,\n",
        " 1868,\n",
        " 1771,\n",
        " 2015,\n",
        " 1535,\n",
        " 710,\n",
        " 3007]\n",
        "available_pool_set = set(available_pool_indices)\n",
        "train_set = set(train_indices)\n",
        "available_pool_indices = list(available_pool_set - train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmcf2jc6h2S9"
      },
      "outputs": [],
      "source": [
        "# train_indices = []\n",
        "iterations = 20\n",
        "budget_per_iter = 60\n",
        "num_epoch = 15\n",
        "selection_criteria = ['uncertainty_kmeans']\n",
        "accuracy_scores_dict = defaultdict(list)\n",
        "model = base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "4UGO2jW1h5Ql",
        "outputId": "0993cc7b-2fc2-4a84-a31a-ef45ed5febff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/anaconda/envs/py38_default/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 0 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 2.0915, Accuracy: 0.1111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 2.0637, Accuracy: 0.2444\n",
            "Epoch [3/15], Loss: 2.0343, Accuracy: 0.2889\n",
            "Epoch [4/15], Loss: 1.9987, Accuracy: 0.4222\n",
            "Epoch [5/15], Loss: 1.9534, Accuracy: 0.5111\n",
            "Epoch [6/15], Loss: 1.9010, Accuracy: 0.5889\n",
            "Epoch [7/15], Loss: 1.8471, Accuracy: 0.6111\n",
            "Epoch [8/15], Loss: 1.7989, Accuracy: 0.5444\n",
            "Epoch [9/15], Loss: 1.7316, Accuracy: 0.6333\n",
            "Epoch [10/15], Loss: 1.6892, Accuracy: 0.6667\n",
            "Epoch [11/15], Loss: 1.6717, Accuracy: 0.7111\n",
            "Epoch [12/15], Loss: 1.6204, Accuracy: 0.7778\n",
            "Epoch [13/15], Loss: 1.5838, Accuracy: 0.7889\n",
            "Epoch [14/15], Loss: 1.5654, Accuracy: 0.8333\n",
            "Epoch [15/15], Loss: 1.5196, Accuracy: 0.8222\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 1 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.7922, Accuracy: 0.6067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.7900, Accuracy: 0.5933\n",
            "Epoch [3/15], Loss: 1.7641, Accuracy: 0.6133\n",
            "Epoch [4/15], Loss: 1.7225, Accuracy: 0.6133\n",
            "Epoch [5/15], Loss: 1.6758, Accuracy: 0.6333\n",
            "Epoch [6/15], Loss: 1.6924, Accuracy: 0.6400\n",
            "Epoch [7/15], Loss: 1.6425, Accuracy: 0.6667\n",
            "Epoch [8/15], Loss: 1.6028, Accuracy: 0.7067\n",
            "Epoch [9/15], Loss: 1.5848, Accuracy: 0.6867\n",
            "Epoch [10/15], Loss: 1.5675, Accuracy: 0.7467\n",
            "Epoch [11/15], Loss: 1.5503, Accuracy: 0.7267\n",
            "Epoch [12/15], Loss: 1.4977, Accuracy: 0.7800\n",
            "Epoch [13/15], Loss: 1.4936, Accuracy: 0.7867\n",
            "Epoch [14/15], Loss: 1.4870, Accuracy: 0.8267\n",
            "Epoch [15/15], Loss: 1.5002, Accuracy: 0.7867\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 2 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5934, Accuracy: 0.6571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5742, Accuracy: 0.6667\n",
            "Epoch [3/15], Loss: 1.5522, Accuracy: 0.7190\n",
            "Epoch [4/15], Loss: 1.5498, Accuracy: 0.7333\n",
            "Epoch [5/15], Loss: 1.5399, Accuracy: 0.7190\n",
            "Epoch [6/15], Loss: 1.5121, Accuracy: 0.7381\n",
            "Epoch [7/15], Loss: 1.5102, Accuracy: 0.7476\n",
            "Epoch [8/15], Loss: 1.4972, Accuracy: 0.7333\n",
            "Epoch [9/15], Loss: 1.4940, Accuracy: 0.7524\n",
            "Epoch [10/15], Loss: 1.4768, Accuracy: 0.7905\n",
            "Epoch [11/15], Loss: 1.4639, Accuracy: 0.7952\n",
            "Epoch [12/15], Loss: 1.4571, Accuracy: 0.7952\n",
            "Epoch [13/15], Loss: 1.4608, Accuracy: 0.7905\n",
            "Epoch [14/15], Loss: 1.4285, Accuracy: 0.8238\n",
            "Epoch [15/15], Loss: 1.4231, Accuracy: 0.8143\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 3 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5812, Accuracy: 0.7074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6077, Accuracy: 0.6963\n",
            "Epoch [3/15], Loss: 1.5727, Accuracy: 0.7037\n",
            "Epoch [4/15], Loss: 1.5767, Accuracy: 0.7593\n",
            "Epoch [5/15], Loss: 1.5497, Accuracy: 0.7444\n",
            "Epoch [6/15], Loss: 1.5431, Accuracy: 0.7556\n",
            "Epoch [7/15], Loss: 1.5207, Accuracy: 0.7519\n",
            "Epoch [8/15], Loss: 1.5353, Accuracy: 0.7444\n",
            "Epoch [9/15], Loss: 1.5157, Accuracy: 0.7926\n",
            "Epoch [10/15], Loss: 1.5024, Accuracy: 0.7704\n",
            "Epoch [11/15], Loss: 1.4786, Accuracy: 0.8370\n",
            "Epoch [12/15], Loss: 1.4931, Accuracy: 0.7852\n",
            "Epoch [13/15], Loss: 1.4847, Accuracy: 0.8222\n",
            "Epoch [14/15], Loss: 1.4845, Accuracy: 0.8000\n",
            "Epoch [15/15], Loss: 1.4725, Accuracy: 0.8111\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 4 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6207, Accuracy: 0.7212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5778, Accuracy: 0.6636\n",
            "Epoch [3/15], Loss: 1.6154, Accuracy: 0.6879\n",
            "Epoch [4/15], Loss: 1.5653, Accuracy: 0.7424\n",
            "Epoch [5/15], Loss: 1.5623, Accuracy: 0.7455\n",
            "Epoch [6/15], Loss: 1.5462, Accuracy: 0.7485\n",
            "Epoch [7/15], Loss: 1.5289, Accuracy: 0.7485\n",
            "Epoch [8/15], Loss: 1.5166, Accuracy: 0.7545\n",
            "Epoch [9/15], Loss: 1.5230, Accuracy: 0.7758\n",
            "Epoch [10/15], Loss: 1.5009, Accuracy: 0.7879\n",
            "Epoch [11/15], Loss: 1.5199, Accuracy: 0.7758\n",
            "Epoch [12/15], Loss: 1.4936, Accuracy: 0.7879\n",
            "Epoch [13/15], Loss: 1.4827, Accuracy: 0.7939\n",
            "Epoch [14/15], Loss: 1.4910, Accuracy: 0.8030\n",
            "Epoch [15/15], Loss: 1.5072, Accuracy: 0.7788\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 5 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6155, Accuracy: 0.6769\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5870, Accuracy: 0.6692\n",
            "Epoch [3/15], Loss: 1.6486, Accuracy: 0.6282\n",
            "Epoch [4/15], Loss: 1.6599, Accuracy: 0.6128\n",
            "Epoch [5/15], Loss: 1.5923, Accuracy: 0.7000\n",
            "Epoch [6/15], Loss: 1.5793, Accuracy: 0.7256\n",
            "Epoch [7/15], Loss: 1.5418, Accuracy: 0.7308\n",
            "Epoch [8/15], Loss: 1.5614, Accuracy: 0.7308\n",
            "Epoch [9/15], Loss: 1.5532, Accuracy: 0.7410\n",
            "Epoch [10/15], Loss: 1.5624, Accuracy: 0.7128\n",
            "Epoch [11/15], Loss: 1.4980, Accuracy: 0.7744\n",
            "Epoch [12/15], Loss: 1.4935, Accuracy: 0.7795\n",
            "Epoch [13/15], Loss: 1.5216, Accuracy: 0.7641\n",
            "Epoch [14/15], Loss: 1.5159, Accuracy: 0.7846\n",
            "Epoch [15/15], Loss: 1.4663, Accuracy: 0.7795\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 6 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6084, Accuracy: 0.6956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6509, Accuracy: 0.6533\n",
            "Epoch [3/15], Loss: 1.6305, Accuracy: 0.5844\n",
            "Epoch [4/15], Loss: 1.7000, Accuracy: 0.5244\n",
            "Epoch [5/15], Loss: 1.6318, Accuracy: 0.6511\n",
            "Epoch [6/15], Loss: 1.5915, Accuracy: 0.6444\n",
            "Epoch [7/15], Loss: 1.6032, Accuracy: 0.6911\n",
            "Epoch [8/15], Loss: 1.5443, Accuracy: 0.7111\n",
            "Epoch [9/15], Loss: 1.5548, Accuracy: 0.7400\n",
            "Epoch [10/15], Loss: 1.4944, Accuracy: 0.7444\n",
            "Epoch [11/15], Loss: 1.5203, Accuracy: 0.7222\n",
            "Epoch [12/15], Loss: 1.5316, Accuracy: 0.7689\n",
            "Epoch [13/15], Loss: 1.5816, Accuracy: 0.6200\n",
            "Epoch [14/15], Loss: 1.5843, Accuracy: 0.5889\n",
            "Epoch [15/15], Loss: 1.4914, Accuracy: 0.7800\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 7 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.7001, Accuracy: 0.5255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6568, Accuracy: 0.6353\n",
            "Epoch [3/15], Loss: 1.5875, Accuracy: 0.6863\n",
            "Epoch [4/15], Loss: 1.6084, Accuracy: 0.6686\n",
            "Epoch [5/15], Loss: 1.5733, Accuracy: 0.7020\n",
            "Epoch [6/15], Loss: 1.5473, Accuracy: 0.7196\n",
            "Epoch [7/15], Loss: 1.5435, Accuracy: 0.7353\n",
            "Epoch [8/15], Loss: 1.5640, Accuracy: 0.6824\n",
            "Epoch [9/15], Loss: 1.5538, Accuracy: 0.7235\n",
            "Epoch [10/15], Loss: 1.5332, Accuracy: 0.7294\n",
            "Epoch [11/15], Loss: 1.5235, Accuracy: 0.7059\n",
            "Epoch [12/15], Loss: 1.5170, Accuracy: 0.7275\n",
            "Epoch [13/15], Loss: 1.5126, Accuracy: 0.7529\n",
            "Epoch [14/15], Loss: 1.5172, Accuracy: 0.7373\n",
            "Epoch [15/15], Loss: 1.5002, Accuracy: 0.7686\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 8 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5684, Accuracy: 0.6754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5674, Accuracy: 0.7035\n",
            "Epoch [3/15], Loss: 1.5887, Accuracy: 0.6579\n",
            "Epoch [4/15], Loss: 1.5292, Accuracy: 0.7246\n",
            "Epoch [5/15], Loss: 1.5488, Accuracy: 0.7018\n",
            "Epoch [6/15], Loss: 1.5341, Accuracy: 0.7158\n",
            "Epoch [7/15], Loss: 1.5171, Accuracy: 0.7281\n",
            "Epoch [8/15], Loss: 1.5179, Accuracy: 0.7456\n",
            "Epoch [9/15], Loss: 1.5175, Accuracy: 0.7316\n",
            "Epoch [10/15], Loss: 1.4959, Accuracy: 0.7649\n",
            "Epoch [11/15], Loss: 1.4899, Accuracy: 0.7702\n",
            "Epoch [12/15], Loss: 1.4822, Accuracy: 0.7789\n",
            "Epoch [13/15], Loss: 1.5449, Accuracy: 0.6842\n",
            "Epoch [14/15], Loss: 1.4957, Accuracy: 0.7456\n",
            "Epoch [15/15], Loss: 1.4978, Accuracy: 0.7789\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 9 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5833, Accuracy: 0.6667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5554, Accuracy: 0.7032\n",
            "Epoch [3/15], Loss: 1.5682, Accuracy: 0.6825\n",
            "Epoch [4/15], Loss: 1.5473, Accuracy: 0.6968\n",
            "Epoch [5/15], Loss: 1.5695, Accuracy: 0.6762\n",
            "Epoch [6/15], Loss: 1.5799, Accuracy: 0.6413\n",
            "Epoch [7/15], Loss: 1.5276, Accuracy: 0.7270\n",
            "Epoch [8/15], Loss: 1.5172, Accuracy: 0.7460\n",
            "Epoch [9/15], Loss: 1.5138, Accuracy: 0.7429\n",
            "Epoch [10/15], Loss: 1.5251, Accuracy: 0.7143\n",
            "Epoch [11/15], Loss: 1.5339, Accuracy: 0.7016\n",
            "Epoch [12/15], Loss: 1.5115, Accuracy: 0.7444\n",
            "Epoch [13/15], Loss: 1.4948, Accuracy: 0.7698\n",
            "Epoch [14/15], Loss: 1.4953, Accuracy: 0.7635\n",
            "Epoch [15/15], Loss: 1.4917, Accuracy: 0.7714\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 10 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.6100, Accuracy: 0.6623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.6082, Accuracy: 0.6246\n",
            "Epoch [3/15], Loss: 1.5813, Accuracy: 0.6812\n",
            "Epoch [4/15], Loss: 1.5873, Accuracy: 0.6725\n",
            "Epoch [5/15], Loss: 1.5403, Accuracy: 0.7072\n",
            "Epoch [6/15], Loss: 1.5646, Accuracy: 0.6928\n",
            "Epoch [7/15], Loss: 1.5305, Accuracy: 0.7203\n",
            "Epoch [8/15], Loss: 1.5595, Accuracy: 0.6725\n",
            "Epoch [9/15], Loss: 1.5294, Accuracy: 0.7246\n",
            "Epoch [10/15], Loss: 1.5113, Accuracy: 0.7348\n",
            "Epoch [11/15], Loss: 1.5069, Accuracy: 0.7290\n",
            "Epoch [12/15], Loss: 1.5128, Accuracy: 0.7478\n",
            "Epoch [13/15], Loss: 1.4966, Accuracy: 0.7594\n",
            "Epoch [14/15], Loss: 1.4990, Accuracy: 0.7406\n",
            "Epoch [15/15], Loss: 1.5162, Accuracy: 0.7232\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 11 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5179, Accuracy: 0.7240\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5045, Accuracy: 0.7453\n",
            "Epoch [3/15], Loss: 1.5281, Accuracy: 0.7013\n",
            "Epoch [4/15], Loss: 1.5354, Accuracy: 0.7107\n",
            "Epoch [5/15], Loss: 1.5004, Accuracy: 0.7387\n",
            "Epoch [6/15], Loss: 1.5271, Accuracy: 0.7067\n",
            "Epoch [7/15], Loss: 1.5068, Accuracy: 0.7427\n",
            "Epoch [8/15], Loss: 1.5013, Accuracy: 0.7413\n",
            "Epoch [9/15], Loss: 1.4995, Accuracy: 0.7347\n",
            "Epoch [10/15], Loss: 1.4982, Accuracy: 0.7467\n",
            "Epoch [11/15], Loss: 1.4898, Accuracy: 0.7547\n",
            "Epoch [12/15], Loss: 1.4837, Accuracy: 0.7573\n",
            "Epoch [13/15], Loss: 1.4898, Accuracy: 0.7547\n",
            "Epoch [14/15], Loss: 1.4760, Accuracy: 0.7693\n",
            "Epoch [15/15], Loss: 1.4559, Accuracy: 0.7907\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 12 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5313, Accuracy: 0.7222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5241, Accuracy: 0.7284\n",
            "Epoch [3/15], Loss: 1.5516, Accuracy: 0.6840\n",
            "Epoch [4/15], Loss: 1.5503, Accuracy: 0.6988\n",
            "Epoch [5/15], Loss: 1.5277, Accuracy: 0.7049\n",
            "Epoch [6/15], Loss: 1.5372, Accuracy: 0.7025\n",
            "Epoch [7/15], Loss: 1.5264, Accuracy: 0.7062\n",
            "Epoch [8/15], Loss: 1.5091, Accuracy: 0.7222\n",
            "Epoch [9/15], Loss: 1.5193, Accuracy: 0.7160\n",
            "Epoch [10/15], Loss: 1.5150, Accuracy: 0.7346\n",
            "Epoch [11/15], Loss: 1.5043, Accuracy: 0.7346\n",
            "Epoch [12/15], Loss: 1.4942, Accuracy: 0.7469\n",
            "Epoch [13/15], Loss: 1.4925, Accuracy: 0.7407\n",
            "Epoch [14/15], Loss: 1.4917, Accuracy: 0.7333\n",
            "Epoch [15/15], Loss: 1.4980, Accuracy: 0.7568\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 13 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5169, Accuracy: 0.7264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5151, Accuracy: 0.7253\n",
            "Epoch [3/15], Loss: 1.5201, Accuracy: 0.7138\n",
            "Epoch [4/15], Loss: 1.4995, Accuracy: 0.7356\n",
            "Epoch [5/15], Loss: 1.5005, Accuracy: 0.7621\n",
            "Epoch [6/15], Loss: 1.5124, Accuracy: 0.7368\n",
            "Epoch [7/15], Loss: 1.5107, Accuracy: 0.7241\n",
            "Epoch [8/15], Loss: 1.4787, Accuracy: 0.7609\n",
            "Epoch [9/15], Loss: 1.4749, Accuracy: 0.7782\n",
            "Epoch [10/15], Loss: 1.5265, Accuracy: 0.7379\n",
            "Epoch [11/15], Loss: 1.5597, Accuracy: 0.6667\n",
            "Epoch [12/15], Loss: 1.5016, Accuracy: 0.7379\n",
            "Epoch [13/15], Loss: 1.4833, Accuracy: 0.7448\n",
            "Epoch [14/15], Loss: 1.5550, Accuracy: 0.5931\n",
            "Epoch [15/15], Loss: 1.5018, Accuracy: 0.7057\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 14 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5532, Accuracy: 0.7022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5684, Accuracy: 0.7043\n",
            "Epoch [3/15], Loss: 1.5470, Accuracy: 0.7140\n",
            "Epoch [4/15], Loss: 1.5036, Accuracy: 0.7441\n",
            "Epoch [5/15], Loss: 1.5311, Accuracy: 0.7204\n",
            "Epoch [6/15], Loss: 1.5280, Accuracy: 0.7215\n",
            "Epoch [7/15], Loss: 1.5235, Accuracy: 0.7280\n",
            "Epoch [8/15], Loss: 1.5064, Accuracy: 0.7473\n",
            "Epoch [9/15], Loss: 1.6059, Accuracy: 0.5720\n",
            "Epoch [10/15], Loss: 1.5546, Accuracy: 0.6817\n",
            "Epoch [11/15], Loss: 1.6802, Accuracy: 0.4710\n",
            "Epoch [12/15], Loss: 1.7405, Accuracy: 0.3806\n",
            "Epoch [13/15], Loss: 1.6332, Accuracy: 0.5473\n",
            "Epoch [14/15], Loss: 1.5241, Accuracy: 0.7290\n",
            "Epoch [15/15], Loss: 1.5070, Accuracy: 0.7495\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 15 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5612, Accuracy: 0.6838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5347, Accuracy: 0.7030\n",
            "Epoch [3/15], Loss: 1.5391, Accuracy: 0.6990\n",
            "Epoch [4/15], Loss: 1.5270, Accuracy: 0.7152\n",
            "Epoch [5/15], Loss: 1.5315, Accuracy: 0.7010\n",
            "Epoch [6/15], Loss: 1.5128, Accuracy: 0.7343\n",
            "Epoch [7/15], Loss: 1.5236, Accuracy: 0.7172\n",
            "Epoch [8/15], Loss: 1.5052, Accuracy: 0.7293\n",
            "Epoch [9/15], Loss: 1.5102, Accuracy: 0.7141\n",
            "Epoch [10/15], Loss: 1.5100, Accuracy: 0.7152\n",
            "Epoch [11/15], Loss: 1.5029, Accuracy: 0.7303\n",
            "Epoch [12/15], Loss: 1.4954, Accuracy: 0.7384\n",
            "Epoch [13/15], Loss: 1.4847, Accuracy: 0.7545\n",
            "Epoch [14/15], Loss: 1.5061, Accuracy: 0.7323\n",
            "Epoch [15/15], Loss: 1.4776, Accuracy: 0.7818\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 16 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5329, Accuracy: 0.6924\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5252, Accuracy: 0.6990\n",
            "Epoch [3/15], Loss: 1.5189, Accuracy: 0.7076\n",
            "Epoch [4/15], Loss: 1.5116, Accuracy: 0.7267\n",
            "Epoch [5/15], Loss: 1.5150, Accuracy: 0.7229\n",
            "Epoch [6/15], Loss: 1.4994, Accuracy: 0.7362\n",
            "Epoch [7/15], Loss: 1.4839, Accuracy: 0.7495\n",
            "Epoch [8/15], Loss: 1.4886, Accuracy: 0.7476\n",
            "Epoch [9/15], Loss: 1.4866, Accuracy: 0.7371\n",
            "Epoch [10/15], Loss: 1.4875, Accuracy: 0.7524\n",
            "Epoch [11/15], Loss: 1.4838, Accuracy: 0.7476\n",
            "Epoch [12/15], Loss: 1.4830, Accuracy: 0.7581\n",
            "Epoch [13/15], Loss: 1.4885, Accuracy: 0.7571\n",
            "Epoch [14/15], Loss: 1.4673, Accuracy: 0.7610\n",
            "Epoch [15/15], Loss: 1.4831, Accuracy: 0.7467\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 17 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5479, Accuracy: 0.6838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5340, Accuracy: 0.6892\n",
            "Epoch [3/15], Loss: 1.5294, Accuracy: 0.7045\n",
            "Epoch [4/15], Loss: 1.5288, Accuracy: 0.6928\n",
            "Epoch [5/15], Loss: 1.5087, Accuracy: 0.7387\n",
            "Epoch [6/15], Loss: 1.5277, Accuracy: 0.7009\n",
            "Epoch [7/15], Loss: 1.5073, Accuracy: 0.7162\n",
            "Epoch [8/15], Loss: 1.5045, Accuracy: 0.7261\n",
            "Epoch [9/15], Loss: 1.4955, Accuracy: 0.7351\n",
            "Epoch [10/15], Loss: 1.5019, Accuracy: 0.7405\n",
            "Epoch [11/15], Loss: 1.4983, Accuracy: 0.7396\n",
            "Epoch [12/15], Loss: 1.4985, Accuracy: 0.7315\n",
            "Epoch [13/15], Loss: 1.5002, Accuracy: 0.7342\n",
            "Epoch [14/15], Loss: 1.4949, Accuracy: 0.7432\n",
            "Epoch [15/15], Loss: 1.4941, Accuracy: 0.7315\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 18 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5232, Accuracy: 0.7120\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5217, Accuracy: 0.7128\n",
            "Epoch [3/15], Loss: 1.5042, Accuracy: 0.7419\n",
            "Epoch [4/15], Loss: 1.5033, Accuracy: 0.7214\n",
            "Epoch [5/15], Loss: 1.5102, Accuracy: 0.7239\n",
            "Epoch [6/15], Loss: 1.4890, Accuracy: 0.7393\n",
            "Epoch [7/15], Loss: 1.4957, Accuracy: 0.7444\n",
            "Epoch [8/15], Loss: 1.5019, Accuracy: 0.7291\n",
            "Epoch [9/15], Loss: 1.5123, Accuracy: 0.7068\n",
            "Epoch [10/15], Loss: 1.5042, Accuracy: 0.7325\n",
            "Epoch [11/15], Loss: 1.4972, Accuracy: 0.7342\n",
            "Epoch [12/15], Loss: 1.4862, Accuracy: 0.7385\n",
            "Epoch [13/15], Loss: 1.4721, Accuracy: 0.7615\n",
            "Epoch [14/15], Loss: 1.5003, Accuracy: 0.7342\n",
            "Epoch [15/15], Loss: 1.4856, Accuracy: 0.7487\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------- Number of Iteration 19 ---------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Loss: 1.5142, Accuracy: 0.7195\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/15], Loss: 1.5040, Accuracy: 0.7317\n",
            "Epoch [3/15], Loss: 1.5033, Accuracy: 0.7472\n",
            "Epoch [4/15], Loss: 1.5167, Accuracy: 0.7228\n",
            "Epoch [5/15], Loss: 1.4977, Accuracy: 0.7374\n",
            "Epoch [6/15], Loss: 1.4850, Accuracy: 0.7512\n",
            "Epoch [7/15], Loss: 1.4917, Accuracy: 0.7455\n",
            "Epoch [8/15], Loss: 1.4788, Accuracy: 0.7593\n",
            "Epoch [9/15], Loss: 1.4741, Accuracy: 0.7707\n",
            "Epoch [10/15], Loss: 1.4694, Accuracy: 0.7724\n",
            "Epoch [11/15], Loss: 1.4834, Accuracy: 0.7553\n",
            "Epoch [12/15], Loss: 1.4623, Accuracy: 0.7724\n",
            "Epoch [13/15], Loss: 1.4948, Accuracy: 0.7268\n",
            "Epoch [14/15], Loss: 1.4729, Accuracy: 0.7675\n",
            "Epoch [15/15], Loss: 1.4722, Accuracy: 0.7667\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_443073/3868758775.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for criterion in selection_criteria:\n",
        "    AL_class = ActiveLearningPipeline(model=model,\n",
        "                                      test_indices=test_indices,\n",
        "                                      available_pool_indices=available_pool_indices,\n",
        "                                      train_indices=train_indices,\n",
        "                                      selection_criterion=criterion,\n",
        "                                      iterations=iterations,\n",
        "                                      budget_per_iter=budget_per_iter,\n",
        "                                      num_epochs=num_epoch)\n",
        "    accuracy_scores_dict[criterion] = AL_class.run_pipeline()\n",
        "generate_plot(accuracy_scores_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5sKVVjSzEl7",
        "outputId": "79a03208-b298-4097-dd21-e36fdf2292d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'uncertainty_kmeans': [0.5198412698412698,\n",
              "              0.6329365079365079,\n",
              "              0.6183862433862434,\n",
              "              0.6236772486772486,\n",
              "              0.6428571428571428,\n",
              "              0.656084656084656,\n",
              "              0.6177248677248677,\n",
              "              0.6699735449735449,\n",
              "              0.6673280423280423,\n",
              "              0.673941798941799,\n",
              "              0.6574074074074073,\n",
              "              0.6402116402116402,\n",
              "              0.6428571428571428,\n",
              "              0.6660052910052909,\n",
              "              0.6818783068783069,\n",
              "              0.6626984126984127,\n",
              "              0.6924603174603174,\n",
              "              0.693121693121693,\n",
              "              0.691137566137566,\n",
              "              0.693121693121693]})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_scores_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MQzLd-vwdlx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}